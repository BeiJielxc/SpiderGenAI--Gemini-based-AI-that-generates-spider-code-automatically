"""
PyGen é”™è¯¯æ¡ˆä¾‹åº“ - ç»“æ„åŒ–çš„ Few-shot é”™è¯¯ç»éªŒ

è¿™ä¸ªæ¨¡å—ç»´æŠ¤äº† LLM ç”Ÿæˆçˆ¬è™«ä»£ç æ—¶å¸¸è§çš„é”™è¯¯æ¨¡å¼ï¼Œ
ä»¥ Few-shot çš„å½¢å¼æ³¨å…¥åˆ° System Prompt ä¸­ï¼Œå¸®åŠ©æ¨¡å‹é¿å…é‡å¤çŠ¯é”™ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    from error_cases import get_error_cases_prompt
    system_prompt += get_error_cases_prompt()

æ‰©å±•æ–¹å¼ï¼š
    åœ¨ ERROR_CASES åˆ—è¡¨ä¸­æ·»åŠ æ–°çš„é”™è¯¯æ¡ˆä¾‹å­—å…¸
"""

from typing import List, Dict, Any
from dataclasses import dataclass, field
from enum import Enum


class ErrorSeverity(Enum):
    """é”™è¯¯ä¸¥é‡ç¨‹åº¦"""
    CRITICAL = "critical"     # å¯¼è‡´è„šæœ¬å®Œå…¨å¤±è´¥
    HIGH = "high"             # å¯¼è‡´å¤§é‡æ•°æ®ä¸¢å¤±/é”™è¯¯
    MEDIUM = "medium"         # éƒ¨åˆ†æ•°æ®é”™è¯¯
    LOW = "low"               # è¾¹ç¼˜æƒ…å†µé—®é¢˜


class ErrorCategory(Enum):
    """é”™è¯¯ç±»åˆ«"""
    SELECTOR = "selector"           # é€‰æ‹©å™¨ç›¸å…³
    DATE_EXTRACTION = "date"        # æ—¥æœŸæå–ç›¸å…³
    PAGINATION = "pagination"       # åˆ†é¡µç›¸å…³
    SCHEMA = "schema"               # è¾“å‡ºæ ¼å¼ç›¸å…³
    HTML_PARSING = "html_parsing"   # HTML è§£æç›¸å…³
    SPA = "spa"                     # SPA/åŠ¨æ€é¡µé¢ç›¸å…³
    ROBUSTNESS = "robustness"       # å¥å£®æ€§ç›¸å…³


@dataclass
class ErrorCase:
    """é”™è¯¯æ¡ˆä¾‹æ•°æ®ç»“æ„"""
    id: str                              # å”¯ä¸€æ ‡è¯†
    title: str                           # é”™è¯¯æ ‡é¢˜
    category: ErrorCategory              # é”™è¯¯ç±»åˆ«
    severity: ErrorSeverity              # ä¸¥é‡ç¨‹åº¦
    symptom: str                         # ç—‡çŠ¶æè¿°
    root_cause: str                      # æ ¹å› åˆ†æ
    bad_pattern: str                     # é”™è¯¯ä»£ç æ¨¡å¼
    good_pattern: str                    # æ­£ç¡®ä»£ç æ¨¡å¼
    fix_instruction: str                 # ä¿®å¤æŒ‡ä»¤
    detection_hints: List[str] = field(default_factory=list)  # æ£€æµ‹å…³é”®è¯


# ============================================================================
# é”™è¯¯æ¡ˆä¾‹åº“ - åœ¨æ­¤æ·»åŠ æ–°çš„é”™è¯¯æ¡ˆä¾‹
# ============================================================================

ERROR_CASES: List[ErrorCase] = [
    
    # -------------------------------------------------------------------------
    # Case 1: ç¡¬ç¼–ç åˆ—ç´¢å¼•æå–æ—¥æœŸ
    # -------------------------------------------------------------------------
    ErrorCase(
        id="ERR_001",
        title="ç¡¬ç¼–ç åˆ—ç´¢å¼•æå–æ—¥æœŸ",
        category=ErrorCategory.DATE_EXTRACTION,
        severity=ErrorSeverity.CRITICAL,
        symptom="åœ¨æŸäº›ç½‘ç«™æ­£å¸¸ï¼Œæ¢ç«™å IndexError æˆ–æ—¥æœŸé”™ä½",
        root_cause="""LLM çœ‹åˆ°è¡¨å¤´ [é¡¹ç›®åç§°, ä¸»ä½“ç­‰çº§, å€ºé¡¹ç­‰çº§, è¯„çº§å±•æœ›, å…¬å‘Šæ—¶é—´, ä¸‹è½½]ï¼Œ
æ¨æ–­æ—¥æœŸåœ¨ç¬¬5åˆ—ï¼ˆç´¢å¼•4ï¼‰ã€‚ä½†ä¸åŒç½‘ç«™åˆ—é¡ºåºä¸åŒï¼Œå³ä½¿åŒä¸€ç½‘ç«™æ”¹ç‰ˆåä¹Ÿå¯èƒ½å˜ã€‚
è¿™æ˜¯"åˆ—é¡ºåºå›ºå®š"çš„å‡è®¾ï¼Œå±äºæ³›åŒ–ç­–ç•¥ä¸å¤Ÿä¿å®ˆã€‚""",
        bad_pattern="""# âŒ é”™è¯¯å†™æ³•ï¼šç¡¬ç¼–ç åˆ—ç´¢å¼•
date_elem = tds[4].select_one('span')
date_text = tds[3].get_text()
date = row.query_selector_all('td')[4].inner_text()""",
        good_pattern="""# âœ… æ­£ç¡®å†™æ³•ï¼šæ™ºèƒ½æ‰«ææ•´è¡Œ
date = _pygen_smart_find_date_in_row_bs4(tds)  # BeautifulSoup
date = _pygen_smart_find_date_in_row_pw(tds)   # Playwright

# æˆ–æ‰‹åŠ¨å®ç°æ™ºèƒ½æ‰«æ
def find_date_in_row(tds):
    import re
    date_re = re.compile(r'(\\d{4}[-/.]\\d{1,2}[-/.]\\d{1,2})')
    for td in tds:
        for tag in ['span', 'time']:
            elem = td.select_one(tag)
            if elem:
                m = date_re.search(elem.get_text(strip=True))
                if m: return m.group(1)
        m = date_re.search(td.get_text(strip=True))
        if m: return m.group(1)
    return ""
""",
        fix_instruction="ä½¿ç”¨ _pygen_smart_find_date_in_row_* å‡½æ•°æ‰«ææ•´è¡Œï¼Œä¸è¦å‡è®¾æ—¥æœŸåœ¨å›ºå®šåˆ—",
        detection_hints=["tds[", ".get_text(", "date", "query_selector"]
    ),
    
    # -------------------------------------------------------------------------
    # Case 2: tbody tr é“¾å¼è°ƒç”¨ç©ºæŒ‡é’ˆ
    # -------------------------------------------------------------------------
    ErrorCase(
        id="ERR_002",
        title="tbody tr é“¾å¼è°ƒç”¨ç©ºæŒ‡é’ˆ",
        category=ErrorCategory.HTML_PARSING,
        severity=ErrorSeverity.HIGH,
        symptom="'NoneType' object has no attribute 'find_all'",
        root_cause="""LLM çœ‹åˆ°çš„ HTML: <table><tr>...</tr></table>ï¼ˆæ—  tbodyï¼‰
ä½†ä¹ æƒ¯æ€§å†™å‡º: table.find('tbody').find_all('tr')
å¾ˆå¤šæ•™ç¨‹å’Œè®­ç»ƒæ•°æ®ä¸­éƒ½ç”¨ tbody trï¼ŒLLM æ²¡ä»”ç»†æ£€æŸ¥ HTML å°±ç”¨äº†ä¹ æƒ¯å†™æ³•ã€‚""",
        bad_pattern="""# âŒ é”™è¯¯å†™æ³•ï¼šé“¾å¼è°ƒç”¨å¯èƒ½ç©ºæŒ‡é’ˆ
rows = table.find('tbody').find_all('tr')
items = soup.find('div').find_all('li')
data = container.find('ul').find('li').get_text()""",
        good_pattern="""# âœ… æ­£ç¡®å†™æ³•1ï¼šä¼˜å…ˆä½¿ç”¨ CSS é€‰æ‹©å™¨ï¼ˆè¿”å›ç©ºåˆ—è¡¨è€Œé Noneï¼‰
rows = soup.select('table tbody tr')
rows = soup.select('table tr')  # è‹¥æ²¡æœ‰ tbody

# âœ… æ­£ç¡®å†™æ³•2ï¼šå¦‚æœå¿…é¡»ç”¨ findï¼Œåš None æ£€æŸ¥
tbody = table.find('tbody')
rows = tbody.find_all('tr') if tbody else table.find_all('tr')

# âœ… æ­£ç¡®å†™æ³•3ï¼šä½¿ç”¨ walrus æ“ä½œç¬¦
if (tbody := table.find('tbody')):
    rows = tbody.find_all('tr')
else:
    rows = table.find_all('tr')""",
        fix_instruction="ä¼˜å…ˆç”¨ soup.select('table tbody tr')ï¼Œæˆ–å¯¹æ¯å±‚ find ç»“æœåš None æ£€æŸ¥",
        detection_hints=[".find('tbody').find_all", ".find(", ").find_all("]
    ),
    
    # -------------------------------------------------------------------------
    # Case 3: åªå¤„ç†ç¬¬ä¸€é¡µæ—¥æœŸï¼ˆåˆ†é¡µæ—¥æœŸä¸¢å¤±ï¼‰
    # -------------------------------------------------------------------------
    ErrorCase(
        id="ERR_003",
        title="åªå¤„ç†ç¬¬ä¸€é¡µæ—¥æœŸ",
        category=ErrorCategory.PAGINATION,
        severity=ErrorSeverity.CRITICAL,
        symptom="ç¬¬ä¸€é¡µæœ‰æ—¥æœŸï¼Œåç»­é¡µæ—¥æœŸå…¨éƒ¨ä¸ºç©º",
        root_cause="""LLM è®¾è®¡çš„æµç¨‹:
1. fetch_page_data() å¾ªç¯è·å–æ‰€æœ‰é¡µé¢çš„æ•°æ®
2. extract_dates_from_rendered_page() åªæ‰“å¼€ç¬¬ä¸€é¡µæå–æ—¥æœŸ
3. åˆå¹¶ â†’ å¤§éƒ¨åˆ†è®°å½•æ²¡æœ‰æ—¥æœŸ

LLM èƒ½ç†è§£åˆ†é¡µï¼Œä½†åœ¨è®¾è®¡"æ—¥æœŸæå–"æ¨¡å—æ—¶æ²¡æœ‰è€ƒè™‘åˆ°åˆ†é¡µåœºæ™¯ï¼Œæ¶æ„æ€è€ƒä¸å®Œæ•´ã€‚""",
        bad_pattern="""# âŒ é”™è¯¯å†™æ³•ï¼šæ—¥æœŸæå–ä¸åˆ†é¡µåˆ†ç¦»
def main():
    all_reports = []
    for page in range(1, total_pages + 1):
        reports = fetch_page_data(page)  # åªè·å–æ•°æ®ï¼Œæ²¡æœ‰æ—¥æœŸ
        all_reports.extend(reports)
    
    # åªä»ç¬¬ä¸€é¡µæå–æ—¥æœŸï¼
    dates = extract_dates_from_page(page_url)
    for i, report in enumerate(all_reports):
        report['date'] = dates[i] if i < len(dates) else ''""",
        good_pattern="""# âœ… æ­£ç¡®å†™æ³•ï¼šåœ¨åŒä¸€ä¸ªå¾ªç¯ä¸­æå–æ—¥æœŸ
def fetch_page_data(page_num):
    # ... è·å– HTML/API å“åº” ...
    for row in rows:
        tds = row.select('td')
        name = tds[0].get_text(strip=True)
        date = _pygen_smart_find_date_in_row_bs4(tds)  # åŒæ­¥æå–æ—¥æœŸ
        download_url = ...
        reports.append({
            "name": name,
            "date": date,  # æ—¥æœŸåœ¨è¿™é‡Œå°±æå–äº†
            "downloadUrl": download_url,
            "fileType": file_type
        })
    return reports

# âœ… å¦‚æœå¿…é¡»ç”¨ Playwright æå–æ—¥æœŸï¼Œæ¯é¡µéƒ½è¦å¤„ç†
def fetch_all_with_dates():
    all_reports = []
    for page in range(1, total_pages + 1):
        reports = fetch_page_data(page)
        dates = extract_dates_for_page(page)  # æ¯é¡µéƒ½æå–æ—¥æœŸ
        for r, d in zip(reports, dates):
            r['date'] = d
        all_reports.extend(reports)""",
        fix_instruction="åœ¨è·å–æ¯é¡µæ•°æ®æ—¶åŒæ­¥æå–æ—¥æœŸï¼Œä¸è¦åˆ†æˆä¸¤ä¸ªé˜¶æ®µå¤„ç†",
        detection_hints=["extract_dates", "for page", "all_reports"]
    ),
    
    # -------------------------------------------------------------------------
    # Case 4: span vs ç›´æ¥æ–‡æœ¬ - æ ·æœ¬åå·®
    # -------------------------------------------------------------------------
    ErrorCase(
        id="ERR_004",
        title="å‡è®¾æ—¥æœŸæ€»åœ¨ span æ ‡ç­¾ä¸­",
        category=ErrorCategory.DATE_EXTRACTION,
        severity=ErrorSeverity.MEDIUM,
        symptom="éƒ¨åˆ†ç½‘ç«™æ—¥æœŸæå–ä¸ºç©º",
        root_cause="""LLM çœ‹åˆ°çš„æ ·æœ¬: <td><span>2026-01-04</span></td>
LLM å‡è®¾çš„: æ—¥æœŸéƒ½åœ¨ span é‡Œ
ä½†å®é™…æƒ…å†µ: æœ‰çš„æ˜¯ <td>2026-01-04</td>ï¼ˆç›´æ¥æ–‡æœ¬ï¼‰
æœ‰çš„æ˜¯ <td><time>2026-01-04</time></td>
LLM å¯èƒ½åªå…³æ³¨äº†éƒ¨åˆ†æ ·æœ¬ï¼Œæ²¡æœ‰åšé˜²å¾¡æ€§ç¼–ç¨‹ã€‚""",
        bad_pattern="""# âŒ é”™è¯¯å†™æ³•ï¼šåªæ£€æŸ¥ span
date_elem = td.select_one('span')
date = date_elem.get_text() if date_elem else ''""",
        good_pattern="""# âœ… æ­£ç¡®å†™æ³•ï¼šå¤šç­–ç•¥å°è¯•
def extract_date_from_cell(td):
    import re
    date_re = re.compile(r'(\\d{4}[-/.]\\d{1,2}[-/.]\\d{1,2})')
    
    # ç­–ç•¥1ï¼šå°è¯•å¸¸è§çš„æ—¥æœŸå®¹å™¨æ ‡ç­¾
    for tag in ['span', 'time', 'em', 'strong']:
        elem = td.select_one(tag)
        if elem:
            m = date_re.search(elem.get_text(strip=True))
            if m:
                return m.group(1)
    
    # ç­–ç•¥2ï¼šç›´æ¥ä» td æ–‡æœ¬æå–
    m = date_re.search(td.get_text(strip=True))
    if m:
        return m.group(1)
    
    return ''""",
        fix_instruction="ä¾æ¬¡å°è¯• span/time/ç›´æ¥æ–‡æœ¬ ç­‰å¤šç§æ¨¡å¼ï¼Œåšé˜²å¾¡æ€§ç¼–ç¨‹",
        detection_hints=["select_one('span')", "find('span')"]
    ),
    
    # -------------------------------------------------------------------------
    # Case 5: SPA é¡µé¢ç”¨ requests æŠ“ HTML
    # -------------------------------------------------------------------------
    ErrorCase(
        id="ERR_005",
        title="ç”¨ requests æŠ“å– SPA é¡µé¢å†…å®¹",
        category=ErrorCategory.SPA,
        severity=ErrorSeverity.CRITICAL,
        symptom="date å…¨éƒ¨ä¸ºç©ºï¼Œæˆ–æŠ“å–åˆ°çš„å†…å®¹æ˜¯ç©ºæ¨¡æ¿",
        root_cause="""SPAï¼ˆå•é¡µåº”ç”¨ï¼‰çš„æ•°æ®æ˜¯é€šè¿‡ JavaScript åœ¨å®¢æˆ·ç«¯æ¸²æŸ“çš„ã€‚
requests.get() åªèƒ½æ‹¿åˆ°æœåŠ¡ç«¯è¿”å›çš„ HTML éª¨æ¶ï¼Œçœ‹ä¸åˆ°æ¸²æŸ“åçš„å†…å®¹ã€‚
LLM æ²¡æœ‰è¯†åˆ«å‡ºé¡µé¢æ˜¯ SPAï¼Œæˆ–è€…çŸ¥é“ä½†æ²¡æ­£ç¡®å¤„ç†ã€‚""",
        bad_pattern="""# âŒ é”™è¯¯å†™æ³•ï¼šç”¨ requests æŠ“ SPA é¡µé¢
resp = requests.get("https://example.com/#/rating/list")
soup = BeautifulSoup(resp.text, 'html.parser')
dates = soup.select('span.list-time')  # é€šå¸¸ä¸ºç©ºï¼""",
        good_pattern="""# âœ… æ­£ç¡®å†™æ³•ï¼šç”¨ Playwright æ¸²æŸ“åæå–
from playwright.sync_api import sync_playwright

def extract_from_spa(url):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(url, wait_until="domcontentloaded")
        page.wait_for_timeout(1500)  # ç­‰å¾… JS æ¸²æŸ“
        
        # ç°åœ¨å¯ä»¥æ‹¿åˆ°æ¸²æŸ“åçš„å†…å®¹
        items = page.query_selector_all('.list-item')
        results = []
        for item in items:
            date_el = item.query_selector('span.list-time')
            date = date_el.inner_text() if date_el else ''
            results.append(date)
        
        browser.close()
    return results

# âœ… æ··åˆæ¨¡å¼ï¼šAPI è·å–ä¸»æ•°æ®ï¼ŒPlaywright åªç”¨äºæ—¥æœŸ
# è¿™æ ·å¯ä»¥å¹³è¡¡é€Ÿåº¦å’Œæ­£ç¡®æ€§""",
        fix_instruction="SPA é¡µé¢å¿…é¡»ç”¨ Playwright æ¸²æŸ“åæå–ï¼Œæˆ–è€…ç›´æ¥è°ƒç”¨å…¶ API æ¥å£",
        detection_hints=["requests.get", "/#/", "hash", "spa"]
    ),
    
    # -------------------------------------------------------------------------
    # Case 6: è¾“å‡ºå­—æ®µåç”¨ title è€Œä¸æ˜¯ name
    # -------------------------------------------------------------------------
    ErrorCase(
        id="ERR_006",
        title="è¾“å‡ºå­—æ®µåä½¿ç”¨ title è€Œä¸æ˜¯ name",
        category=ErrorCategory.SCHEMA,
        severity=ErrorSeverity.HIGH,
        symptom="å‰ç«¯æ— æ³•æ˜¾ç¤ºæŠ¥å‘Šåç§°",
        root_cause="""ç³»ç»Ÿè¦æ±‚è¾“å‡ºå­—æ®µåå¿…é¡»æ˜¯ name/date/downloadUrl/fileTypeã€‚
ä½† LLM ä¹ æƒ¯æ€§ä½¿ç”¨äº† title ä½œä¸ºå­—æ®µåã€‚
è¿™æ˜¯å¯¹è¾“å‡ºè§„èŒƒç†è§£ä¸å‡†ç¡®å¯¼è‡´çš„ã€‚""",
        bad_pattern="""# âŒ é”™è¯¯å†™æ³•ï¼šä½¿ç”¨ title ä½œä¸ºå­—æ®µå
reports.append({
    "title": item_title,  # åº”è¯¥æ˜¯ "name"
    "date": date,
    "url": download_url,   # åº”è¯¥æ˜¯ "downloadUrl"
    "type": "pdf"          # åº”è¯¥æ˜¯ "fileType"
})""",
        good_pattern="""# âœ… æ­£ç¡®å†™æ³•ï¼šä½¿ç”¨è§„å®šçš„å­—æ®µå
reports.append({
    "name": item_title,       # å¿…é¡»ç”¨ name
    "date": date,
    "downloadUrl": download_url,  # å¿…é¡»ç”¨ downloadUrl
    "fileType": "pdf"             # å¿…é¡»ç”¨ fileType
})""",
        fix_instruction="è¾“å‡º JSON å¿…é¡»ä½¿ç”¨å­—æ®µå: name, date, downloadUrl, fileType",
        detection_hints=['"title":', "'title':"]
    ),
    
    # -------------------------------------------------------------------------
    # Case 7: æ—¥æœŸèŒƒå›´è¿‡æ»¤ä¿ç•™äº†æ— æ—¥æœŸè®°å½•
    # -------------------------------------------------------------------------
    ErrorCase(
        id="ERR_007",
        title="æ—¥æœŸèŒƒå›´è¿‡æ»¤æ—¶ä¿ç•™æ— æ—¥æœŸè®°å½•",
        category=ErrorCategory.DATE_EXTRACTION,
        severity=ErrorSeverity.MEDIUM,
        symptom="è¾“å‡ºä¸­åŒ…å«å¤§é‡æ— æ—¥æœŸçš„è®°å½•",
        root_cause="""ç”¨æˆ·è¦æ±‚æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤ï¼Œä½†è„šæœ¬å¯¹æ— æ—¥æœŸè®°å½•åšäº†"ä¿ç•™"å¤„ç†ã€‚
è¿™é€šå¸¸æ˜¯ LLM æƒ³"ä¿è¯æ•°æ®å®Œæ•´æ€§"çš„å¥½æ„ï¼Œä½†è¿åäº†ç”¨æˆ·çš„è¿‡æ»¤è¦æ±‚ã€‚""",
        bad_pattern="""# âŒ é”™è¯¯å†™æ³•ï¼šä¿ç•™æ— æ—¥æœŸè®°å½•
if date_str and start_date <= date_str <= end_date:
    filtered.append(report)
elif not date_str:
    # æ— æ—¥æœŸä¹Ÿä¿ç•™
    filtered.append(report)""",
        good_pattern="""# âœ… æ­£ç¡®å†™æ³•ï¼šä¸¥æ ¼æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤
for report in reports:
    date_str = report.get('date', '')
    if not date_str:
        continue  # æ— æ—¥æœŸç›´æ¥è·³è¿‡
    if start_date <= date_str <= end_date:
        filtered.append(report)""",
        fix_instruction="å½“ç”¨æˆ·æŒ‡å®šæ—¥æœŸèŒƒå›´æ—¶ï¼Œæ— æ—¥æœŸè®°å½•å¿…é¡»ä¸¢å¼ƒï¼Œä¸è¦ä¿ç•™",
        detection_hints=["elif not date", "if not date", "æ— æ—¥æœŸ"]
    ),
    
    # -------------------------------------------------------------------------
    # Case 8: ä»æ ‡é¢˜çŒœæµ‹æ—¥æœŸ
    # -------------------------------------------------------------------------
    ErrorCase(
        id="ERR_008",
        title="ä»æŠ¥å‘Šæ ‡é¢˜çŒœæµ‹æ—¥æœŸ",
        category=ErrorCategory.DATE_EXTRACTION,
        severity=ErrorSeverity.CRITICAL,
        symptom="æ—¥æœŸå…¨æ˜¯å¹´æœ«ï¼ˆ12-31ï¼‰æˆ–æ ¼å¼é”™è¯¯",
        root_cause="""LLM çœ‹åˆ°æ ‡é¢˜ "2025å¹´åº¦ä¸»åŠ¨è¯„çº§æŠ¥å‘Š"ï¼Œä»ä¸­æå– 2025ï¼Œ
ç„¶åæ‹¼æˆ 2025-12-31 ä½œä¸ºæ—¥æœŸã€‚è¿™æ˜¯å®Œå…¨é”™è¯¯çš„åšæ³•ã€‚
æŠ¥å‘Šçš„å‘å¸ƒæ—¥æœŸå’Œæ ‡é¢˜ä¸­çš„å¹´ä»½æ˜¯ä¸åŒçš„æ¦‚å¿µã€‚""",
        bad_pattern="""# âŒ é”™è¯¯å†™æ³•ï¼šä»æ ‡é¢˜æå–å¹´ä»½ä½œä¸ºæ—¥æœŸ
import re
title = "2025å¹´åº¦ä¸»åŠ¨è¯„çº§æŠ¥å‘Š"
year = re.search(r'(\\d{4})å¹´', title)
if year:
    date = f"{year.group(1)}-12-31"  # å®Œå…¨é”™è¯¯ï¼""",
        good_pattern="""# âœ… æ­£ç¡®å†™æ³•ï¼šåªä»æ­£è§„æ—¥æœŸæºè·å–
# 1. ä¼˜å…ˆä» API å“åº”çš„æ—¥æœŸå­—æ®µè·å–
date = item.get('rankdate') or item.get('publishtime') or ''

# 2. ä» HTML çš„æ—¥æœŸå…ƒç´ è·å–
date = _pygen_smart_find_date_in_row_bs4(tds)

# 3. å¦‚æœæ— æ³•è·å–ï¼Œç•™ç©ºè€Œä¸æ˜¯çŒœæµ‹
if not date:
    date = ''  # ç•™ç©ºï¼Œä¸è¦çŒœ""",
        fix_instruction="ç»å¯¹ç¦æ­¢ä»æ ‡é¢˜çŒœæµ‹æ—¥æœŸï¼Œæ— æ³•è·å–æ—¶ç•™ç©º",
        detection_hints=["å¹´åº¦", "å¹´æŠ¥", "12-31"]
    ),
    
    # -------------------------------------------------------------------------
    # Case 9: é™æ€ HTML é¡µé¢æœªæå–æ—¥æœŸ
    # -------------------------------------------------------------------------
    ErrorCase(
        id="ERR_009",
        title="é™æ€ HTML é¡µé¢æœªæå–æ—¥æœŸ",
        category=ErrorCategory.DATE_EXTRACTION,
        severity=ErrorSeverity.CRITICAL,
        symptom="æ‰€æœ‰è®°å½•çš„ date å­—æ®µä¸ºç©ºï¼Œå³ä½¿é¡µé¢ HTML ä¸­æ˜ç¡®æœ‰æ—¥æœŸæ˜¾ç¤º",
        root_cause="""LLM ç”Ÿæˆçš„ä»£ç è§£æäº† HTML è¡¨æ ¼æå–äº†æ ‡é¢˜å’Œä¸‹è½½é“¾æ¥ï¼Œ
ä½†å®Œå…¨é—æ¼äº†æ—¥æœŸæå–é€»è¾‘ã€‚è¿™é€šå¸¸å‘ç”Ÿåœ¨ï¼š
1. LLM ä¸“æ³¨äºæå–ä¸»è¦å­—æ®µï¼ˆæ ‡é¢˜ã€é“¾æ¥ï¼‰ï¼Œå¿˜è®°æ—¥æœŸ
2. æ—¥æœŸæ˜¾ç¤ºåœ¨è¡¨æ ¼ä¸­ä½† LLM æ²¡æœ‰è¯†åˆ«åˆ°å¯¹åº”åˆ—
3. ä»£ç åªæå–äº†éƒ¨åˆ†å­—æ®µï¼Œæ²¡æœ‰è¦†ç›–å®Œæ•´çš„è¾“å‡º schema

è¿™å¯¼è‡´æœ€ç»ˆç»“æœä¸­æ‰€æœ‰è®°å½•çš„æ—¥æœŸä¸ºç©ºï¼Œè¢«æ—¥æœŸèŒƒå›´è¿‡æ»¤å™¨å…¨éƒ¨ä¸¢å¼ƒã€‚""",
        bad_pattern="""# âŒ é”™è¯¯å†™æ³•ï¼šéå†è¡¨æ ¼ä½†æ²¡æœ‰æå–æ—¥æœŸ
def parse_list(html):
    soup = BeautifulSoup(html, 'html.parser')
    rows = soup.select('table tr')
    results = []
    for row in rows[1:]:
        cols = row.select('td')
        if cols:
            results.append({
                "name": cols[0].get_text(strip=True),
                "downloadUrl": cols[-1].select_one('a')['href'],
                # ç¼ºå°‘ date å­—æ®µçš„æå–ï¼
            })
    return results""",
        good_pattern="""# âœ… æ­£ç¡®å†™æ³•ï¼šåœ¨åŒä¸€ä¸ªå¾ªç¯ä¸­åŒæ—¶æå–æ—¥æœŸ
def parse_list(html):
    soup = BeautifulSoup(html, 'html.parser')
    rows = soup.select('table tr')
    results = []
    for row in rows[1:]:
        cols = row.select('td')
        if cols:
            # ä½¿ç”¨æ™ºèƒ½æ—¥æœŸæ‰«æå‡½æ•°æå–æ—¥æœŸ
            date = _pygen_smart_find_date_in_row_bs4(cols)
            results.append({
                "name": cols[0].get_text(strip=True),
                "date": date,  # æ—¥æœŸåœ¨è¿™é‡Œæå–
                "downloadUrl": cols[-1].select_one('a')['href'],
                "fileType": "pdf"
            })
    return results""",
        fix_instruction="åœ¨éå†è¡¨æ ¼è¡Œæ—¶ï¼ŒåŒæ­¥ä½¿ç”¨ _pygen_smart_find_date_in_row_bs4(tds) æå–æ—¥æœŸ",
        detection_hints=["soup.select", "table tr", "BeautifulSoup"]
    ),
    
    # -------------------------------------------------------------------------
    # Case 10: print è¾“å‡ºåŒ…å«é ASCII å­—ç¬¦å¯¼è‡´ç¼–ç é”™è¯¯
    # -------------------------------------------------------------------------
    ErrorCase(
        id="ERR_010",
        title="print è¾“å‡ºåŒ…å«é ASCII å­—ç¬¦å¯¼è‡´ GBK ç¼–ç é”™è¯¯",
        category=ErrorCategory.ROBUSTNESS,
        severity=ErrorSeverity.HIGH,
        symptom="'gbk' codec can't encode character '\\u2713' (æˆ–å…¶ä»– Unicode å­—ç¬¦)",
        root_cause="""Windows å‘½ä»¤è¡Œé»˜è®¤ä½¿ç”¨ GBK ç¼–ç ï¼Œæ— æ³•æ˜¾ç¤ºæŸäº› Unicode å­—ç¬¦ï¼ˆå¦‚ âœ“ã€âœ—ã€â†’ ç­‰ï¼‰ã€‚
å½“è„šæœ¬ä½¿ç”¨ print() è¾“å‡ºè¿™äº›å­—ç¬¦æ—¶ä¼šæŠ¥ç¼–ç é”™è¯¯å¹¶ç»ˆæ­¢è¿è¡Œã€‚
è¿™æ˜¯è·¨å¹³å°å…¼å®¹æ€§é—®é¢˜ï¼Œåœ¨ Linux/macOS ä¸Šé€šå¸¸ä¸ä¼šå‡ºç°ã€‚""",
        bad_pattern="""# âŒ é”™è¯¯å†™æ³•ï¼šä½¿ç”¨ Unicode ç‰¹æ®Šå­—ç¬¦
print(f"âœ“ å·²ä¿å­˜ {len(data)} æ¡è®°å½•")
print("âœ— ä¸‹è½½å¤±è´¥")
print("â†’ æ­£åœ¨å¤„ç†...")""",
        good_pattern="""# âœ… æ­£ç¡®å†™æ³•ï¼šä½¿ç”¨ ASCII å­—ç¬¦æˆ–å®‰å…¨è¾“å‡º
print(f"[OK] å·²ä¿å­˜ {len(data)} æ¡è®°å½•")
print("[FAIL] ä¸‹è½½å¤±è´¥")
print("-> æ­£åœ¨å¤„ç†...")

# æˆ–ä½¿ç”¨å®‰å…¨è¾“å‡ºå‡½æ•°
def safe_print(msg):
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', 'replace').decode())

safe_print("âœ“ ä»»åŠ¡å®Œæˆ")""",
        fix_instruction="é¿å…åœ¨ print ä¸­ä½¿ç”¨ Unicode ç‰¹æ®Šå­—ç¬¦ï¼ˆâœ“âœ—â†’ç­‰ï¼‰ï¼Œæ”¹ç”¨ ASCII å­—ç¬¦å¦‚ [OK]ã€[FAIL]ã€->",
        detection_hints=["print", "âœ“", "âœ—", "â†’", "âœ”", "âœ˜"]
    ),
    
    # -------------------------------------------------------------------------
    # Case 11: åŠ¨æ€é¡µé¢ä½¿ç”¨é™æ€ HTML è§£æ
    # -------------------------------------------------------------------------
    ErrorCase(
        id="ERR_011",
        title="åŠ¨æ€åŠ è½½é¡µé¢é”™è¯¯ä½¿ç”¨é™æ€ HTML è§£æ",
        category=ErrorCategory.SPA,
        severity=ErrorSeverity.CRITICAL,
        symptom="è¡¨æ ¼/åˆ—è¡¨ä¸ºç©ºï¼ˆæœªæ‰¾åˆ°æ•°æ®è¡Œï¼‰ï¼Œå³ä½¿é¡µé¢åœ¨æµè§ˆå™¨ä¸­æ˜¾ç¤ºæ­£å¸¸",
        root_cause="""é¡µé¢æ•°æ®é€šè¿‡ JavaScript åŠ¨æ€åŠ è½½ï¼ˆSPA/AJAXï¼‰ï¼Œä½†è„šæœ¬ä½¿ç”¨ requests.get() + BeautifulSoup è§£æ HTMLã€‚
requests åªèƒ½è·å–åˆå§‹ HTML éª¨æ¶ï¼Œçœ‹ä¸åˆ° JavaScript æ¸²æŸ“åå¡«å……çš„æ•°æ®ã€‚
å¸¸è§è¡¨ç°ï¼š
1. "æœªæ‰¾åˆ°æ•°æ®è¡Œ" æˆ– "æœªæ‰¾åˆ°è¡¨æ ¼"
2. è¡¨æ ¼å­˜åœ¨ä½† tbody ä¸ºç©º
3. API è¯·æ±‚è¢«æ•è·ä½†ä»£ç æ²¡æœ‰ä½¿ç”¨

æ­£ç¡®åšæ³•æ˜¯ç›´æ¥è°ƒç”¨æ•è·åˆ°çš„ API æ¥å£è·å– JSON æ•°æ®ã€‚""",
        bad_pattern="""# âŒ é”™è¯¯å†™æ³•ï¼šç”¨ requests è§£æåŠ¨æ€åŠ è½½çš„é¡µé¢
import requests
from bs4 import BeautifulSoup

def fetch_data():
    response = requests.get("https://example.com/list.html")
    soup = BeautifulSoup(response.text, 'html.parser')
    # è¡¨æ ¼å­˜åœ¨ä½† tbody æ˜¯ç©ºçš„ï¼å› ä¸ºæ•°æ®æ˜¯ JS å¡«å……çš„
    rows = table.select('tbody tr')  # è¿”å›ç©ºåˆ—è¡¨
    # ...

# é—®é¢˜ï¼šAPI è¯·æ±‚ä¿¡æ¯å·²ç»æä¾›ï¼Œä½†ä»£ç æ²¡æœ‰ä½¿ç”¨""",
        good_pattern="""# âœ… æ­£ç¡®å†™æ³•ï¼šç›´æ¥è°ƒç”¨ API è·å– JSON æ•°æ®
import requests

# ä½¿ç”¨æ•è·åˆ°çš„ API ç«¯ç‚¹
API_URL = "https://example.com/api/list"

def fetch_data(page=1):
    params = {
        "pageNo": page,
        "pageSize": 20,
    }
    response = requests.get(API_URL, params=params, headers=HEADERS)
    data = response.json()
    
    reports = []
    for item in data.get("data", {}).get("rows", []):
        reports.append({
            "name": item.get("title", ""),
            "date": item.get("rankdate", ""),
            "downloadUrl": item.get("fileUrl", ""),
            "fileType": "pdf"
        })
    return reports

# å…³é”®ï¼šä» API å“åº”ä¸­æå–æ•°æ®ï¼Œä¸è¦è§£æ HTML""",
        fix_instruction="å½“æ•è·åˆ° API è¯·æ±‚æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ requests è°ƒç”¨ API è·å– JSON æ•°æ®ï¼Œè€Œä¸æ˜¯è§£æ HTML",
        detection_hints=["BeautifulSoup", "tbody tr", "æœªæ‰¾åˆ°", "table.select"]
    ),
    
]


def get_error_cases_prompt(
    categories: List[ErrorCategory] = None,
    severity_threshold: ErrorSeverity = ErrorSeverity.LOW
) -> str:
    """
    ç”Ÿæˆé”™è¯¯æ¡ˆä¾‹çš„ Prompt æ–‡æœ¬
    
    Args:
        categories: è¦åŒ…å«çš„é”™è¯¯ç±»åˆ«ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
        severity_threshold: ä¸¥é‡ç¨‹åº¦é˜ˆå€¼ï¼ŒåªåŒ…å«å¤§äºç­‰äºæ­¤çº§åˆ«çš„é”™è¯¯
    
    Returns:
        æ ¼å¼åŒ–çš„ Prompt æ–‡æœ¬
    """
    severity_order = {
        ErrorSeverity.CRITICAL: 4,
        ErrorSeverity.HIGH: 3,
        ErrorSeverity.MEDIUM: 2,
        ErrorSeverity.LOW: 1
    }
    
    threshold = severity_order[severity_threshold]
    
    filtered_cases = []
    for case in ERROR_CASES:
        # è¿‡æ»¤ç±»åˆ«
        if categories and case.category not in categories:
            continue
        # è¿‡æ»¤ä¸¥é‡ç¨‹åº¦
        if severity_order[case.severity] < threshold:
            continue
        filtered_cases.append(case)
    
    if not filtered_cases:
        return ""
    
    lines = [
        "",
        "## ã€é‡è¦ã€‘å†å²é”™è¯¯æ¡ˆä¾‹è¦ç‚¹ï¼ˆè¯·å‹¿é‡å¤è¿™äº›é”™è¯¯ï¼‰",
        "",
        "ä»¥ä¸‹ä»…ä¿ç•™â€œé”™åœ¨å“ªé‡Œ / åº”è¯¥æ€ä¹ˆå†™â€çš„è¦ç‚¹ï¼ˆä¸å«ç¤ºä¾‹ä»£ç ï¼‰ï¼š",
        ""
    ]
    
    for i, case in enumerate(filtered_cases, 1):
        severity_emoji = {
            ErrorSeverity.CRITICAL: "ğŸ”´",
            ErrorSeverity.HIGH: "ğŸŸ ",
            ErrorSeverity.MEDIUM: "ğŸŸ¡",
            ErrorSeverity.LOW: "ğŸŸ¢"
        }[case.severity]
        
        # ç²¾ç®€ç‰ˆï¼šåªæä¾›â€œé”™è¯¯ + åº”è¯¥æ€ä¹ˆå†™â€ï¼Œä¸æä¾›ä»»ä½•ç¤ºä¾‹ä»£ç å—
        lines.extend([
            f"### Case {i}: {case.title} {severity_emoji}",
            f"- é”™è¯¯ï¼š{case.symptom}",
            f"- åº”è¯¥ï¼š{case.fix_instruction}",
            ""
        ])
    
    return "\n".join(lines)


def get_detection_patterns() -> Dict[str, ErrorCase]:
    """
    è·å–ç”¨äºæ£€æµ‹é”™è¯¯çš„æ¨¡å¼
    
    Returns:
        {æ£€æµ‹å…³é”®è¯: å¯¹åº”çš„é”™è¯¯æ¡ˆä¾‹} çš„æ˜ å°„
    """
    patterns = {}
    for case in ERROR_CASES:
        for hint in case.detection_hints:
            patterns[hint] = case
    return patterns


def get_error_case_by_id(error_id: str) -> Optional[ErrorCase]:
    """æ ¹æ® ID è·å–é”™è¯¯æ¡ˆä¾‹"""
    for case in ERROR_CASES:
        if case.id == error_id:
            return case
    return None


def add_error_case(case: ErrorCase) -> None:
    """åŠ¨æ€æ·»åŠ é”™è¯¯æ¡ˆä¾‹ï¼ˆè¿è¡Œæ—¶æ‰©å±•ï¼‰"""
    # æ£€æŸ¥ ID æ˜¯å¦é‡å¤
    existing_ids = {c.id for c in ERROR_CASES}
    if case.id in existing_ids:
        raise ValueError(f"é”™è¯¯æ¡ˆä¾‹ ID '{case.id}' å·²å­˜åœ¨")
    ERROR_CASES.append(case)


# å¯¼å‡ºç±»å‹æ³¨è§£ç”¨
from typing import Optional


if __name__ == "__main__":
    # æµ‹è¯•ï¼šæ‰“å°æ‰€æœ‰é”™è¯¯æ¡ˆä¾‹
    print(get_error_cases_prompt())
    print("\n" + "=" * 60 + "\n")
    print("æ£€æµ‹æ¨¡å¼:", list(get_detection_patterns().keys()))

