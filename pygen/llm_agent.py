"""
LLMä»£ç†æ¨¡å— - PyGençˆ¬è™«è„šæœ¬ç”Ÿæˆæ ¸å¿ƒ

ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹åˆ†æé¡µé¢ç»“æ„ï¼Œç”Ÿæˆç‹¬ç«‹å¯è¿è¡Œçš„Pythonçˆ¬è™«è„šæœ¬ã€‚
å¢å¼ºç‰ˆï¼šæ”¯æŒSPAé¡µé¢åˆ†ç±»å‚æ•°è¯†åˆ«å’Œå¤„ç†ã€‚

æ¶æ„å¢å¼º v2.0:
- é›†æˆ Validator + Signals Collector + Failure Classifier
- ç»“æ„åŒ–é”™è¯¯æ¡ˆä¾‹ Few-shot æ³¨å…¥
- è‡ªåŠ¨ä¿®å¤å¾ªç¯

æ¶æ„å¢å¼º v3.0:
- æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾ç‰‡é™„ä»¶ï¼‰
- æ”¯æŒ Gemini API
- æ–°é—»èˆ†æƒ…åœºæ™¯æ”¯æŒ

æ¶æ„å¢å¼º v3.1:
- æ”¯æŒ Anthropic Claude APIï¼ˆClaude Sonnet/Opusï¼‰
"""
import json
import re
import base64
import codecs
import requests
from bs4 import BeautifulSoup, Comment
from typing import Dict, Any, List, Optional, Tuple
from openai import OpenAI
from pathlib import Path

# å¯¼å…¥å¢å¼ºæ¨¡å—
try:
    from error_cases import get_error_cases_prompt, ErrorSeverity
    from validator import StaticCodeValidator, validate_code
    from failure_classifier import FailureClassifier, FailureReport, FailureType
    from post_processor import apply_conditional_post_processing
except ImportError:
    # ä½œä¸ºåŒ…å¯¼å…¥æ—¶ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
    from .error_cases import get_error_cases_prompt, ErrorSeverity
    from .validator import StaticCodeValidator, validate_code
    from .failure_classifier import FailureClassifier, FailureReport, FailureType
    from .post_processor import apply_conditional_post_processing


# é™„ä»¶æ•°æ®ç±»å‹
class AttachmentData:
    """å›¾ç‰‡/æ–‡ä»¶é™„ä»¶"""
    def __init__(self, filename: str, base64_data: str, mime_type: str):
        self.filename = filename
        self.base64 = base64_data
        self.mime_type = mime_type


class LLMAgent:
    """LLMæ™ºèƒ½ä»£ç† - çˆ¬è™«è„šæœ¬ç”Ÿæˆå™¨ï¼ˆå¢å¼ºç‰ˆ v3.0 - å¤šæ¨¡æ€æ”¯æŒï¼‰"""

    def __init__(
        self,
        api_key: str,
        model: str = "qwen-max",
        base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
        max_repair_attempts: int = 2,
        enable_error_cases: bool = True,
        enable_auto_repair: bool = True,
        provider: str = "openai"  # 'openai' | 'gemini' | 'claude'
    ):
        """
        åˆå§‹åŒ–LLMä»£ç†

        Args:
            api_key: API Key
            model: æ¨¡å‹åç§°
            base_url: APIåŸºç¡€URL
            max_repair_attempts: æœ€å¤§ä¿®å¤å°è¯•æ¬¡æ•°
            enable_error_cases: æ˜¯å¦åœ¨ prompt ä¸­æ³¨å…¥é”™è¯¯æ¡ˆä¾‹
            provider: API æä¾›å•† ('openai' å…¼å®¹æ¨¡å¼ã€'gemini' æˆ– 'claude')
        """
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        self.max_repair_attempts = max_repair_attempts
        self.enable_error_cases = enable_error_cases
        # æ˜¯å¦å¯ç”¨â€œéªŒè¯+è‡ªåŠ¨ä¿®å¤å¾ªç¯â€ã€‚å…³é—­æ—¶å°†ç›´æ¥è¿”å›æ¨¡å‹åŸå§‹ç”Ÿæˆä»£ç ã€‚
        self.enable_auto_repair = enable_auto_repair
        self.provider = provider
        # ç”¨äºè°ƒè¯•ï¼šæ ‡è®°å½“å‰è°ƒç”¨æ‰€å±ä»»åŠ¡ï¼ˆç”±ä¸Šå±‚ä¼ å…¥ï¼‰
        self._task_id: Optional[str] = None

        # æ£€æµ‹ API æä¾›å•†
        if 'gemini' in model.lower() or 'generativelanguage.googleapis.com' in base_url:
            self.provider = 'gemini'
            self.client = None  # Gemini ä½¿ç”¨ REST API
        elif 'claude' in model.lower() or 'anthropic.com' in base_url:
            self.provider = 'claude'
            self.client = None  # Claude ä½¿ç”¨ REST API
        else:
            self.provider = 'openai'
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )

        # Tokenç»Ÿè®¡
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0
        
        # éªŒè¯å™¨å’Œåˆ†ç±»å™¨
        self.code_validator = StaticCodeValidator()
        self.failure_classifier = FailureClassifier(llm_client=self.client)

    def _dbg_prefix(self) -> str:
        """ç»Ÿä¸€çš„è°ƒè¯•å‰ç¼€ï¼Œä¾¿äºåŒºåˆ†å¹¶å‘ä»»åŠ¡æ—¥å¿—"""
        return f"[DEBUG][task={self._task_id}]" if self._task_id else "[DEBUG]"

    def _call_llm(
        self,
        system_prompt: str,
        user_prompt: str,
        attachments: Optional[List[AttachmentData]] = None,
        temperature: float = 0.2
    ) -> str:
        """
        ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£ï¼Œæ”¯æŒ OpenAI å…¼å®¹æ¨¡å¼ã€Gemini API å’Œ Claude API
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            attachments: å›¾ç‰‡é™„ä»¶åˆ—è¡¨ï¼ˆå¤šæ¨¡æ€ï¼‰
            temperature: ç”Ÿæˆæ¸©åº¦
            
        Returns:
            LLM ç”Ÿæˆçš„æ–‡æœ¬
        """
        if self.provider == 'gemini':
            return self._call_gemini(system_prompt, user_prompt, attachments, temperature)
        elif self.provider == 'claude':
            return self._call_claude(system_prompt, user_prompt, attachments, temperature)
        else:
            return self._call_openai(system_prompt, user_prompt, attachments, temperature)
    
    def _call_openai(
        self,
        system_prompt: str,
        user_prompt: str,
        attachments: Optional[List[AttachmentData]] = None,
        temperature: float = 0.2
    ) -> str:
        """OpenAI å…¼å®¹æ¨¡å¼è°ƒç”¨"""
        print(f"{self._dbg_prefix()} æ­£åœ¨è°ƒç”¨ OpenAI å…¼å®¹ API (model={self.model})...")
        
        # æ˜¾ç¤ºé™„ä»¶æ¥æ”¶çŠ¶æ€
        if attachments:
            print(f"{self._dbg_prefix()} âœ“ æ”¶åˆ° {len(attachments)} ä¸ªé™„ä»¶:")
            for i, att in enumerate(attachments):
                size_kb = len(att.base64) * 3 // 4 // 1024
                print(f"{self._dbg_prefix()}   [{i+1}] {att.filename} ({att.mime_type}, ~{size_kb}KB)")
        else:
            print(f"{self._dbg_prefix()} âš  æœªæ”¶åˆ°ä»»ä½•é™„ä»¶/æˆªå›¾")
        
        messages = [
            {"role": "system", "content": system_prompt}
        ]
        
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
        if attachments:
            user_content = [{"type": "text", "text": user_prompt}]
            for att in attachments:
                if att.mime_type.startswith('image/'):
                    user_content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{att.mime_type};base64,{att.base64}"
                        }
                    })
            messages.append({"role": "user", "content": user_content})
        else:
            messages.append({"role": "user", "content": user_prompt})
        
        # kimi-k2.5 æ¨¡å‹è¦æ±‚ temperature=1ï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨ä¼ å…¥çš„ temperature
        final_temperature = 1.0 if self.model == "kimi-k2.5" else temperature
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=final_temperature
        )
        
        # ç»Ÿè®¡ Token
        if hasattr(response, 'usage') and response.usage:
            self.total_prompt_tokens += response.usage.prompt_tokens
            self.total_completion_tokens += response.usage.completion_tokens
        
        return response.choices[0].message.content
    
    def _call_gemini(
        self,
        system_prompt: str,
        user_prompt: str,
        attachments: Optional[List[AttachmentData]] = None,
        temperature: float = 0.2
    ) -> str:
        """Gemini API è°ƒç”¨ï¼ˆREST APIï¼‰- æ”¯æŒæµå¼å“åº”ä»¥é¿å…è¶…æ—¶"""
        # ä½¿ç”¨ streamGenerateContent æ¥å£
        url = f"{self.base_url}models/{self.model}:streamGenerateContent?key={self.api_key}"
        
        print(f"{self._dbg_prefix()} Gemini API URL: {url[:80]}... (æµå¼æ¨¡å¼)")
        
        # æ˜¾ç¤ºé™„ä»¶æ¥æ”¶çŠ¶æ€
        if attachments:
            print(f"{self._dbg_prefix()} âœ“ æ”¶åˆ° {len(attachments)} ä¸ªé™„ä»¶:")
            for i, att in enumerate(attachments):
                size_kb = len(att.base64) * 3 // 4 // 1024
                print(f"{self._dbg_prefix()}   [{i+1}] {att.filename} ({att.mime_type}, ~{size_kb}KB)")
        else:
            print(f"{self._dbg_prefix()} âš  æœªæ”¶åˆ°ä»»ä½•é™„ä»¶/æˆªå›¾")
        
        # æ„å»ºè¯·æ±‚ä½“ - ä½¿ç”¨ Gemini çš„æ ‡å‡†æ ¼å¼
        payload = {
            "contents": [{
                "parts": [{"text": user_prompt}]
            }],
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": 16384
            }
        }
        
        # æ·»åŠ ç³»ç»ŸæŒ‡ä»¤ï¼ˆGemini 2.0+ æ”¯æŒ systemInstructionï¼‰
        payload["systemInstruction"] = {
            "parts": [{"text": system_prompt}]
        }
        
        # æ·»åŠ å›¾ç‰‡åˆ°å†…å®¹ä¸­
        if attachments:
            for att in attachments:
                if att.mime_type.startswith('image/'):
                    payload["contents"][0]["parts"].append({
                        "inlineData": {
                            "mimeType": att.mime_type,
                            "data": att.base64
                        }
                    })
                    print(f"[DEBUG] æ·»åŠ å›¾ç‰‡é™„ä»¶: {att.mime_type}")
        
        headers = {"Content-Type": "application/json"}
        
        try:
            print(f"[DEBUG] æ­£åœ¨è°ƒç”¨ Gemini API (model={self.model}, stream=True)...")
            # å¼€å¯ stream=True å»ºç«‹é•¿è¿æ¥
            response = requests.post(url, headers=headers, json=payload, timeout=180, stream=True)
            
            print(f"[DEBUG] Gemini å“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code != 200:
                # å¦‚æœçŠ¶æ€ç é”™è¯¯ï¼Œå°è¯•è¯»å–éƒ¨åˆ†å†…å®¹ä½œä¸ºé”™è¯¯ä¿¡æ¯
                try:
                    error_text = response.text[:500]
                except:
                    error_text = "Unknown error"
                raise Exception(f"Gemini API error: {response.status_code} - {error_text}")
            
            # å¢é‡æ¥æ”¶å¹¶æ‹¼è£…å“åº”
            full_text = ""
            chunk_count = 0
            
            # Gemini æµå¼è¿”å›çš„æ˜¯ JSON æ•°ç»„ç»“æ„: [{...}, {...}]
            # requests.iter_lines() ä¼šæŒ‰è¡Œè¯»å–ï¼Œæˆ‘ä»¬éœ€è¦è§£æè¿™äº› JSON å¯¹è±¡
            # æ ¼å¼é€šå¸¸æ˜¯: "[" (ç¬¬ä¸€è¡Œ), "{" (å¯¹è±¡å¼€å§‹)...
            # ä½† REST API å¯èƒ½è¿”å›ç´§å‡‘çš„ JSON æ•°ç»„ã€‚
            # æ›´ç¨³å¦¥çš„æ–¹å¼æ˜¯ï¼šæ‰‹åŠ¨å¤„ç† JSON å¯¹è±¡æµã€‚
            
            # ç®€å•å¤„ç†ï¼šç´¯ç§¯æ‰€æœ‰æ–‡æœ¬ç„¶åè§£æï¼Œæˆ–è€…å°è¯•å¢é‡è§£æ
            # ç”±äº iter_lines å¤„ç† JSON æ•°ç»„æ¯”è¾ƒéº»çƒ¦ï¼Œæˆ‘ä»¬å…ˆå°è¯•ç›´æ¥è§£æ response.json() 
            # ä½† response.json() ä¼šç­‰å¾…æ•´ä¸ªå“åº”ç»“æŸï¼Œå¯èƒ½æ— æ³•è§£å†³è¶…æ—¶é—®é¢˜ã€‚
            # æ­£ç¡®åšæ³•æ˜¯å¤„ç† streamã€‚Gemini çš„ REST stream è¿”å›ä¸€ç³»åˆ— JSON å¯¹è±¡ï¼Œä»¥é€—å·åˆ†éš”ï¼ŒåŒ…è£¹åœ¨ [] ä¸­ã€‚
            
            buffer = ""
            # ä½¿ç”¨å¢é‡è§£ç å™¨å¤„ç†å¯èƒ½è¢«åˆ†å‰²çš„å¤šå­—èŠ‚ UTF-8 å­—ç¬¦
            decoder = codecs.getincrementaldecoder('utf-8')('replace')
            for chunk in response.iter_content(chunk_size=None):
                if not chunk:
                    continue
                # å¢é‡è§£ç ï¼šè‡ªåŠ¨å¤„ç†è·¨ chunk çš„å¤šå­—èŠ‚å­—ç¬¦
                chunk_str = decoder.decode(chunk, final=False)
                buffer += chunk_str
                
                # å°è¯•ä» buffer ä¸­æå–å®Œæ•´çš„ JSON å¯¹è±¡
                # æ³¨æ„ï¼šGemini è¿”å›çš„æ˜¯ä¸€ä¸ª JSON åˆ—è¡¨ï¼Œé¦–å°¾æœ‰ [ ]ï¼Œä¸­é—´ç”¨ , åˆ†éš”
                # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å¢é‡è§£æå™¨
                while True:
                    # æŸ¥æ‰¾å¯èƒ½çš„ JSON å¯¹è±¡ç»“æŸä½ç½®
                    # å®é™…æµä¸­æ¯ä¸ª chunk å¾€å¾€æ˜¯ä¸€ä¸ªå®Œæ•´çš„ candidate å¯¹è±¡ï¼ˆä½†ä¹Ÿå¯èƒ½è¢«æˆªæ–­ï¼‰
                    # ä¸ºäº†ç¨³å¥ï¼Œæˆ‘ä»¬è¿™é‡Œåšä¸€ä¸ªç®€å•çš„å…¨é‡ç´¯ç§¯ï¼Œå› ä¸º requests.stream=True å·²ç»
                    # ä¿è¯äº†è¿æ¥æ˜¯æ´»è·ƒçš„ (Active)ï¼Œä¸ä¼šå› ä¸º TTFB è¿‡é•¿è¢«æ–­å¼€ã€‚
                    # åªè¦æ•°æ®åœ¨ä¼ è¾“ï¼Œä»£ç†å°±ä¸ä¼šæ–­å¼€ã€‚
                    # æ‰€ä»¥æˆ‘ä»¬å¯ä»¥åªç´¯ç§¯ bufferï¼Œæœ€åä¸€æ¬¡æ€§è§£æï¼ˆæˆ–è€…åˆ†å—è§£æä»¥æ˜¾ç¤ºè¿›åº¦ï¼‰ã€‚
                    break
            
            # åˆ·æ–°è§£ç å™¨ä¸­å¯èƒ½æ®‹ç•™çš„å­—èŠ‚
            buffer += decoder.decode(b'', final=True)
            
            # æµä¼ è¾“å®Œæˆåï¼Œbuffer ä¸­åŒ…å«å®Œæ•´çš„ JSON æ•°ç»„å­—ç¬¦ä¸²
            try:
                # æ¸…ç†å¯èƒ½çš„å‰åç©ºç™½
                json_str = buffer.strip()
                # å°è¯•ä¿®æ­£å¯èƒ½çš„æˆªæ–­ï¼ˆè™½ç„¶ stream=True åº”è¯¥æ¥æ”¶å®Œæ•´çš„ï¼‰
                results = json.loads(json_str)
            except json.JSONDecodeError as e:
                # å°è¯•å¤„ç†å¸¸è§çš„æµå¼æ ¼å¼é—®é¢˜ï¼ˆå¦‚ç¼ºå°‘é—­åˆæ‹¬å·ï¼‰
                print(f"[WARN] JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
                if not json_str.endswith(']'):
                    json_str += ']'
                try:
                    results = json.loads(json_str)
                except:
                    raise Exception(f"Gemini å“åº”æµè§£æå¤±è´¥ (é•¿åº¦: {len(buffer)})")

            # éå†æ‰€æœ‰å€™é€‰é¡¹æ‹¼è£…å®Œæ•´æ–‡æœ¬
            for result in results:
                # æ£€æŸ¥æ˜¯å¦æœ‰å®‰å…¨é˜»æ­¢
                if 'promptFeedback' in result:
                    feedback = result['promptFeedback']
                    if feedback.get('blockReason'):
                        print(f"[WARN] éƒ¨åˆ†å†…å®¹è¢«é˜»æ­¢: {feedback.get('blockReason')}")
                
                if 'candidates' in result and len(result['candidates']) > 0:
                    candidate = result['candidates'][0]
                    if 'content' in candidate and 'parts' in candidate['content']:
                        text_parts = [p.get('text', '') for p in candidate['content']['parts'] if 'text' in p]
                        full_text += ''.join(text_parts)
            
            # ç»Ÿè®¡å¤§è‡´ token æ•°ï¼ˆä¼°ç®—ï¼‰
            estimated_tokens = len(full_text) // 4
            self.total_completion_tokens += estimated_tokens
            print(f"[DEBUG] Gemini ç”Ÿæˆæ–‡æœ¬æ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
            
            if not full_text:
                raise Exception("Gemini å“åº”ä¸ºç©º")
                
            return full_text
            
        except requests.exceptions.Timeout:
            raise Exception("Gemini API è¯·æ±‚è¶…æ—¶ï¼ˆ180ç§’ï¼‰")
        except requests.exceptions.ConnectionError as e:
            raise Exception(f"Gemini API è¿æ¥é”™è¯¯: {e}")

    def _call_claude(
        self,
        system_prompt: str,
        user_prompt: str,
        attachments: Optional[List[AttachmentData]] = None,
        temperature: float = 0.2
    ) -> str:
        """Anthropic Claude API è°ƒç”¨ï¼ˆREST APIï¼‰"""
        # Claude API ç«¯ç‚¹
        url = "https://api.anthropic.com/v1/messages"
        
        print(f"{self._dbg_prefix()} Claude API URL: {url}")
        print(f"{self._dbg_prefix()} æ­£åœ¨è°ƒç”¨ Claude API (model={self.model})...")
        
        # æ˜¾ç¤ºé™„ä»¶æ¥æ”¶çŠ¶æ€
        if attachments:
            print(f"{self._dbg_prefix()} âœ“ æ”¶åˆ° {len(attachments)} ä¸ªé™„ä»¶:")
            for i, att in enumerate(attachments):
                # è®¡ç®—å›¾ç‰‡å¤§å°ï¼ˆbase64 è½¬å®é™…å¤§å°çº¦ 3/4ï¼‰
                size_kb = len(att.base64) * 3 // 4 // 1024
                print(f"{self._dbg_prefix()}   [{i+1}] {att.filename} ({att.mime_type}, ~{size_kb}KB)")
        else:
            print(f"{self._dbg_prefix()} âš  æœªæ”¶åˆ°ä»»ä½•é™„ä»¶/æˆªå›¾")
        
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.api_key,
            "anthropic-version": "2023-06-01"
        }
        
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        user_content = []
        image_count = 0
        
        # æ·»åŠ å›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if attachments:
            for att in attachments:
                if att.mime_type.startswith('image/'):
                    user_content.append({
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": att.mime_type,
                            "data": att.base64
                        }
                    })
                    image_count += 1
            print(f"{self._dbg_prefix()} âœ“ å·²å°† {image_count} å¼ å›¾ç‰‡æ·»åŠ åˆ° Claude è¯·æ±‚ä¸­")
        
        # æ·»åŠ æ–‡æœ¬
        user_content.append({
            "type": "text",
            "text": user_prompt
        })
        
        payload = {
            "model": self.model,
            "max_tokens": 16384,
            "temperature": temperature,
            "system": system_prompt,
            "messages": [
                {"role": "user", "content": user_content}
            ]
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=180)
            
            print(f"{self._dbg_prefix()} Claude å“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code != 200:
                error_text = response.text[:500] if len(response.text) > 500 else response.text
                raise Exception(f"Claude API error: {response.status_code} - {error_text}")
            
            result = response.json()
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            content = result.get("content", [])
            if content and len(content) > 0:
                text_parts = [block.get("text", "") for block in content if block.get("type") == "text"]
                generated_text = ''.join(text_parts)
                
                # ç»Ÿè®¡ Token
                usage = result.get("usage", {})
                if usage:
                    self.total_prompt_tokens += usage.get("input_tokens", 0)
                    self.total_completion_tokens += usage.get("output_tokens", 0)
                
                print(f"{self._dbg_prefix()} Claude ç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(generated_text)} å­—ç¬¦")
                return generated_text
            
            raise Exception(f"Claude API response format error: {str(result)[:500]}")
            
        except requests.exceptions.Timeout:
            raise Exception("Claude API è¯·æ±‚è¶…æ—¶ï¼ˆ180ç§’ï¼‰")
        except requests.exceptions.ConnectionError as e:
            raise Exception(f"Claude API è¿æ¥é”™è¯¯: {e}")

    def generate_crawler_script(
        self,
        page_url: str,
        page_html: str,
        page_structure: Dict[str, Any],
        network_requests: Dict[str, List[Dict[str, Any]]],
        user_requirements: Optional[str] = None,
        start_date: str = "",
        end_date: str = "",
        enhanced_analysis: Optional[Dict[str, Any]] = None,
        attachments: Optional[List[AttachmentData]] = None,
        run_mode: str = "enterprise_report",
        crawl_mode: str = "single_page",
        task_id: Optional[str] = None
    ) -> str:
        """
        åˆ†æé¡µé¢å¹¶ç”Ÿæˆç‹¬ç«‹å¯è¿è¡Œçš„Pythonçˆ¬è™«è„šæœ¬

        Args:
            page_url: ç›®æ ‡é¡µé¢URL
            page_html: å®Œæ•´çš„é¡µé¢HTML
            page_structure: é¡µé¢ç»“æ„åˆ†æç»“æœ
            network_requests: æ•è·çš„ç½‘ç»œè¯·æ±‚
            user_requirements: ç”¨æˆ·é¢å¤–éœ€æ±‚ï¼ˆå¯é€‰ï¼‰
            start_date: çˆ¬å–å¼€å§‹æ—¶é—´ï¼ˆYYYY-MM-DDï¼‰
            end_date: çˆ¬å–ç»“æŸæ—¶é—´ï¼ˆYYYY-MM-DDï¼‰
            enhanced_analysis: å¢å¼ºåˆ†æç»“æœï¼ˆåŒ…å«æ•°æ®çŠ¶æ€ã€äº¤äº’APIã€å‚æ•°åˆ†æï¼‰
            attachments: å›¾ç‰‡é™„ä»¶ï¼ˆå¤šæ¨¡æ€è¾“å…¥ï¼Œç”¨æˆ·æä¾›çš„æˆªå›¾ï¼‰
            run_mode: è¿è¡Œæ¨¡å¼ ('enterprise_report' | 'news_sentiment')
            crawl_mode: çˆ¬å–æ¨¡å¼ ('single_page' | 'multi_page' | 'auto_detect')

        Returns:
            ç”Ÿæˆçš„Pythonçˆ¬è™«è„šæœ¬ä»£ç 
        """

        # è®°å½•ä»»åŠ¡IDï¼ˆä¾¿äºåŒºåˆ†å¹¶å‘ä»»åŠ¡æ—¥å¿—ï¼‰
        self._task_id = task_id

        # å‡†å¤‡APIè¯·æ±‚ä¿¡æ¯ï¼ˆé‡ç‚¹å…³æ³¨ï¼‰
        api_info = self._extract_api_info(network_requests, enhanced_analysis)

        # å‡†å¤‡é¡µé¢ç»“æ„æ‘˜è¦
        structure_summary = self._summarize_structure(page_structure)
        
        # å‡†å¤‡å¢å¼ºåˆ†ææ‘˜è¦
        enhanced_summary = self._summarize_enhanced_analysis(enhanced_analysis) if enhanced_analysis else ""

        system_prompt = self._build_system_prompt(run_mode=run_mode, crawl_mode=crawl_mode)
        user_prompt = self._build_user_prompt(
            page_url=page_url,
            page_html=page_html,
            structure_summary=structure_summary,
            api_info=api_info,
            user_requirements=user_requirements,
            start_date=start_date,
            end_date=end_date,
            enhanced_summary=enhanced_summary
        )

        def _needs_rendered_dom_dates() -> bool:
            """å½“ API æ— æœ‰æ•ˆæ—¥æœŸä¸”é¡µé¢æç¤ºå­˜åœ¨â€œæ—¥æœŸ-æ¡ç›®å…³è”æ ·æœ¬â€/SPAçº¿ç´¢æ—¶ï¼Œå¼ºåˆ¶è¦æ±‚ç”¨æµè§ˆå™¨æå–æ—¥æœŸã€‚"""
            try:
                has_samples = bool((page_structure or {}).get("dateItemSamples"))
                spa = (page_structure or {}).get("spaHints") or {}
                is_spa = bool(spa.get("hasHashRoute") or spa.get("hasAppRoot"))
                return has_samples or is_spa
            except Exception:
                return False

        def _script_looks_wrong_for_spa_dates(code: str) -> bool:
            """
            è½»é‡è´¨é‡é—¸é—¨ï¼šå¦‚æœéœ€è¦æ¸²æŸ“åDOMæ—¥æœŸï¼Œå´ç”Ÿæˆäº†â€œrequests æŠ“ç«™ç‚¹ä¸»é¡µ/æ­£åˆ™ span.list-timeâ€ç­‰å…¸å‹é”™è¯¯æ¨¡å¼ï¼Œåˆ™è§¦å‘ä¸€æ¬¡é‡è¯•ã€‚
            åªåšéå¸¸ä¿å®ˆçš„åˆ¤æ–­ï¼Œé¿å…è¯¯ä¼¤æ­£å¸¸è„šæœ¬ã€‚
            """
            if not code:
                return True
            c = code.lower()
            # æ˜æ˜¾ç”¨ requests å»æŠ“ä¸»é¡µ/æ ¹è·¯å¾„æ¥æŠ½æ—¥æœŸï¼ˆSPAå¸¸è§å¤±è´¥ï¼‰
            bad_requests_html = ("requests.get(" in c) and ("list-time" in c or "span.list-time" in c)
            # æ²¡æœ‰ä»»ä½• playwright ç›¸å…³ import/ä½¿ç”¨
            no_playwright = ("playwright" not in c) and ("sync_playwright" not in c) and ("async_playwright" not in c)
            # åŒæ—¶å‡ºç°â€œä»htmlæå–æ—¥æœŸâ€çš„è¯­ä¹‰
            mentions_html_dates = ("html" in c and "date" in c) or ("ä»html" in code)
            return bad_requests_html and no_playwright and mentions_html_dates

        def _script_looks_like_wrong_output_schema(code: str) -> bool:
            """
            è½»é‡è´¨é‡é—¸é—¨ï¼šè¾“å‡º schema å¿…é¡»åŒ…å« name å­—æ®µã€‚
            å¦‚æœè„šæœ¬æ˜æ˜¾åœ¨ reports é‡Œå†™å…¥ title è€Œä¸æ˜¯ nameï¼Œè§¦å‘ä¸€æ¬¡é‡è¯•ä»¥æå‡æ³›åŒ–ç¨³å®šæ€§ã€‚
            """
            if not code:
                return True
            c = code.lower()
            # å¸¸è§é”™è¯¯ï¼šæŠŠè®°å½•å†™æˆ {"title": ...} è€Œä¸æ˜¯ {"name": ...}
            uses_title_key = '"title"' in c or "'title'" in c
            uses_name_key = '"name"' in c or "'name'" in c
            # å…¼å®¹ç­–ç•¥ï¼štitle å¯è§†ä¸º name çš„åˆ«åï¼ˆåç«¯è§£æä¼šå½’ä¸€åŒ–ä¸º nameï¼‰ï¼Œä¸åº”è§¦å‘é‡è¯•/ä¿®å¤
            # ä»ä¿ç•™åŸæ„ï¼šå¦‚æœæ—¢æ²¡æœ‰ name ä¹Ÿæ²¡æœ‰ title æ‰ç®—â€œschema å¯ç–‘â€
            if uses_name_key or uses_title_key:
                return False
            return True

        def _script_looks_like_keeps_undated_records(code: str) -> bool:
            """
            è´¨é‡é—¸é—¨ï¼šå½“è¦æ±‚æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤æ—¶ï¼Œè„šæœ¬ä¸åº”â€œä¿ç•™æ— æ—¥æœŸè®°å½•â€ã€‚
            è¿™é‡Œç”¨å¯å‘å¼æ£€æµ‹å…¸å‹é”™è¯¯åˆ†æ”¯ï¼šelif not date_str: ... append(...)
            """
            if not code:
                return False
            c = code.lower()
            patterns = [
                "elif not date_str",
                "if not date_str",
                "date_str and start_date <= date_str <= end_date",
                "filtered_reports.append(report)",
                "all_reports.append(report)",
                "å·²ä¿ç•™",
                "æ— æ—¥æœŸ",
            ]
            hit = sum(1 for p in patterns if p in c)
            # ä¿å®ˆï¼šåŒæ—¶å‡ºç°â€œæ— æ—¥æœŸ/ä¿ç•™â€+ append æ›´å¯ç–‘
            return ("æ— æ—¥æœŸ" in code and "append(" in c) or hit >= 5

        def _script_looks_like_brittle_html_parsing(code: str) -> bool:
            """
            è´¨é‡é—¸é—¨ï¼šæ‹¦æˆªå…¸å‹çš„ BeautifulSoup é“¾å¼ find().find_all() ç©ºæŒ‡é’ˆå†™æ³•ï¼Œ
            é¿å…è¿è¡Œæ—¶æŠ¥ï¼š'NoneType' object has no attribute 'find_all'ã€‚
            """
            if not code:
                return False
            c = code.lower()
            # æå¸¸è§å‘ï¼štable.find('tbody').find_all('tr')
            if ".find('tbody').find_all(" in c or ".find(\"tbody\").find_all(" in c:
                return True
            # æ›´æ³›åŒ–ï¼šä»»æ„ .find(...).find_all(...) é“¾å¼
            if ".find(" in c and ").find_all(" in c:
                # åªè¦è„šæœ¬é‡ŒåŒæ—¶ç”¨åˆ°äº† bs4/BeautifulSoupï¼Œå°±è®¤ä¸ºé£é™©å¾ˆé«˜
                if "beautifulsoup" in c or "from bs4 import" in c:
                    return True
            return False

        def _script_looks_like_hardcoded_date_column(code: str) -> bool:
            """
            è´¨é‡é—¸é—¨ï¼šæ‹¦æˆªç¡¬ç¼–ç åˆ—ç´¢å¼•æå–æ—¥æœŸçš„è„†å¼±å†™æ³•ã€‚
            ä¸åŒç½‘ç«™è¡¨æ ¼ç»“æ„å·®å¼‚å¤§ï¼Œæ—¥æœŸå¯èƒ½åœ¨ä»»æ„åˆ—ä½ç½®ã€‚
            """
            if not code:
                return False
            
            # å¦‚æœä½¿ç”¨äº†æ³¨å…¥çš„æ™ºèƒ½æ—¥æœŸæ‰«æå‡½æ•°ï¼Œåˆ™è®¤ä¸ºæ˜¯å®‰å…¨çš„
            if "_pygen_smart_find_date_in_row" in code:
                return False
            
            # æ£€æµ‹ç¡¬ç¼–ç åˆ—ç´¢å¼•æå–æ—¥æœŸçš„æ¨¡å¼
            import re
            # åŒ¹é… tds[æ•°å­—] åè·Ÿæ—¥æœŸç›¸å…³æ“ä½œ
            hardcoded_patterns = [
                r"tds\[\d+\]\.query_selector\(['\"]span['\"]\)",  # Playwright: tds[4].query_selector('span')
                r"tds\[\d+\]\.select_one\(['\"]span['\"]\)",      # BS4: tds[4].select_one('span')
                r"tds\[\d+\]\.get_text\(",                        # tds[3].get_text()
                r"tds\[\d+\]\.inner_text\(",                      # tds[4].inner_text()
            ]
            
            for pattern in hardcoded_patterns:
                if re.search(pattern, code):
                    # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦æ˜¯æ—¥æœŸæå–ä¸Šä¸‹æ–‡
                    # æŸ¥æ‰¾é™„è¿‘æ˜¯å¦æœ‰ date ç›¸å…³å˜é‡å
                    if "date" in code.lower():
                        return True
            
            return False

        try:
            # =====================================================================
            # ç¬¬ä¸€æ¬¡ç”Ÿæˆï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
            # =====================================================================
            # å¦‚æœæœ‰å›¾ç‰‡é™„ä»¶ï¼Œæ·»åŠ è¯¦ç»†çš„æç¤ºè¯´æ˜
            if attachments:
                attachment_hint = """

## ã€é‡è¦ã€‘ç”¨æˆ·æä¾›çš„å‚è€ƒæˆªå›¾

ç”¨æˆ·å·²æä¾›é¡µé¢æˆªå›¾ï¼Œè¯·ä»”ç»†åˆ†æï¼š

**è¯†åˆ«æ¡†é€‰/æ ‡æ³¨åŒºåŸŸ**ï¼š
   - å¦‚æœæˆªå›¾ä¸­æœ‰çº¢è‰²æ¡†ã€é«˜äº®åŒºåŸŸã€ç®­å¤´æŒ‡å‘æˆ–å…¶ä»–æ ‡æ³¨ï¼Œè¿™è¡¨ç¤ºç”¨æˆ·æƒ³è¦çˆ¬å–çš„**å…·ä½“åŒºåŸŸ**ï¼Œè¯¥åŒºåŸŸé€šå¸¸æ˜¯ç½‘é¡µä¸­çš„ä¸€ä¸ªæ¿å—æˆ–åˆ—è¡¨ã€è¡¨æ ¼ï¼Œç”Ÿæˆçš„ä»£ç åº”è¯¥**ç²¾ç¡®å®šä½**åˆ°è¯¥åŒºåŸŸçš„ CSS é€‰æ‹©å™¨æˆ– XPath
   - åªçˆ¬å–è¢«æ ‡æ³¨/æ¡†é€‰çš„åŒºåŸŸå†…å®¹ï¼Œ**ä¸è¦**çˆ¬å–é¡µé¢ä¸Šçš„å…¶ä»–åŒºåŸŸæˆ–æ¿å—
è¯·åœ¨ç”Ÿæˆä»£ç æ—¶ï¼Œå…ˆæè¿°ä½ ä»æˆªå›¾ä¸­è¯†åˆ«åˆ°çš„ç›®æ ‡åŒºåŸŸï¼Œå¹¶åœ¨ä»£ç æ³¨é‡Šä¸­è¯´æ˜ä½ è¯†åˆ«åˆ°çš„ç›®æ ‡åŒºåŸŸæ˜¯ä»€ä¹ˆï¼ˆå¯¹åº”ç½‘é¡µé¡µé¢ä¸­çš„ä»€ä¹ˆæ¿å—ï¼‰
"""
                # å°†æˆªå›¾çº¦æŸæ”¾åˆ° user_prompt æœ€å‰é¢ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
                user_prompt = attachment_hint.strip() + "\n\n" + user_prompt
                print(f"{self._dbg_prefix()} å·²æ·»åŠ æˆªå›¾æç¤ºåˆ° promptï¼Œé™„ä»¶æ•°é‡: {len(attachments)}")
            
            content = self._call_llm(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                attachments=attachments,
                temperature=0.2
            )
            
            script = self._extract_code_from_response(content)

            # =====================================================================
            # å¯é€‰ï¼šå…³é—­è‡ªåŠ¨ä¿®å¤/éªŒè¯ï¼ˆç›´æ¥è¿”å› LLM åŸå§‹ä»£ç ï¼‰
            # =====================================================================
            if not self.enable_auto_repair:
                print(f"{self._dbg_prefix()} âš  å·²å…³é—­è‡ªåŠ¨ä¿®å¤/éªŒè¯ï¼šç›´æ¥è¿”å› LLM åŸå§‹ç”Ÿæˆä»£ç ")
                return script

            # =====================================================================
            # æ­¥éª¤1ï¼šé¢„æ£€æŸ¥ï¼ˆè·å–é—®é¢˜åˆ—è¡¨ï¼Œç”¨äºå†³å®šåå¤„ç†ï¼‰
            # =====================================================================
            pre_issues = self.code_validator.validate(script, page_structure=page_structure)
            
            # =====================================================================
            # æ­¥éª¤2ï¼šæ¡ä»¶æ€§åå¤„ç†ï¼ˆæ ¹æ®é—®é¢˜å†³å®šæ³¨å…¥å“ªäº›å¢å¼ºä»£ç ï¼‰
            # =====================================================================
            # åå¤„ç†æ”¾åœ¨ LLM ä¿®å¤ä¹‹å‰ï¼Œä¸ºåç»­ä¿®å¤æä¾›å·¥å…·å‡½æ•°
            script, injection_log = apply_conditional_post_processing(
                script_code=script,
                issues=pre_issues,
                page_structure=page_structure
            )
            
            if injection_log:
                print(f"{self._dbg_prefix()} ğŸ”§ æ¡ä»¶æ€§åå¤„ç†ï¼š")
                for log in injection_log:
                    print(f"{self._dbg_prefix()}   - {log}")
            
            # =====================================================================
            # æ­¥éª¤3ï¼šLLM ä¿®å¤å¾ªç¯ï¼ˆåŸºäºåå¤„ç†åçš„ä»£ç ï¼‰
            # =====================================================================
            # æ£€æµ‹æ˜¯å¦æ•è·åˆ° API è¯·æ±‚
            api_requests = network_requests.get("api_requests", [])
            has_api_requests = len(api_requests) > 0
            
            script, repair_log = self._validate_and_repair(
                script=script,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                page_structure=page_structure,
                context_checks={
                    "needs_rendered_dom_dates": _needs_rendered_dom_dates(),
                    "has_api_requests": has_api_requests,
                }
            )
            
            # è®°å½•æ£€æŸ¥/ä¿®å¤æ—¥å¿—ï¼ˆå¯ç”¨äºåç»­åˆ†æï¼‰
            # æ³¨æ„ï¼šrepair_log æ—¢å¯èƒ½åŒ…å«â€œè§¦å‘ä¿®å¤â€çš„åŸå› ï¼Œä¹Ÿå¯èƒ½åªæ˜¯â€œå­˜åœ¨è­¦å‘Šä½†æœªä¿®å¤â€ã€‚
            if repair_log:
                print(f"{self._dbg_prefix()} ğŸ” ä»£ç æ£€æŸ¥/ä¿®å¤æ—¥å¿—ï¼š")
                for log in repair_log:
                    print(f"{self._dbg_prefix()}   - {log}")

            return script

        except Exception as e:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._generate_fallback_script(page_url, run_mode=run_mode)
    
    def _validate_and_repair(
        self,
        script: str,
        system_prompt: str,
        user_prompt: str,
        page_structure: Dict[str, Any],
        context_checks: Dict[str, bool]
    ) -> Tuple[str, List[str]]:
        """
        éªŒè¯ä»£ç å¹¶å°è¯•ä¿®å¤
        
        Args:
            script: ç”Ÿæˆçš„ä»£ç 
            system_prompt: ç³»ç»Ÿæç¤º
            user_prompt: ç”¨æˆ·æç¤º
            page_structure: é¡µé¢ç»“æ„
            context_checks: ä¸Šä¸‹æ–‡ç›¸å…³æ£€æŸ¥ç»“æœ
            
        Returns:
            (æœ€ç»ˆä»£ç , ä¿®å¤æ—¥å¿—åˆ—è¡¨)
        """
        repair_log = []
        current_script = script
        
        for attempt in range(self.max_repair_attempts):
            # ä½¿ç”¨éªŒè¯å™¨æ£€æŸ¥ä»£ç ï¼ˆä¼ å…¥ page_structure è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€æŸ¥ï¼‰
            issues = self.code_validator.validate(current_script, page_structure=page_structure)
            
            # åŠ å…¥ä¸Šä¸‹æ–‡ç›¸å…³çš„æ£€æŸ¥
            context_issues = self._check_context_issues(current_script, context_checks)
            
            # åˆå¹¶é—®é¢˜
            all_issues = issues + context_issues
            
            # å¦‚æœæ²¡æœ‰é”™è¯¯çº§åˆ«çš„é—®é¢˜ï¼Œè¿”å›å½“å‰ä»£ç 
            has_errors = any(i.severity.value == "error" for i in all_issues)
            if not has_errors:
                if all_issues:
                    # æœ‰è­¦å‘Šä½†æ— é”™è¯¯ï¼šæŠŠè­¦å‘Šä¹Ÿæ‰“å°å‡ºæ¥ï¼Œé¿å…ç”¨æˆ·è§‰å¾—â€œè«åå…¶å¦™ä¿®å¤/æ²¡åŸå› â€ã€‚
                    warnings = [i for i in all_issues if i.severity.value == "warning"]
                    infos = [i for i in all_issues if i.severity.value == "info"]
                    repair_log.append(
                        f"ç¬¬{attempt+1}è½®: æ£€æŸ¥é€šè¿‡ï¼ˆ{len(warnings)}ä¸ªè­¦å‘Š{'' if not infos else f'ï¼Œ{len(infos)}ä¸ªæç¤º'}ï¼‰"
                    )
                    for w in warnings[:10]:
                        msg = f"- [{w.code}] {w.message}"
                        if w.suggestion:
                            msg += f"ï¼ˆå»ºè®®: {w.suggestion}ï¼‰"
                        repair_log.append(msg)
                    for inf in infos[:5]:
                        msg = f"- [{inf.code}] {inf.message}"
                        if getattr(inf, "suggestion", ""):
                            msg += f"ï¼ˆå»ºè®®: {inf.suggestion}ï¼‰"
                        repair_log.append(msg)
                break
            
            # æœ‰é”™è¯¯ï¼Œå°è¯•ä¿®å¤
            error_issues = [i for i in all_issues if i.severity.value == "error"]
            repair_log.append(f"ç¬¬{attempt+1}è½®: å‘ç°{len(error_issues)}ä¸ªé”™è¯¯ï¼Œå°è¯•ä¿®å¤")
            # è®°å½•è§¦å‘ä¿®å¤çš„å…·ä½“åŸå› ï¼ˆä¾¿äºå®šä½ï¼‰
            if error_issues:
                repair_log.append("è§¦å‘ä¿®å¤çš„åŸå› ï¼š")
                for i in error_issues:
                    msg = f"- [{i.code}] {i.message}"
                    if i.suggestion:
                        msg += f"ï¼ˆå»ºè®®: {i.suggestion}ï¼‰"
                    repair_log.append(msg)
            
            # ç”Ÿæˆä¿®å¤æç¤ºï¼ˆä¼ å…¥ page_structureï¼Œè®© LLM çŸ¥é“æ­£ç¡®çš„é¡µé¢ç»“æ„ï¼‰
            repair_prompt = self._build_repair_prompt(all_issues, current_script, page_structure)
            
            # è°ƒç”¨ LLM ä¿®å¤
            try:
                repair_response = self._call_llm(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt + "\n\n" + repair_prompt,
                    attachments=None,  # ä¿®å¤æ—¶ä¸éœ€è¦å›¾ç‰‡
                    temperature=0.1  # ä½æ¸©åº¦æé«˜ç¨³å®šæ€§
                )
                
                new_script = self._extract_code_from_response(repair_response)
                
                if new_script and new_script != current_script:
                    current_script = new_script
                else:
                    repair_log.append(f"ç¬¬{attempt+1}è½®: ä¿®å¤æœªäº§ç”Ÿå˜åŒ–ï¼Œåœæ­¢")
                    break
                    
            except Exception as e:
                repair_log.append(f"ç¬¬{attempt+1}è½®: ä¿®å¤è°ƒç”¨å¤±è´¥ - {str(e)}")
                break
        
        return current_script, repair_log
    
    def _check_context_issues(
        self,
        code: str,
        context_checks: Dict[str, bool]
    ) -> List:
        """æ£€æŸ¥ä¸Šä¸‹æ–‡ç›¸å…³çš„é—®é¢˜"""
        from validator import CodeIssue, IssueSeverity
        
        issues = []
        code_lower = code.lower()
        
        # æ£€æŸ¥0: Windows å…¼å®¹æ€§ (Emoji æ£€æµ‹)
        # æ£€æµ‹ä»£ç ä¸­æ˜¯å¦åŒ…å«å¯èƒ½å¯¼è‡´ Windows GBK ç»ˆç«¯å´©æºƒçš„ Emoji å­—ç¬¦
        # ä¸»è¦æ˜¯è¶…å‡ºåŸºæœ¬å¤šè¯­è¨€å¹³é¢(BMP)çš„å­—ç¬¦ï¼Œå³ ord > 65535 (å¦‚ ğŸš€ \U0001f680)
        has_emoji = False
        for char in code:
            if ord(char) > 0xFFFF:
                has_emoji = True
                break
        
        if has_emoji and "print" in code_lower:
            issues.append(CodeIssue(
                code="WIN_COMPAT_001",
                severity=IssueSeverity.ERROR,
                message="ä»£ç ä¸­åŒ…å« Emoji æˆ–ç‰¹æ®Šå­—ç¬¦ (ord > 65535)ï¼Œåœ¨ Windows æ§åˆ¶å°è¾“å‡ºä¼šå¯¼è‡´ UnicodeEncodeError å´©æºƒã€‚",
                suggestion="è¯·ç§»é™¤ print() è¯­å¥ä¸­çš„æ‰€æœ‰ Emoji è¡¨æƒ…ï¼ˆå¦‚ ğŸš€, âœ…, âŒ ç­‰ï¼‰ï¼Œä»…ä½¿ç”¨æ–‡æœ¬ç¬¦å·ã€‚"
            ))

        # æ£€æŸ¥1: SPA æ—¥æœŸæå–é—®é¢˜
        if context_checks.get("needs_rendered_dom_dates"):
            # å¦‚æœéœ€è¦æ¸²æŸ“åDOMæ—¥æœŸï¼Œä½†ä»£ç ç”¨ requests æŠ“ HTML
            bad_requests_html = ("requests.get(" in code_lower) and ("list-time" in code_lower or "span.list-time" in code_lower)
            no_playwright = ("playwright" not in code_lower) and ("sync_playwright" not in code_lower)
            if bad_requests_html and no_playwright:
                issues.append(CodeIssue(
                    code="CTX_001",
                    severity=IssueSeverity.ERROR,
                    message="SPA é¡µé¢éœ€è¦ç”¨ Playwright æå–æ—¥æœŸï¼Œä½†ä»£ç ä½¿ç”¨äº† requests",
                    suggestion="æ”¹ç”¨ Playwright ä»æ¸²æŸ“å DOM æå–æ—¥æœŸ"
                ))
        
        # æ£€æŸ¥2: åˆ†é¡µåœºæ™¯ä¸‹æ—¥æœŸå¯èƒ½åªå¤„ç†ç¬¬ä¸€é¡µ
        if "for page" in code_lower or "while" in code_lower:
            if "extract_date" in code_lower:
                # ç®€å•å¯å‘ï¼šæ—¥æœŸæå–å‡½æ•°æ˜¯å¦åœ¨åˆ†é¡µå¾ªç¯å†…
                lines = code.split('\n')
                in_loop = False
                date_in_loop = False
                
                for line in lines:
                    if 'for ' in line.lower() and 'page' in line.lower():
                        in_loop = True
                    if in_loop and ('extract_date' in line.lower() or '_pygen_smart_find_date' in line):
                        date_in_loop = True
                        break
                
                if not date_in_loop and 'extract_date' in code_lower:
                    issues.append(CodeIssue(
                        code="CTX_002",
                        severity=IssueSeverity.WARNING,
                        message="æ—¥æœŸæå–å¯èƒ½ä¸åœ¨åˆ†é¡µå¾ªç¯å†…ï¼Œä¼šå¯¼è‡´åªæœ‰ç¬¬ä¸€é¡µæœ‰æ—¥æœŸ",
                        suggestion="åœ¨æ¯é¡µæ•°æ®è·å–æ—¶åŒæ­¥æå–æ—¥æœŸ"
                    ))
        
        # æ£€æŸ¥3: æœ‰ API è¯·æ±‚å¯ç”¨ä½†é”™è¯¯ä½¿ç”¨ HTML è§£æ
        if context_checks.get("has_api_requests"):
            # æ£€æµ‹æ˜¯å¦ä½¿ç”¨ BeautifulSoup è§£æ HTML è€Œä¸æ˜¯è°ƒç”¨ API
            uses_beautifulsoup = "beautifulsoup" in code_lower or "from bs4" in code_lower
            parses_table_html = "table" in code_lower and ("tbody" in code_lower or "tr" in code_lower)
            uses_requests_get_html = "requests.get(" in code_lower and ".text" in code_lower
            
            # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº† APIï¼ˆè¿”å› JSONï¼‰
            uses_api = ".json()" in code_lower
            
            # å¦‚æœä½¿ç”¨ BeautifulSoup è§£æ HTMLï¼Œä½†æœ‰ API å¯ç”¨ä¸”æ²¡æœ‰è°ƒç”¨ API
            if uses_beautifulsoup and parses_table_html and uses_requests_get_html and not uses_api:
                issues.append(CodeIssue(
                    code="ERR_011",
                    severity=IssueSeverity.ERROR,
                    message="æ£€æµ‹åˆ° API è¯·æ±‚å¯ç”¨ï¼Œä½†ä»£ç ä½¿ç”¨ BeautifulSoup è§£æ HTML è€Œéè°ƒç”¨ API",
                    suggestion="é¡µé¢æ•°æ®é€šè¿‡ API åŠ¨æ€åŠ è½½ï¼Œå¿…é¡»ä½¿ç”¨ requests è°ƒç”¨ API è·å– JSON æ•°æ®ï¼Œè€Œä¸æ˜¯è§£æ HTML"
                ))
        
        # æ£€æŸ¥4: CATEGORIES å­—å…¸ä¸­åˆ†ç±»å‚æ•°é‡å¤ï¼ˆè‡´å‘½é€»è¾‘é”™è¯¯ï¼‰
        # æ£€æµ‹æ¨¡å¼ï¼šCATEGORIES = { "xxx": {...}, "yyy": {...} } å…¶ä¸­å¤šä¸ªåˆ†ç±»çš„å‚æ•°å€¼å®Œå…¨ç›¸åŒ
        # è¿™ä¼šå¯¼è‡´è™½ç„¶éå†äº†å¤šä¸ªåˆ†ç±»ï¼Œä½†å®é™…åªè¯·æ±‚äº†åŒä¸€ä¸ªåˆ†ç±»çš„æ•°æ®
        if "categories" in code_lower and "for " in code_lower:
            import re
            # æå– CATEGORIES å­—å…¸å®šä¹‰
            categories_match = re.search(
                r'CATEGORIES\s*=\s*\{([^}]+(?:\{[^}]*\}[^}]*)*)\}',
                code,
                re.DOTALL | re.IGNORECASE
            )
            if categories_match:
                categories_block = categories_match.group(1)
                # æå–æ‰€æœ‰å­å­—å…¸ï¼ˆåˆ†ç±»å‚æ•°ï¼‰
                param_dicts = re.findall(r'\{([^}]+)\}', categories_block)
                if len(param_dicts) >= 2:
                    # æ ‡å‡†åŒ–å‚æ•°å­—å…¸ï¼ˆå»é™¤ç©ºæ ¼ã€å¼•å·å·®å¼‚ï¼‰ä»¥æ£€æµ‹é‡å¤
                    normalized_params = []
                    for pd in param_dicts:
                        # ç§»é™¤ç©ºæ ¼å’Œå¼•å·ç±»å‹å·®å¼‚
                        normalized = re.sub(r'[\s\'"]', '', pd.lower())
                        normalized_params.append(normalized)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤
                    unique_params = set(normalized_params)
                    if len(unique_params) == 1 and len(normalized_params) > 1:
                        # æ‰€æœ‰åˆ†ç±»çš„å‚æ•°å®Œå…¨ç›¸åŒï¼è¿™æ˜¯è‡´å‘½é”™è¯¯
                        issues.append(CodeIssue(
                            code="CAT_DUP_001",
                            severity=IssueSeverity.ERROR,
                            message=f"CATEGORIES å­—å…¸ä¸­çš„ {len(normalized_params)} ä¸ªåˆ†ç±»å‚æ•°å®Œå…¨ç›¸åŒï¼Œå®é™…åªä¼šè¯·æ±‚åŒä¸€ä¸ªåˆ†ç±»çš„æ•°æ®",
                            suggestion="æ¯ä¸ªåˆ†ç±»å¿…é¡»æœ‰ä¸åŒçš„å‚æ•°å€¼ï¼ˆå¦‚ä¸åŒçš„ levelthree/categoryIdï¼‰ã€‚è¯·æ£€æŸ¥ verified_category_mapping å¹¶ä½¿ç”¨æ­£ç¡®çš„åˆ†ç±» ID"
                        ))
                    elif len(unique_params) < len(normalized_params):
                        # éƒ¨åˆ†é‡å¤
                        dup_count = len(normalized_params) - len(unique_params)
                        issues.append(CodeIssue(
                            code="CAT_DUP_002",
                            severity=IssueSeverity.WARNING,
                            message=f"CATEGORIES å­—å…¸ä¸­æœ‰ {dup_count} ä¸ªåˆ†ç±»çš„å‚æ•°ä¸å…¶ä»–åˆ†ç±»é‡å¤",
                            suggestion="è¯·æ£€æŸ¥ CATEGORIES å­—å…¸ï¼Œç¡®ä¿æ¯ä¸ªåˆ†ç±»éƒ½æœ‰å”¯ä¸€çš„åŒºåˆ†å‚æ•°"
                        ))
        
        return issues
    
    def _build_repair_prompt(self, issues: List, current_code: str, page_structure: Optional[Dict[str, Any]] = None) -> str:
        """
        æ„å»ºä¿®å¤æç¤ºï¼ˆå¸¦é¡µé¢ç»“æ„ä¸Šä¸‹æ–‡ï¼‰
        
        Args:
            issues: æ£€æŸ¥å‘ç°çš„é—®é¢˜åˆ—è¡¨
            current_code: å½“å‰ä»£ç 
            page_structure: é¡µé¢ç»“æ„ä¿¡æ¯ï¼ˆå¸®åŠ© LLM æ­£ç¡®ä¿®å¤ï¼‰
        """
        lines = [
            "ã€ä»£ç æ£€æŸ¥å‘ç°ä»¥ä¸‹é—®é¢˜ï¼Œè¯·ä¿®æ­£ã€‘\n"
        ]
        
        for issue in issues:
            severity_icon = "ğŸ”´" if issue.severity.value == "error" else "ğŸŸ¡"
            lines.append(f"{severity_icon} [{issue.code}] {issue.message}")
            if hasattr(issue, 'suggestion') and issue.suggestion:
                lines.append(f"   ä¿®å¤å»ºè®®: {issue.suggestion}")
            lines.append("")
        
        # ğŸ”‘ æ–°å¢ï¼šæ·»åŠ é¡µé¢ç»“æ„ä¿¡æ¯ï¼Œå¸®åŠ© LLM æ­£ç¡®ä¿®å¤
        if page_structure:
            lines.append("")
            lines.append("## é¡µé¢ç»“æ„å‚è€ƒï¼ˆå¸®åŠ©ä½ æ­£ç¡®ä¿®å¤ï¼‰\n")
            
            # è¡¨æ ¼ä¿¡æ¯
            tables = page_structure.get('tables', [])
            if tables:
                table = tables[0]
                lines.append(f"- è¡¨æ ¼åˆ—æ•°: {table.get('columnCount', 'æœªçŸ¥')}")
                headers = table.get('headers', [])
                if headers:
                    lines.append(f"- è¡¨å¤´: {headers[:8]}")
                
                # æ—¥æœŸåˆ—ä¿¡æ¯ï¼ˆå…³é”®ï¼ï¼‰
                date_indices = table.get('dateColumnIndices', [])
                date_hints = table.get('dateColumnHints', [])
                if date_indices:
                    lines.append(f"- **æ—¥æœŸåˆ—ä½ç½®**: ç´¢å¼• {date_indices} (ä»0å¼€å§‹)")
                    for hint in date_hints[:3]:
                        lines.append(f"  - ç¬¬{hint.get('columnIndex')}åˆ—ã€Œ{hint.get('headerText', '?')}ã€æ£€æµ‹åˆ°æ—¥æœŸ")
                
                # ä¸‹è½½é“¾æ¥åˆ—
                download_indices = table.get('downloadColumnIndices', [])
                if download_indices:
                    lines.append(f"- ä¸‹è½½é“¾æ¥åˆ—: ç´¢å¼• {download_indices}")
                
                # é¦–è¡Œé¢„è§ˆ
                first_row = table.get('firstRowPreview', [])
                if first_row:
                    lines.append(f"- é¦–è¡Œæ•°æ®é¢„è§ˆ: {first_row[:6]}")
            
            # SPA çº¿ç´¢
            spa_hints = page_structure.get('spaHints', {})
            if spa_hints and (spa_hints.get('hasHashRoute') or spa_hints.get('hasAppRoot')):
                lines.append(f"- **SPA é¡µé¢**: hasHashRoute={spa_hints.get('hasHashRoute')}, hasAppRoot={spa_hints.get('hasAppRoot')}")
                lines.append("  - å»ºè®®ä½¿ç”¨ Playwright æ¸²æŸ“åæå–ï¼Œæˆ–è°ƒç”¨ API è·å– JSON")
            
            # æ—¥æœŸå…ƒç´ 
            date_elements = page_structure.get('dateElements', [])
            if date_elements:
                lines.append(f"- é¡µé¢ä¸­æ£€æµ‹åˆ° {len(date_elements)} ä¸ªæ—¥æœŸå…ƒç´ ")
                for de in date_elements[:2]:
                    lines.append(f"  - ã€Œ{de.get('dateValue')}ã€ä½äº {de.get('selector')}")
            
            lines.append("")
        
        lines.extend([
            "",
            "**ä¿®å¤è¦æ±‚**:",
            "1. ä¿æŒä»£ç çš„æ•´ä½“ç»“æ„å’ŒåŠŸèƒ½ä¸å˜",
            "2. åªä¿®å¤ä¸Šè¿°æŒ‡å‡ºçš„é—®é¢˜",
            "3. å‚è€ƒä¸Šæ–¹çš„ã€Œé¡µé¢ç»“æ„å‚è€ƒã€ï¼Œä½¿ç”¨æ­£ç¡®çš„åˆ—ç´¢å¼•æˆ–é€‰æ‹©å™¨",
            "4. å¦‚æœä¸ç¡®å®šåˆ—ç´¢å¼•ï¼Œä½¿ç”¨ PyGen æ³¨å…¥çš„å·¥å…·å‡½æ•°ï¼ˆå¦‚ _pygen_smart_find_date_in_row_*ï¼‰",
            "5. è¾“å‡ºå®Œæ•´çš„ã€å¯è¿è¡Œçš„ Python ä»£ç ",
            "",
            "è¯·é‡æ–°ç”Ÿæˆä¿®å¤åçš„å®Œæ•´ä»£ç ï¼š"
        ])
        
        return "\n".join(lines)

    def _build_system_prompt(self, run_mode: str = "enterprise_report", crawl_mode: str = "single_page") -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆå¢å¼ºç‰ˆ v3.0 - å«é”™è¯¯æ¡ˆä¾‹ï¼Œæ”¯æŒå¤šåœºæ™¯ï¼‰
        
        Args:
            run_mode: è¿è¡Œæ¨¡å¼ ('enterprise_report' | 'news_sentiment')
            crawl_mode: çˆ¬å–æ¨¡å¼ ('single_page' | 'multi_page' | 'auto_detect')
        """
        
        # è·å–é”™è¯¯æ¡ˆä¾‹ Few-shot
        error_cases_section = ""
        if self.enable_error_cases:
            error_cases_section = get_error_cases_prompt(severity_threshold=ErrorSeverity.MEDIUM)
        
        # æ ¹æ®çˆ¬å–æ¨¡å¼ç”Ÿæˆä¸åŒçš„åˆ†ç±»ç­–ç•¥æç¤ºï¼ˆæ‰€æœ‰è¿è¡Œæ¨¡å¼é€šç”¨ï¼‰
        crawl_mode_instruction = ""
        if crawl_mode == "single_page":
            crawl_mode_instruction = """
## ã€å½“å‰çˆ¬å–æ¨¡å¼ï¼šå•ä¸€æ¿å—çˆ¬å–ã€‘

âš ï¸ **é‡è¦**ï¼šç”¨æˆ·é€‰æ‹©äº†ã€Œå•ä¸€æ¿å—çˆ¬å–ã€æ¨¡å¼ï¼Œè¿™æ„å‘³ç€ï¼š
1. **åªæŠ“å–å½“å‰é¡µé¢é»˜è®¤æ˜¾ç¤ºçš„æ•°æ®**ï¼Œä¸è¦éå†å¤šä¸ªåˆ†ç±»/æ¿å—
2. **ç¦æ­¢**å®šä¹‰ CATEGORIES å­—å…¸æ¥éå†å¤šä¸ªåˆ†ç±»
3. å¦‚æœé¡µé¢æœ‰åˆ†é¡µï¼Œå¯ä»¥ç¿»é¡µæŠ“å–ï¼Œä½†ä¸è¦åˆ‡æ¢åˆ†ç±»/æ¿å—
"""
        elif crawl_mode == "multi_page":
            crawl_mode_instruction = """
## ã€å½“å‰çˆ¬å–æ¨¡å¼ï¼šå¤šæ¿å—çˆ¬å–ã€‘

ç”¨æˆ·é€‰æ‹©äº†ã€Œå¤šæ¿å—çˆ¬å–ã€æ¨¡å¼ï¼Œè¿™æ„å‘³ç€ï¼š
1. éœ€è¦éå†å¤šä¸ªåˆ†ç±»/æ¿å—æ¥è·å–å®Œæ•´æ•°æ®
2. ä½¿ç”¨ã€Œå¢å¼ºåˆ†æç»“æœã€ä¸­æä¾›çš„ `verified_category_mapping` ä½œä¸ºåˆ†ç±»å­—å…¸
3. å¦‚æœ verified_category_mapping æä¾›äº† `menu_to_filters`ï¼šè¡¨ç¤ºâ€œåŒä¸€ä¸ªåˆ—è¡¨æ¥å£ + ä¸åŒ filters å‚æ•°â€ï¼Œåº”éå†è¿™äº› filters æŠ“å–
4. å¦‚æœ verified_category_mapping æä¾›äº† `menu_to_urls`ï¼šè¡¨ç¤ºâ€œä¸åŒæ¿å—å¯¹åº”ä¸åŒåˆ—è¡¨é¡µ URLï¼ˆæœåŠ¡ç«¯æ¸²æŸ“/è·³è½¬å‹èœå•ï¼‰â€ï¼Œåº”éå†è¿™äº› URL é€ä¸ªæŠ“å–
5. å¦‚æœæ²¡æœ‰æä¾› verified_category_mappingï¼ŒæŒ‰ç…§æ•è·è¯·æ±‚ä¸­çš„åˆ†ç±»å‚æ•°æ„å»º
"""
        elif crawl_mode == "auto_detect":
            crawl_mode_instruction = """
## ã€å½“å‰çˆ¬å–æ¨¡å¼ï¼šè‡ªåŠ¨æ¢æµ‹æ¿å—ã€‘

ç”¨æˆ·é€‰æ‹©äº†ã€Œè‡ªåŠ¨æ¢æµ‹æ¿å—ã€æ¨¡å¼ï¼Œè¿™æ„å‘³ç€ï¼š
1. æ ¹æ®é¡µé¢ç»“æ„å’Œ API å‚æ•°è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦éå†å¤šä¸ªåˆ†ç±»
2. **å¿…é¡»**éå†ã€Œå¢å¼ºåˆ†æç»“æœã€ä¸­æä¾›çš„ `verified_category_mapping` (åˆ†ç±»æ˜ å°„è¡¨) ä¸­çš„æ‰€æœ‰æ¡ç›®
3. å¦‚æœ verified_category_mapping ä¸ºç©ºï¼Œåˆ™åªçˆ¬å–å½“å‰é¡µé¢é»˜è®¤æ˜¾ç¤ºçš„æ•°æ®
"""
        
        # æ ¹æ®è¿è¡Œæ¨¡å¼é€‰æ‹©ä¸åŒçš„ prompt
        if run_mode == "news_sentiment":
            return self._build_news_system_prompt() + crawl_mode_instruction + error_cases_section
        
        # ä¼ä¸šæŠ¥å‘Šæ¨¡å¼çš„çˆ¬å–æ¨¡å¼æç¤ºï¼ˆæ›´è¯¦ç»†ï¼‰
        detailed_crawl_mode_instruction = ""
        if crawl_mode == "single_page":
            detailed_crawl_mode_instruction = """
## ã€å½“å‰çˆ¬å–æ¨¡å¼ï¼šå•ä¸€æ¿å—çˆ¬å–ã€‘

âš ï¸ **é‡è¦**ï¼šç”¨æˆ·é€‰æ‹©äº†ã€Œå•ä¸€æ¿å—çˆ¬å–ã€æ¨¡å¼ï¼Œè¿™æ„å‘³ç€ï¼š
1. **åªæŠ“å–å½“å‰é¡µé¢é»˜è®¤æ˜¾ç¤ºçš„æ•°æ®**ï¼Œä¸è¦éå†å¤šä¸ªåˆ†ç±»/æ¿å—
2. **ç¦æ­¢**å®šä¹‰ CATEGORIES å­—å…¸æ¥éå†å¤šä¸ªåˆ†ç±»
3. å¦‚æœ API éœ€è¦åˆ†ç±»å‚æ•°ï¼Œä½¿ç”¨é¡µé¢å½“å‰çš„é»˜è®¤å€¼æˆ–ä»æ•è·çš„è¯·æ±‚ä¸­æå–çš„å€¼
4. ç”Ÿæˆçš„è„šæœ¬åº”è¯¥ç®€å•ç›´æ¥ï¼Œåªé’ˆå¯¹å•ä¸€æ•°æ®æº

```python
# âŒ é”™è¯¯ï¼šå•ä¸€æ¿å—æ¨¡å¼ä¸‹ä¸åº”è¯¥éå†å¤šä¸ªåˆ†ç±»
CATEGORIES = {"æ·±å¸‚": "szse", "æ²ªå¸‚": "sse", "åŒ—äº¤æ‰€": "bj"}
for cat_name, col_val in CATEGORIES.items():
    fetch_data(cat_name, col_val)

# âœ… æ­£ç¡®ï¼šç›´æ¥ä½¿ç”¨é»˜è®¤åˆ†ç±»æˆ–å½“å‰é¡µé¢çš„å‚æ•°
def fetch_data():
    # ä½¿ç”¨ä»æ•è·è¯·æ±‚ä¸­æå–çš„é»˜è®¤å‚æ•°
    params = {"column": "szse", "pageNum": 1, "pageSize": 30}
    response = requests.post(API_URL, data=params)
```
"""
        elif crawl_mode == "multi_page":
            detailed_crawl_mode_instruction = """
## ã€å½“å‰çˆ¬å–æ¨¡å¼ï¼šå¤šæ¿å—çˆ¬å–ã€‘

ç”¨æˆ·é€‰æ‹©äº†ã€Œå¤šæ¿å—çˆ¬å–ã€æ¨¡å¼ï¼Œè¿™æ„å‘³ç€ï¼š
1. éœ€è¦éå†å¤šä¸ªåˆ†ç±»/æ¿å—æ¥è·å–å®Œæ•´æ•°æ®
2. ä½¿ç”¨ã€Œå¢å¼ºåˆ†æç»“æœã€ä¸­æä¾›çš„ `verified_category_mapping` ä½œä¸ºåˆ†ç±»å­—å…¸
3. å¦‚æœ verified_category_mapping æä¾›äº† `menu_to_filters`ï¼šè¡¨ç¤ºâ€œåŒä¸€ä¸ªåˆ—è¡¨æ¥å£ + ä¸åŒ filters å‚æ•°â€ï¼Œåº”éå†è¿™äº› filters æŠ“å–
4. å¦‚æœ verified_category_mapping æä¾›äº† `menu_to_urls`ï¼šè¡¨ç¤ºâ€œä¸åŒæ¿å—å¯¹åº”ä¸åŒåˆ—è¡¨é¡µ URLï¼ˆæœåŠ¡ç«¯æ¸²æŸ“/è·³è½¬å‹èœå•ï¼‰â€ï¼Œåº”éå†è¿™äº› URL é€ä¸ªæŠ“å–
5. å¦‚æœæ²¡æœ‰æä¾› verified_category_mappingï¼ŒæŒ‰ç…§æ•è·è¯·æ±‚ä¸­çš„åˆ†ç±»å‚æ•°æ„å»º
"""
        elif crawl_mode == "auto_detect":
            detailed_crawl_mode_instruction = """
## ã€å½“å‰çˆ¬å–æ¨¡å¼ï¼šè‡ªåŠ¨æ¢æµ‹æ¿å—ã€‘

ç”¨æˆ·é€‰æ‹©äº†ã€Œè‡ªåŠ¨æ¢æµ‹æ¿å—ã€æ¨¡å¼ï¼Œè¿™æ„å‘³ç€ï¼š
1. æ ¹æ®é¡µé¢ç»“æ„å’Œ API å‚æ•°è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦éå†å¤šä¸ªåˆ†ç±»
2. **å¿…é¡»**éå†ã€Œå¢å¼ºåˆ†æç»“æœã€ä¸­æä¾›çš„ `verified_category_mapping` (åˆ†ç±»æ˜ å°„è¡¨) ä¸­çš„æ‰€æœ‰æ¡ç›®
3. **ä¸¥ç¦**åªæŠ“å–å…¶ä¸­ä¸€ä¸ªåˆ†ç±»ï¼Œå¿…é¡»ç”Ÿæˆå¾ªç¯ä»£ç å¤„ç†æ˜ å°„è¡¨ä¸­çš„æ¯ä¸€ä¸ªåˆ†ç±»
"""
        
        base_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonçˆ¬è™«å·¥ç¨‹å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„é¡µé¢ç»“æ„å’Œç½‘ç»œè¯·æ±‚ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ª**å®Œæ•´ã€ç‹¬ç«‹ã€å¯ç›´æ¥è¿è¡Œ**çš„Pythonçˆ¬è™«è„šæœ¬ã€‚

## æ ¸å¿ƒè¦æ±‚

1. **ç‹¬ç«‹æ€§**ï¼šç”Ÿæˆçš„è„šæœ¬å¿…é¡»æ˜¯å®Œå…¨ç‹¬ç«‹çš„ï¼Œç”¨æˆ·åªéœ€è¦ `pip install` å¿…è¦çš„åº“å°±èƒ½ç›´æ¥è¿è¡Œ
2. **å®Œæ•´æ€§**ï¼šåŒ…å«æ‰€æœ‰å¿…è¦çš„å¯¼å…¥è¯­å¥ã€å‡½æ•°å®šä¹‰ã€ä¸»ç¨‹åºå…¥å£
3. **å¥å£®æ€§**ï¼šåŒ…å«é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶ã€è¯·æ±‚é—´éš”
4. **å¯è¯»æ€§**ï¼šä»£ç è¦æœ‰æ¸…æ™°çš„ä¸­æ–‡æ³¨é‡Š

## æŠ€æœ¯é€‰å‹ç­–ç•¥

### ã€ç¡¬çº¦æŸã€‘å¹³å°å…¼å®¹æ€§ä¸åçˆ¬è™«ï¼ˆé˜²å´©æºƒ/é˜²æ‹¦æˆªï¼‰
1. **ç¦æ­¢åœ¨ print() è¾“å‡ºä¸­ä½¿ç”¨ Emoji è¡¨æƒ…**ï¼ˆå¦‚ ğŸš€, âœ…, âŒ, âš ï¸ ç­‰ï¼‰ã€‚
   - Windows é»˜è®¤æ§åˆ¶å° (GBK) æ— æ³•ç¼–ç è¿™äº›å­—ç¬¦ï¼Œä¼šå¯¼è‡´ `UnicodeEncodeError` å¹¶ä½¿ç¨‹åºå´©æºƒã€‚
   - åªèƒ½ä½¿ç”¨æ ‡å‡† ASCII å­—ç¬¦ã€ä¸­æ–‡å­—ç¬¦æˆ–æ ‡å‡†æ ‡ç‚¹ã€‚
2. ç¡®ä¿æ–‡ä»¶ç¼–ç å£°æ˜ä¸º `# -*- coding: utf-8 -*-`ï¼ˆæ¨¡æ¿å·²åŒ…å«ï¼‰ã€‚
3. **ã€å¿…é¡»ã€‘Playwright æ— å¤´æ¨¡å¼ä¸åçˆ¬é…ç½®**ï¼š
   - å¦‚æœä»£ç ä¸­ä½¿ç”¨ Playwrightï¼Œ**å¿…é¡»**è®¾ç½® `headless=True`ã€‚
   - **å¿…é¡»**æ·»åŠ  `--disable-blink-features=AutomationControlled` å¯åŠ¨å‚æ•°ã€‚
   - **å¿…é¡»**åœ¨ context ä¸­è®¾ç½®æ ‡å‡†çš„ User-Agentã€‚
   - **ã€åçˆ¬å…œåº•ã€‘å¿…é¡»å°è¯•åº”ç”¨ playwright-stealth**ï¼š
     ```python
     # å¿…é¡»åŒ…å«æ­¤å¯¼å…¥é€»è¾‘
     try:
         from playwright_stealth import stealth_sync
     except ImportError:
         stealth_sync = None
     
     # ... page åˆ›å»ºå ...
     if stealth_sync:
         try:
             stealth_sync(page)
         except: pass
     ```
   - åœ¨ `page.goto` åï¼Œ**å¿…é¡»**æ·»åŠ éšæœºç­‰å¾…æˆ–æ˜¾å¼ç­‰å¾…ï¼ˆå¦‚ `page.wait_for_timeout(3000)`ï¼‰ã€‚

### ã€ç¡¬çº¦æŸã€‘ä¼˜å…ˆçº§è§„åˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

1. **ä¼˜å…ˆä½¿ç”¨APIæ–¹å¼ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰**ï¼š
   - å¦‚æœ"æ•è·çš„ç½‘ç»œè¯·æ±‚"éƒ¨åˆ†æä¾›äº† API è¯·æ±‚ä¿¡æ¯ï¼Œ**å¿…é¡»**ä½¿ç”¨ `requests` ç›´æ¥è°ƒç”¨è¯¥ API
   - **ç»å¯¹ç¦æ­¢**åœ¨æœ‰ API å¯ç”¨æ—¶ä½¿ç”¨ BeautifulSoup è§£æ HTML
   - API è¿”å› JSON æ•°æ®ï¼Œç›´æ¥ä» JSON ä¸­æå–å­—æ®µï¼Œä¸éœ€è¦è§£æ HTML
   - åŠ¨æ€åŠ è½½é¡µé¢çš„è¡¨æ ¼æ•°æ®åœ¨åˆå§‹ HTML ä¸­æ˜¯ç©ºçš„ï¼ŒBeautifulSoup åªèƒ½çœ‹åˆ°ç©ºè¡¨æ ¼

2. **é™æ€é¡µé¢**ï¼šä»…å½“æ²¡æœ‰æ£€æµ‹åˆ° API è¯·æ±‚æ—¶ï¼Œæ‰ä½¿ç”¨ `requests + BeautifulSoup`
   - è¿™ç§æƒ…å†µä¸‹æ•°æ®ç›´æ¥åµŒå…¥åœ¨ HTML æºç ä¸­
   
3. **åŠ¨æ€é¡µé¢ï¼ˆæŒ‰éœ€ä½¿ç”¨ Playwrightï¼‰**ï¼šå½“ä¸”ä»…å½“å‡ºç°ä¸‹åˆ—æƒ…å†µä¹‹ä¸€æ—¶ä½¿ç”¨ `playwright`ï¼š
   - API **æ²¡æœ‰**æ—¥æœŸå­—æ®µï¼Œæˆ–æ—¥æœŸå­—æ®µåœ¨æ ·ä¾‹ä¸­ä¸º `null/None`
   - é¡µé¢ç»“æ„æ‘˜è¦ä¸­æä¾›äº† **"ğŸ“…ğŸ“„ æ—¥æœŸ-æ¡ç›®å…³è”æ ·æœ¬"**ï¼ˆè¯´æ˜æ—¥æœŸæ¥è‡ªæ¸²æŸ“åçš„DOMï¼‰
   - é¡µé¢ä¸º SPAï¼ˆæ‘˜è¦ä¸­æœ‰ `hasHashRoute/hasAppRoot` çº¿ç´¢ï¼‰
   ä½¿ç”¨æ–¹å¼åº”å°½é‡"**åªä¸ºæ—¥æœŸä½¿ç”¨æµè§ˆå™¨**"ï¼Œä¸»æ•°æ®ä»ä¼˜å…ˆèµ° APIï¼Œä»¥å¹³è¡¡æ€§èƒ½ä¸æ­£ç¡®ç‡ã€‚

### å¸¸è§é”™è¯¯ï¼ˆå¿…é¡»é¿å…ï¼‰

```python
# âŒ é”™è¯¯ï¼šæœ‰ API å¯ç”¨ä½†ç”¨ BeautifulSoup è§£æ HTML
response = requests.get("https://example.com/list.html")
soup = BeautifulSoup(response.text, 'html.parser')
rows = soup.select('table tbody tr')  # è¡¨æ ¼æ˜¯ç©ºçš„ï¼æ•°æ®é€šè¿‡ API åŠ è½½

# âœ… æ­£ç¡®ï¼šç›´æ¥è°ƒç”¨ API è·å– JSON
response = requests.get("https://example.com/api/list", params={"page": 1})
data = response.json()
for item in data["rows"]:
    name = item["title"]
    date = item["rankdate"]
```

## ã€ç¡¬çº¦æŸã€‘ç³»ç»Ÿå…¼å®¹æ€§ä¸ç¨³å®šæ€§ï¼ˆé˜²æ­¢å´©æºƒï¼‰
1. **Windows å…¼å®¹æ€§ï¼ˆç¦æ­¢ Emojiï¼‰**ï¼š
   - **ä¸¥ç¦**åœ¨ `print()` è¾“å‡ºä¸­ä½¿ç”¨ Emojiï¼ˆå¦‚ ğŸš€, âœ…, âŒ, ğŸ“ï¼‰ï¼ŒWindows æ§åˆ¶å°é»˜è®¤ GBK ç¼–ç ä¼šç›´æ¥æŠ¥é”™å´©æºƒï¼ˆUnicodeEncodeErrorï¼‰ã€‚
   - åªèƒ½ä½¿ç”¨çº¯æ–‡æœ¬ç¬¦å·ï¼ˆå¦‚ `[INFO]`, `[ERROR]`, `*`, `+`, `->`ï¼‰ã€‚
2. **å¾ªç¯å¥å£®æ€§**ï¼š
   - åœ¨ `main` å‡½æ•°éå† `CATEGORIES` æ—¶ï¼Œ**å¿…é¡»**å¯¹æ¯ä¸€æ¬¡å¾ªç¯ä½¿ç”¨ `try...except` åŒ…è£¹ã€‚
   - ç¡®ä¿æŸä¸€ä¸ªåˆ†ç±»æŠ¥é”™ï¼ˆå¦‚ç½‘ç»œè¶…æ—¶ã€è§£æé”™è¯¯ï¼‰ä¸ä¼šå¯¼è‡´æ•´ä¸ªè„šæœ¬å´©æºƒï¼Œè€Œæ˜¯æ‰“å°é”™è¯¯å `continue` ç»§ç»­çˆ¬å–ä¸‹ä¸€ä¸ªåˆ†ç±»ã€‚

## ã€é‡è¦ã€‘SPAåŠ¨æ€é¡µé¢å’Œåˆ†ç±»å‚æ•°å¤„ç†

å¾ˆå¤šç°ä»£ç½‘ç«™ä½¿ç”¨SPAæ¶æ„ï¼ˆVue/Reactç­‰ï¼‰ï¼Œç‰¹ç‚¹æ˜¯ï¼š
- é¡µé¢URLä¸å˜ï¼Œæ•°æ®é€šè¿‡APIå¼‚æ­¥åŠ è½½
- **å¿…é¡»é€‰æ‹©åˆ†ç±»/ç­›é€‰æ¡ä»¶æ‰èƒ½æ˜¾ç¤ºæ•°æ®**
- APIéœ€è¦é¢å¤–çš„åˆ†ç±»å‚æ•°ï¼ˆå¦‚ levelone, leveltwo, categoryId, typeId, filters ç­‰ï¼‰

## ã€ç¡¬çº¦æŸã€‘ç¦æ­¢çŒœæµ‹åˆ†ç±»ID/åˆ†ç±»æ˜ å°„ï¼ˆè‡´å‘½é”™è¯¯ï¼‰

1. å¦‚æœ"å¢å¼ºåˆ†æç»“æœ"ä¸­æä¾›äº† `verified_category_mapping.menu_to_filters`ï¼ˆçœŸå®æŠ“åŒ…å¾—åˆ°ï¼‰ï¼Œ**å¿…é¡»ä¸”åªèƒ½**ä½¿ç”¨å®ƒä½œä¸º `CATEGORIES`ã€‚
2. **ç»å¯¹ç¦æ­¢**å‡­ç©ºç¼–é€ /çŒœæµ‹åˆ†ç±»IDï¼š
   - **ç»å¯¹ç¦æ­¢**æ ¹æ®å·²çŸ¥ ID çš„æ•°å­—è§„å¾‹æ¨æµ‹å…¶ä»–åˆ†ç±»çš„ IDï¼ˆä¾‹å¦‚çœ‹åˆ° 81/82/83 å°±çŒœ 84/85/86ï¼‰ã€‚
   - **ç»å¯¹ç¦æ­¢**åœ¨ verified_category_mapping ä¹‹å¤–æ·»åŠ ä»»ä½•é¢å¤–çš„åˆ†ç±»æ¡ç›®ã€‚
   - å³ä½¿ä½ åœ¨æˆªå›¾ä¸­çœ‹åˆ°äº†æ›´å¤šåˆ†ç±»èœå•ï¼Œä½† verified_category_mapping ä¸­æ²¡æœ‰è¯¥åˆ†ç±»çš„ IDï¼Œ**ä¹Ÿç»å¯¹ä¸èƒ½çŒœæµ‹æ·»åŠ **ï¼Œå› ä¸º ID æ˜¯æ•°æ®åº“ä¸»é”®ï¼Œæ— æ³•é€šè¿‡ä»»ä½•è§„å¾‹æ¨å¯¼ã€‚
3. å¦‚æœ verified_category_mapping ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œåº”é€€åŒ–ä¸º"ä»…æŠ“å–å½“å‰é»˜è®¤åˆ†ç±»/ä¸éå†åˆ†ç±»"ï¼Œå¹¶åœ¨ä»£ç æ³¨é‡Šä¸­è¯´æ˜éœ€è¦é¢å¤–æŠ“åŒ…è·å–åˆ†ç±»å­—å…¸ã€‚

## ã€ç¡¬çº¦æŸã€‘CATEGORIES å­—å…¸æ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼Œä¸å¯æ›´æ”¹ï¼‰

ç”Ÿæˆçš„ä»£ç ä¸­ï¼ŒCATEGORIES å­—å…¸**å¿…é¡»ä¸”åªèƒ½**ä½¿ç”¨ä»¥ä¸‹å›ºå®šæ ¼å¼ï¼š

```python
CATEGORIES = {
    "åˆ†ç±»åç§°": {
        "filters": {"launchedstatus": "å¯ç”¨", "levelone": "73", "leveltwo": "74", "levelthree": "121"},
        "orderby": {"rankdate": "desc"}
    },
    # ... å…¶ä»–åˆ†ç±»
}
```

**å¼ºåˆ¶è§„åˆ™ï¼ˆè¿ååˆ™è„šæœ¬100%å¤±è´¥ï¼‰**ï¼š
1. æ¯ä¸ªåˆ†ç±»çš„å€¼å¿…é¡»æ˜¯å­—å…¸ï¼Œä¸”**å¿…é¡»åŒ…å«** `"filters"` å’Œ `"orderby"` ä¸¤ä¸ªé”®
2. **ç¦æ­¢**ä½¿ç”¨å…¶ä»–é”®åå¦‚ `params`ã€`filter`ã€`param`ã€`data` ç­‰æ›¿ä»£ `filters`
3. **ç¦æ­¢**ä½¿ç”¨å…¶ä»–é”®åå¦‚ `sort`ã€`order`ã€`sorting` ç­‰æ›¿ä»£ `orderby`
4. ä»£ç ä¸­è®¿é—®æ—¶**å¿…é¡»**ä½¿ç”¨ `config["filters"]` å’Œ `config["orderby"]`
5. `filters` ä¸­åº”åŒ…å« `launchedstatus` å’Œåˆ†ç±»å±‚çº§IDï¼ˆå¦‚ levelone/leveltwo/levelthreeï¼‰
6. `orderby` é€šå¸¸ä¸º `{"rankdate": "desc"}` æˆ– `{"createtime": "desc"}`

è¿™æ˜¯ç³»ç»Ÿåå¤„ç†æ³¨å…¥æ•°æ®æ—¶ä½¿ç”¨çš„å”¯ä¸€æ ¼å¼ï¼Œä½¿ç”¨å…¶ä»–æ ¼å¼å°†å¯¼è‡´ KeyErrorã€‚

## ã€è‡´å‘½é”™è¯¯ã€‘ç¦æ­¢å¤ç”¨ç›¸åŒçš„åˆ†ç±»å‚æ•°

ç»å¯¹ç¦æ­¢è®© CATEGORIES å­—å…¸ä¸­ä¸åŒåˆ†ç±»ä½¿ç”¨ç›¸åŒå‚æ•°å€¼ï¼ˆå¦‚æ‰€æœ‰åˆ†ç±»çš„ levelthree éƒ½æ˜¯ 83ï¼‰ã€‚
è¿™ä¼šå¯¼è‡´è™½ç„¶ä»£ç éå†äº†å¤šä¸ªåˆ†ç±»ï¼Œä½† API å®é™…åªè¯·æ±‚åŒä¸€ä¸ªåˆ†ç±»çš„æ•°æ®ã€‚
æ¯ä¸ªåˆ†ç±»å¿…é¡»æœ‰è‡³å°‘ä¸€ä¸ªå‚æ•°ä¸å…¶ä»–åˆ†ç±»ä¸åŒã€‚å¦‚æœå‘ç°æ‰€æœ‰åˆ†ç±»å‚æ•°ç›¸åŒï¼Œè¯´æ˜æ²¡æœ‰æ­£ç¡®ä½¿ç”¨ verified_category_mappingã€‚

## ã€å‘ç‚¹é¢„è­¦ã€‘åŒååˆ†ç±»å¤„ç†ï¼ˆå¿…é¡»é€šè¿‡çˆ¶çº§IDè¿‡æ»¤ï¼‰
1. å¾ˆå¤šç½‘ç«™åœ¨ä¸åŒä¸»åˆ†ç±»ä¸‹ä¼šæœ‰åŒåçš„å­åˆ†ç±»ï¼ˆä¾‹å¦‚â€œä¼ä¸šè¯„çº§â€ä¸‹æœ‰â€œä¸»ä½“è¯„çº§â€ï¼Œâ€œé‡‘èæœºæ„è¯„çº§â€ä¸‹ä¹Ÿæœ‰â€œä¸»ä½“è¯„çº§â€ï¼‰ã€‚
2. **ä¸¥ç¦**ç®€å•åœ°é€šè¿‡åç§°æ„å»ºå­—å…¸ï¼ˆ`name -> id`ï¼‰ï¼Œè¿™ä¼šå¯¼è‡´åå‡ºç°çš„åŒååˆ†ç±»è¦†ç›–å‰é¢çš„æ­£ç¡®åˆ†ç±»ã€‚
3. **å¿…é¡»**æ£€æŸ¥åˆ†ç±»çš„å±‚çº§å…³ç³»ï¼ˆå¦‚ `pid`, `parentId`ï¼‰æˆ–æ‰€å±çš„ä¸»åˆ†ç±»IDã€‚
4. å¦‚æœ API è¿”å›äº†æ‰€æœ‰åˆ†ç±»çš„æ‰å¹³åˆ—è¡¨ï¼Œè¯·åŠ¡å¿…é€šè¿‡ `pid` å‰ç¼€æˆ–çˆ¶çº§ ID è¿‡æ»¤å‡ºç›®æ ‡ä¸»åˆ†ç±»ä¸‹çš„å­é¡¹ã€‚

## ã€æ€§èƒ½è¦æ±‚ã€‘æŒ‰æ—¥æœŸå€’åºè¶Šç•Œæå‰åœæ­¢ï¼ˆé¿å…å…¨é‡ç¿»é¡µï¼‰

å¦‚æœåˆ—è¡¨æ¥å£æŒ‰ `rankdate desc`ï¼ˆæˆ–ç­‰ä»·æ—¥æœŸå­—æ®µå€’åºï¼‰æ’åºï¼š  
å½“æŸä¸€é¡µè®°å½•ä¸­çš„ **æœ€è€æ—¥æœŸ < START_DATE** æ—¶ï¼Œåç»­é¡µåªä¼šæ›´è€ï¼Œåº”ç«‹å³åœæ­¢è¯¥åˆ†ç±»åˆ†é¡µå¾ªç¯ã€‚

## ã€å…³é”®ã€‘å‘å¸ƒæ—¥æœŸï¼ˆdateï¼‰é€šç”¨æå–ç­–ç•¥ï¼ˆå¹³è¡¡æ³›åŒ–/æ­£ç¡®ç‡/è¿è¡Œæ—¶é—´ï¼‰

ä½ å¿…é¡»æŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§è·å– `date`ï¼ˆå‘å¸ƒæ—¥æœŸï¼‰ï¼Œå¹¶ä¿æŒâ€œå¯è§£é‡Š + å¯å¯¹é½â€ï¼š

### æ–¹æ¡ˆAï¼ˆä¼˜å…ˆï¼Œæœ€å¿«ï¼‰ï¼šAPI å“åº”ä¸­çš„æ—¥æœŸå­—æ®µ
- å¦‚æœ API ç»“æ„ä¸­å­˜åœ¨æ˜ç¡®çš„æ—¥æœŸå­—æ®µï¼ˆå¹¶ä¸”æ ·ä¾‹å€¼éç©ºï¼‰ï¼Œç›´æ¥å–ç”¨ã€‚
- å¦‚æœå­—æ®µååƒæ—¥æœŸä½†æ ·ä¾‹ä¸º `null/None`ï¼Œ**ä¸è¦**å½“ä½œå¯ç”¨æ—¥æœŸã€‚

### æ–¹æ¡ˆBï¼ˆæ¬¡é€‰ï¼Œé€‚ç”¨äº SPA/åŠ¨æ€æ¸²æŸ“ï¼‰ï¼šç”¨ Playwright ä»"æ¸²æŸ“å DOM"æå–æ¯é¡µæ¡ç›®æ—¥æœŸï¼ˆæ¨èæ··åˆæ¨¡å¼ï¼‰
- æ¡ä»¶ï¼šAPI æ— æœ‰æ•ˆæ—¥æœŸ + æ‘˜è¦ä¸­å­˜åœ¨"ğŸ“…ğŸ“„ æ—¥æœŸ-æ¡ç›®å…³è”æ ·æœ¬" æˆ– SPA çº¿ç´¢ã€‚
- è¦æ±‚ï¼š
  1. ä¸»æ•°æ®ä»ç”¨ API ç¿»é¡µæŠ“å–ï¼ˆ`requests`ï¼‰ï¼Œé¿å…å…¨é‡æµè§ˆå™¨æŠ“å–å¯¼è‡´æ…¢ã€‚
  2. ä»…ä¸º"æ—¥æœŸ"å¯åŠ¨ä¸€ä¸ª Playwright æµè§ˆå™¨å®ä¾‹ï¼Œå¤ç”¨åŒä¸€é¡µã€‚
  3. **å…³é”®**ï¼šå¦‚æœæœ‰åˆ†é¡µï¼Œå¿…é¡»å¯¹æ¯ä¸€é¡µéƒ½æå–æ—¥æœŸï¼Œè€Œä¸æ˜¯åªå¤„ç†ç¬¬ä¸€é¡µï¼
     - å¯¹æ¯ä¸€é¡µï¼š
     - æ‰“å¼€åˆ—è¡¨é¡µï¼ˆhash è·¯ç”±ä¹Ÿè¦ç”¨ Playwright æ‰“å¼€ï¼Œä¾‹å¦‚ `https://.../#/...`ï¼‰
     - ç­‰å¾…æ¸²æŸ“ï¼ˆ`domcontentloaded` + å°‘é‡ `wait_for_timeout` / æˆ–ç­‰å¾…åˆ—è¡¨å®¹å™¨å‡ºç°ï¼‰
     - ä½¿ç”¨"æ—¥æœŸ-æ¡ç›®å…³è”æ ·æœ¬"ä¸­ç»™å‡ºçš„ `containerSelector/dateSelector` æ€è·¯ï¼Œä»æ¯ä¸ªæ¡ç›®å®¹å™¨å†…æå–æ—¥æœŸæ–‡æœ¬ã€‚
  4. å…³è”ç­–ç•¥ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
     - ä¼˜å…ˆç”¨ `downloadUrl`ï¼ˆå¦‚æœ DOM èƒ½æ‹¿åˆ° href/ä¸‹è½½é“¾æ¥ï¼‰
     - å…¶æ¬¡ç”¨ `title` ç²¾ç¡®åŒ¹é…ï¼ˆå»ç©ºæ ¼ã€ç»Ÿä¸€å…¨è§’åŠè§’ï¼‰
     - æœ€åæ‰å…è®¸"æŒ‰é¡ºåº"å…³è”ï¼Œä½†å¿…é¡»åœ¨ä»£ç æ³¨é‡Šä¸­è¯´æ˜é£é™©ï¼Œå¹¶ä¸”è¦åšé•¿åº¦ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆä¸ä¸€è‡´åˆ™ç•™ç©ºï¼‰ã€‚
- **ä¸¥ç¦**ç”¨ `requests.get()` å»æŠ“ SPA çš„ä¸»é¡µ HTML å†ç”¨æ­£åˆ™æ‰¾æ—¥æœŸï¼ˆè¿™é€šå¸¸æ‹¿ä¸åˆ°æ¸²æŸ“åçš„å†…å®¹ï¼Œä¼šå¯¼è‡´ 0 ä¸ªæ—¥æœŸï¼‰ã€‚

### ã€é‡è¦ã€‘é™æ€ HTML é¡µé¢ç›´æ¥ä» requests å“åº”ä¸­æå–æ—¥æœŸ
- æ¡ä»¶ï¼šé¡µé¢æ˜¯**æœåŠ¡ç«¯æ¸²æŸ“çš„é™æ€ HTML**ï¼ˆä¸æ˜¯ SPAï¼‰ï¼Œæ—¥æœŸç›´æ¥åœ¨ HTML æºç ä¸­å¯è§ã€‚
- **ä¼˜å…ˆæ–¹æ¡ˆ**ï¼šåœ¨åŒä¸€ä¸ªå‡½æ•°ä¸­ç›´æ¥æå–æ—¥æœŸï¼Œä¸è¦åˆ†æˆä¸¤ä¸ªé˜¶æ®µï¼
  - åœ¨ `fetch_page_data` å‡½æ•°ä¸­éå†è¡¨æ ¼è¡Œæ—¶ï¼Œç›´æ¥ä½¿ç”¨ `_pygen_smart_find_date_in_row_bs4(tds)` æå–æ—¥æœŸ
  - æ—¥æœŸå’Œå…¶ä»–å­—æ®µï¼ˆæ ‡é¢˜ã€ä¸‹è½½é“¾æ¥ï¼‰åœ¨åŒä¸€ä¸ªå¾ªç¯ä¸­ä¸€èµ·æå–å¹¶å­˜å…¥è®°å½•
  - **ä¸è¦**å…ˆè·å–æ‰€æœ‰è®°å½•å†ç”¨ Playwright å•ç‹¬æå–æ—¥æœŸï¼Œè¿™ä¼šå¯¼è‡´åˆ†é¡µæ—¥æœŸä¸¢å¤±ï¼
- ç¤ºä¾‹ä»£ç ï¼š
```python
def fetch_page_data(page_num):
    # ... è·å– HTML ...
    for row in rows:
        tds = row.select('td')
        name = tds[0].get_text(strip=True)
        date = _pygen_smart_find_date_in_row_bs4(tds)  # ç›´æ¥åœ¨è¿™é‡Œæå–æ—¥æœŸï¼
        download_url = ...
        reports.append({
            "name": name,
            "date": date,  # æ—¥æœŸå·²ç»åœ¨è¿™é‡Œäº†
            "downloadUrl": download_url,
            "fileType": file_type
        })
```

### æ–¹æ¡ˆCï¼ˆå…œåº•ï¼Œæœ‰é™æˆæœ¬ï¼‰ï¼šå°æ‰¹é‡è¯¦æƒ…é¡µè¡¥å…¨æ—¥æœŸ
- å¦‚æœ A/B éƒ½å–ä¸åˆ°æ—¥æœŸï¼šå¯ä»¥åªå¯¹â€œå€™é€‰èŒƒå›´é™„è¿‘â€æˆ–å‰ N æ¡ï¼ˆä¾‹å¦‚ N<=30ï¼‰æ‰“å¼€è¯¦æƒ…é¡µ/æ¥å£è¡¥å…¨æ—¥æœŸï¼Œé¿å…å…¨é‡ 200+ æ¡å¯¼è‡´è¿‡æ…¢ã€‚
- ä»ç„¶ä¸¥ç¦ä»æ ‡é¢˜çŒœæ—¥æœŸã€‚

### ç¦ä»¤ï¼ˆç¡¬çº¦æŸï¼‰
- **ç»å¯¹ç¦æ­¢**ä»æ ‡é¢˜ä¸­â€œçŒœå¹´ä»½/æ‹¼ä¸€ä¸ª 12-31â€ä½œä¸ºæ—¥æœŸã€‚
- å¦‚æœæ— æ³•å¾—åˆ°æ—¥æœŸï¼Œå¡«ç©ºå­—ç¬¦ä¸² `""`ï¼Œå¹¶ä¿è¯è„šæœ¬ä»èƒ½è¾“å‡ºæŠ¥å‘Šè®°å½•ã€‚

## ã€ç¡¬çº¦æŸã€‘æ—¥æœŸèŒƒå›´è¿‡æ»¤å¿…é¡»ä¸¥æ ¼
- å½“ç”¨æˆ·æä¾›äº† `START_DATE/END_DATE` æ—¶ï¼Œæœ€ç»ˆè¾“å‡ºçš„ `reports` **å¿…é¡»åªåŒ…å«**æ»¡è¶³ `START_DATE <= date <= END_DATE` çš„è®°å½•ã€‚
- **date ä¸ºç©º/æ— æ³•è§£æ** çš„è®°å½•ï¼šåœ¨è¿‡æ»¤æ¨¡å¼ä¸‹ **å¿…é¡»ä¸¢å¼ƒ**ï¼ˆä¸è¦â€œä¸ºäº†æ•°é‡å¥½çœ‹â€è€Œä¿ç•™ï¼‰ã€‚
- åªæœ‰å½“ç”¨æˆ·æ²¡æœ‰æä¾›æ—¥æœŸèŒƒå›´ï¼ˆæˆ–æ˜ç¡®è¦æ±‚ä¿ç•™æ— æ—¥æœŸï¼‰æ—¶ï¼Œæ‰å…è®¸è¾“å‡º date ä¸ºç©ºçš„è®°å½•ã€‚

### è¯†åˆ«åˆ†ç±»å‚æ•°çš„æ–¹æ³•

1. æŸ¥çœ‹"å¢å¼ºåˆ†æ"éƒ¨åˆ†çš„ `category_params`ï¼Œè¿™äº›æ˜¯ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«çš„åˆ†ç±»å‚æ•°
2. æ£€æŸ¥APIè¯·æ±‚URLä¸­çš„ `filters` å‚æ•°ï¼Œé€šå¸¸åŒ…å«åˆ†ç±»ID
3. è§‚å¯Ÿä¸åŒèœå•ç‚¹å‡»åAPIè¯·æ±‚å‚æ•°çš„å˜åŒ–

### å¤„ç†åˆ†ç±»å‚æ•°çš„ä»£ç æ¨¡æ¿

```python
# åˆ†ç±»é…ç½®ï¼ˆä»æµè§ˆå™¨åˆ†ææˆ–APIè·å–ï¼‰
CATEGORIES = {
    "åˆ†ç±»åç§°1": {"levelone": "73", "leveltwo": "74", "levelthree": "121"},
    "åˆ†ç±»åç§°2": {"levelone": "73", "leveltwo": "74", "levelthree": "122"},
    # ... æ›´å¤šåˆ†ç±»
}

def fetch_data_by_category(category_name: str, category_params: dict, page: int = 1):
    \"\"\"æŒ‰åˆ†ç±»è·å–æ•°æ®\"\"\"
    filters = {
        "status": "å¯ç”¨",
        **category_params  # åˆå¹¶åˆ†ç±»å‚æ•°
    }
    params = {
        "pageNo": page,
        "pageSize": 20,
        "filters": json.dumps(filters)
    }
    # ...è¯·æ±‚é€»è¾‘

def main():
    all_data = []
    for category_name, category_params in CATEGORIES.items():
        print(f"æ­£åœ¨çˆ¬å–åˆ†ç±»: {category_name}")
        data = fetch_data_by_category(category_name, category_params)
        all_data.extend(data)
```

### ç©ºæ•°æ®æ£€æµ‹

å¦‚æœ"å¢å¼ºåˆ†æ"æ˜¾ç¤º `hasData: false`ï¼Œè¯´æ˜é¡µé¢åˆå§‹çŠ¶æ€æ— æ•°æ®ï¼Œå¿…é¡»ï¼š
1. åˆ†æå¯ç”¨çš„åˆ†ç±»èœå•ï¼ˆ`potentialMenus`ï¼‰
2. åœ¨ä»£ç ä¸­å®šä¹‰åˆ†ç±»é…ç½®
3. éå†æ‰€æœ‰åˆ†ç±»è·å–æ•°æ®

## ã€å¼ºåˆ¶è¦æ±‚ã€‘æå–æŠ¥å‘Šåç§°å’Œä¸‹è½½é“¾æ¥

æ— è®ºæ˜¯ä»€ä¹ˆç±»å‹çš„é¡µé¢ï¼Œç”Ÿæˆçš„çˆ¬è™«è„šæœ¬**å¿…é¡»**æå–ä»¥ä¸‹å­—æ®µï¼š
1. **æŠ¥å‘Šåç§°/æ ‡é¢˜** (name) - æ–‡æ¡£çš„æ ‡é¢˜æˆ–åç§°
2. **ä¸‹è½½é“¾æ¥** (downloadUrl) - PDFæˆ–å…¶ä»–æ–‡ä»¶çš„ä¸‹è½½URL
3. **å‘å¸ƒæ—¥æœŸ** (date) - æŠ¥å‘Šçš„å‘å¸ƒæ—¥æœŸ
4. **æ–‡ä»¶ç±»å‹** (fileType) - å¦‚ pdf, doc, xls ç­‰

### å­—æ®µå‘½åï¼ˆç¡¬çº¦æŸï¼‰
- è¾“å‡º JSON çš„æ¯æ¡è®°å½•**å¿…é¡»**ä½¿ç”¨é”®åï¼š`name`, `date`, `downloadUrl`, `fileType`
- ä½ å¯ä»¥åœ¨ä»£ç å†…éƒ¨ç”¨å˜é‡å `title`ï¼Œä½†å†™å…¥ç»“æœå­—å…¸æ—¶å¿…é¡»æ˜¯ï¼š`"name": title`
- **ä¸è¦**åœ¨æœ€ç»ˆè¾“å‡ºçš„ `reports` ä¸­ä½¿ç”¨ `"title": ...` ä½œä¸ºå­—æ®µåï¼ˆå¦åˆ™å‰ç«¯æ— æ³•æ˜¾ç¤ºåç§°ï¼‰

## ã€ç¡¬çº¦æŸã€‘Playwright äº¤äº’ç¨³å®šæ€§ä¸åçˆ¬ï¼ˆå…³é”®ï¼‰

1. **è§„é¿æ— å¤´æ¨¡å¼æ£€æµ‹**ï¼š
   - å¿…é¡»ä½¿ç”¨ `args=["--disable-blink-features=AutomationControlled"]`ã€‚
   - å¿…é¡»ä½¿ç”¨çœŸå®æµè§ˆå™¨çš„ User-Agentã€‚
   - `navigator.webdriver` å¿…é¡»è¢«å±è”½ï¼ˆPlaywright æŸäº›ç‰ˆæœ¬ä¼šè‡ªåŠ¨å¤„ç†ï¼Œä½†å¯åŠ¨å‚æ•°æ˜¯å¿…é¡»çš„ï¼‰ã€‚

2. **å…ƒç´ äº¤äº’å¿…é¡»å¥å£®**ï¼š
   - **ç¦æ­¢**ç›´æ¥ç”¨ `page.click("text=XXX")` è€Œä¸æ£€æŸ¥å¯è§æ€§ã€‚
   - **å¿…é¡»**ä½¿ç”¨ `locator.wait_for(state="visible", timeout=5000)` ç­‰å¾…å…ƒç´ åŠ è½½ã€‚
   - å¦‚æœè¦ç‚¹å‡»èœå•ï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨ CSS é€‰æ‹©å™¨å®šä½ï¼ˆå› ä¸ºæ–‡æœ¬å¯èƒ½åŒ…å«ç©ºæ ¼æˆ–éšè—å­—ç¬¦ï¼‰ï¼Œæˆ–è€…ä½¿ç”¨ `get_by_text(..., exact=False)` è¿›è¡Œæ¨¡ç³ŠåŒ¹é…ã€‚
   - **å¿…é¡»**å¤„ç†å¯èƒ½çš„å¼¹çª—æˆ–é®ç½©å±‚ï¼ˆè™½ç„¶æ— å¤´æ¨¡å¼çœ‹ä¸è§ï¼Œä½†ç¡®å®å­˜åœ¨ï¼‰ã€‚
   - åœ¨ `click()` å‰æœ€å¥½å…ˆ `hover()`ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸ºï¼Œæœ‰åŠ©äºè§¦å‘ JS äº‹ä»¶ã€‚

3. **åŠ¨æ€åŠ è½½ç­‰å¾…**ï¼š
   - åœ¨ `goto` æˆ– `click` åï¼Œ**å¿…é¡»**æ˜¾å¼ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼ˆå¦‚ `page.wait_for_timeout(2000)`ï¼‰æˆ–ç­‰å¾…ç½‘ç»œç©ºé—²ã€‚
   - ä¸è¦åªä¾èµ– `domcontentloaded`ï¼Œå¾ˆå¤šå•é¡µåº”ç”¨ï¼ˆSPAï¼‰åœ¨ DOM åŠ è½½åè¿˜éœ€è¦å‡ ç§’é’Ÿæ¸²æŸ“æ•°æ®ã€‚

## ã€ç¡¬çº¦æŸã€‘HTML è§£æå¿…é¡»å¥å£®ï¼ˆé¿å… NoneType å´©æºƒï¼Œæå‡æ³›åŒ–èƒ½åŠ›ï¼‰

ä½ ç”Ÿæˆçš„è„šæœ¬ä¸å¾—å‡ºç°"é“¾å¼è°ƒç”¨å¯¼è‡´ç©ºæŒ‡é’ˆ"çš„è„†å¼±å†™æ³•ï¼Œä¾‹å¦‚ï¼š
- âŒ `table.find('tbody').find_all('tr')`
- âŒ `soup.find(...).find_all(...)`ï¼ˆå‰ä¸€ä¸ª find å¯èƒ½è¿”å› Noneï¼‰

å¿…é¡»ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€å®‰å…¨æ–¹å¼ï¼š
1) **ä¼˜å…ˆä½¿ç”¨ CSS é€‰æ‹©å™¨**ï¼ˆæœ€ç¨³ï¼Œè¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯ Noneï¼‰ï¼š
   - âœ… `rows = soup.select('table tbody tr')`
   - âœ… è‹¥æ²¡æœ‰ tbodyï¼š`rows = soup.select('table tr')`
2) å¦‚æœå¿…é¡»ç”¨ `find`ï¼š
   - âœ… `tbody = table.find('tbody')`
   - âœ… `rows = tbody.find_all('tr') if tbody else table.find_all('tr')`

å¹¶ä¸”ï¼š
- è‹¥å…³é”®å®¹å™¨æœªæ‰¾åˆ°ï¼ˆtable/list ä¸ºç©ºï¼‰ï¼Œåº”å½“ **è¿”å›ç©ºç»“æœå¹¶ç»§ç»­/åœæ­¢**ï¼Œä¸è¦æŠ›å¼‚å¸¸ã€‚
- è§£ææ—¶å¯¹æ¯ä¸€å±‚éƒ½åšå­˜åœ¨æ€§æ£€æŸ¥ï¼Œä»»ä½•å­—æ®µç¼ºå¤±éƒ½è¦é™çº§å¤„ç†ã€‚

## ã€ç¡¬çº¦æŸã€‘æ—¥æœŸæå–å¿…é¡»æ³›åŒ–ï¼ˆä¸å¾—ç¡¬ç¼–ç åˆ—ç´¢å¼•ï¼‰

**ç»å¯¹ç¦æ­¢**ç¡¬ç¼–ç è¡¨æ ¼åˆ—ç´¢å¼•æ¥æå–æ—¥æœŸï¼Œä¾‹å¦‚ï¼š
- âŒ `date_elem = tds[4].select_one('span')` â€”â€” ä¸åŒç½‘ç«™æ—¥æœŸå¯èƒ½åœ¨ç¬¬3ã€4ã€5åˆ—æˆ–å…¶ä»–ä½ç½®
- âŒ `date_text = tds[3].get_text()` â€”â€” å‡è®¾æ—¥æœŸå›ºå®šåœ¨æŸåˆ—æ˜¯ä¸å¯é çš„

**å¿…é¡»ä½¿ç”¨æ™ºèƒ½æ‰«æç­–ç•¥**ï¼ˆPyGen ä¼šæ³¨å…¥ `_pygen_smart_find_date_in_row_bs4` å’Œ `_pygen_smart_find_date_in_row_pw` å·¥å…·å‡½æ•°ï¼‰ï¼š

### ç­–ç•¥1ï¼šä½¿ç”¨æ³¨å…¥çš„æ—¥æœŸæå–å·¥å…·ï¼ˆæ¨èï¼‰
```python
# BeautifulSoup æ¨¡å¼
for row in rows:
    tds = row.select('td')
    # ä½¿ç”¨æ³¨å…¥çš„æ™ºèƒ½æ—¥æœŸæ‰«æå‡½æ•°ï¼ˆæ‰«ææ•´è¡Œæ‰€æœ‰åˆ—ï¼‰
    date = _pygen_smart_find_date_in_row_bs4(tds)
    
# Playwright æ¨¡å¼
for row in rows:
    tds = row.query_selector_all('td')
    date = _pygen_smart_find_date_in_row_pw(tds)
```

### ç­–ç•¥2ï¼šæ‰‹åŠ¨å®ç°æ™ºèƒ½æ‰«æï¼ˆå¦‚ä¸ä½¿ç”¨æ³¨å…¥å·¥å…·ï¼‰
```python
def find_date_in_row(tds) -> str:
    \"\"\"æ™ºèƒ½æ‰«æè¡¨æ ¼è¡Œä¸­æ‰€æœ‰å•å…ƒæ ¼æŸ¥æ‰¾æ—¥æœŸ\"\"\"
    import re
    date_re = re.compile(r'(\\d{4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,2}|\\d{4}å¹´\\d{1,2}æœˆ\\d{1,2}æ—¥)')
    for td in tds:
        # å°è¯• spanã€time ç­‰å®¹å™¨
        for tag in ['span', 'time']:
            elem = td.select_one(tag)
            if elem:
                match = date_re.search(elem.get_text(strip=True))
                if match:
                    return match.group(1).replace('/', '-').replace('.', '-')
        # ç›´æ¥è·å– td æ–‡æœ¬
        match = date_re.search(td.get_text(strip=True))
        if match:
            return match.group(1).replace('/', '-').replace('.', '-')
    return ""
```

### å…¶ä»–å¯ç”¨çš„æ³¨å…¥å·¥å…·å‡½æ•°
PyGen ä¼šè‡ªåŠ¨æ³¨å…¥ä»¥ä¸‹å·¥å…·å‡½æ•°ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼š
- `_pygen_normalize_date(date_str)` - æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DD
- `_pygen_smart_find_date_in_row_bs4(tds)` - BeautifulSoup æ¨¡å¼æ™ºèƒ½æ—¥æœŸæ‰«æ
- `_pygen_smart_find_date_in_row_pw(tds)` - Playwright æ¨¡å¼æ™ºèƒ½æ—¥æœŸæ‰«æ  
- `_pygen_extract_date_from_api_item(item)` - ä» API å“åº”æå–æ—¥æœŸ
- `_pygen_merge_dates_by_association(reports, date_map)` - é€šè¿‡å…³è”åˆå¹¶æ—¥æœŸ
- `_pygen_is_date_in_range(date_str, start_date, end_date)` - æ£€æŸ¥æ—¥æœŸèŒƒå›´

## ã€é‡è¦ã€‘æ­£ç¡®æå–å‘å¸ƒæ—¥æœŸ

**ç»å¯¹ç¦æ­¢**ä»æŠ¥å‘Šæ ‡é¢˜ä¸­æå–å¹´ä»½ä½œä¸ºæ—¥æœŸï¼ˆå¦‚ä»"2025å¹´åº¦ä¸»åŠ¨è¯„çº§æŠ¥å‘Š"ä¸­æå–2025ï¼Œç„¶åæ‹¼æ¥æˆ2025-12-31æˆ–ä»»ä½•å›ºå®šæ—¥æœŸï¼‰ã€‚

### æ—¥æœŸæå–ä¼˜å…ˆçº§ï¼ˆæŒ‰é¡ºåºå°è¯•ï¼‰ï¼š

#### æ–¹æ¡ˆ1ï¼šä½¿ç”¨ API å“åº”ä¸­çš„æ—¥æœŸå­—æ®µï¼ˆæœ€ä½³ï¼‰
1. æ£€æŸ¥ API å“åº”å­—æ®µç»“æ„ä¸­æ ‡è®°ä¸º ğŸ“…ã€æ—¥æœŸå­—æ®µã€‘ çš„å­—æ®µ
2. å¸¸è§å­—æ®µåï¼š`rankdate`, `createtime`, `publishtime`, `inputtime`, `addtime`, `updatetime`, `releaseDate`, `pubDate` ç­‰
3. æ—¥æœŸæ ¼å¼éœ€å¤„ç†ï¼šæ—¶é—´æˆ³éœ€è½¬æ¢ã€å­—ç¬¦ä¸²æ—¥æœŸéœ€æ ¼å¼åŒ–ä¸º YYYY-MM-DD

#### æ–¹æ¡ˆ2ï¼šä» HTML é¡µé¢ä¸­æå–æ—¥æœŸï¼ˆå½“ API æ— æ—¥æœŸæ—¶ï¼‰
**å¦‚æœ API å“åº”ä¸­æ²¡æœ‰æœ‰æ•ˆæ—¥æœŸå­—æ®µï¼ˆæˆ–æ ·ä¾‹å€¼ä¸º nullï¼‰ï¼Œå¹¶ä¸”é¡µé¢ç»“æ„æ‘˜è¦é‡Œæä¾›äº† â€œğŸ“…ğŸ“„ æ—¥æœŸ-æ¡ç›®å…³è”æ ·æœ¬â€ æˆ– SPA çº¿ç´¢**ï¼Œåˆ™åº”è¯¥ï¼š
1. **ä¸»æ•°æ®ä»ç”¨ API**ï¼ˆ`requests`ï¼‰ç¿»é¡µæŠ“å–ï¼Œä¿è¯é€Ÿåº¦
2. **æ—¥æœŸç”¨ Playwright æŠ“â€œæ¸²æŸ“å DOMâ€**ï¼ˆé€‚ç”¨äº SPA/CSR/æ··åˆæ¸²æŸ“ï¼‰
3. åœ¨æ¯ä¸ªâ€œæ¡ç›®å®¹å™¨â€å†…ç”¨ç›¸å¯¹é€‰æ‹©å™¨æå–æ—¥æœŸï¼ˆå‚è€ƒ `containerSelector`/`dateSelector` çš„æ ·æœ¬ï¼‰
4. å…³è”æ–¹å¼ï¼šä¼˜å…ˆ `downloadUrl`ï¼ˆè‹¥ DOM å¯å– hrefï¼‰ï¼Œå…¶æ¬¡ `title` ç²¾ç¡®åŒ¹é…ï¼›æœ€åæ‰æŒ‰é¡ºåºä¸”å¿…é¡»åšä¸€è‡´æ€§æ ¡éªŒï¼ˆä¸ä¸€è‡´åˆ™ç•™ç©ºï¼‰
5. **ä¸¥ç¦**ç”¨ `requests.get()` å»æŠ“ SPA çš„ä¸»é¡µ HTML å†ç”¨æ­£åˆ™/é€‰æ‹©å™¨æå–æ—¥æœŸï¼ˆå¸¸å¯¼è‡´ 0 ä¸ªæ—¥æœŸï¼‰

ç¤ºä¾‹ä»£ç ï¼ˆæ—¥æœŸç”¨ Playwright ä»æ¸²æŸ“å DOM æå–ï¼›ä¸»æ•°æ®ä»å»ºè®®èµ° APIï¼‰ï¼š
```python
import re
from playwright.sync_api import sync_playwright

DATE_RE = re.compile(r'(\\d{4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,2}|\\d{4}å¹´\\d{1,2}æœˆ\\d{1,2}æ—¥)')

def extract_dates_from_rendered_list(page_url: str, item_selector: str, date_selector: str) -> list[str]:
    \"\"\"ä»æ¸²æŸ“åçš„åˆ—è¡¨ DOM ä¸­æŒ‰æ¡ç›®æå–æ—¥æœŸï¼ˆé€‚ç”¨äº SPAï¼‰ã€‚\"\"\"
    out: list[str] = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(page_url, wait_until="domcontentloaded", timeout=60000)
        page.wait_for_timeout(1200)
        items = page.query_selector_all(item_selector)
        for it in items:
            el = it.query_selector(date_selector) if date_selector else None
            txt = (el.inner_text().strip() if el else it.inner_text().strip())
            m = DATE_RE.search(txt)
            out.append(m.group(1).replace('/', '-').replace('.', '-') if m else '')
        browser.close()
    return out
```

#### æ–¹æ¡ˆ3ï¼šå®Œå…¨æ²¡æœ‰æ—¥æœŸä¿¡æ¯æ—¶
å¦‚æœ API æ²¡æœ‰æ—¥æœŸå­—æ®µï¼Œé¡µé¢ä¹Ÿæ²¡æœ‰æ£€æµ‹åˆ°æ—¥æœŸå…ƒç´ ï¼Œåˆ™ï¼š
- å°† date å­—æ®µç•™ç©º `""`
- **ç»å¯¹ä¸è¦**ç¡¬ç¼–ç æ—¥æœŸæˆ–ä»æ ‡é¢˜ä¸­çŒœæµ‹

### è¾“å‡ºæ•°æ®æ ¼å¼è¦æ±‚

çˆ¬å–ç»“æœå¿…é¡»ä¿å­˜ä¸ºä»¥ä¸‹ JSON æ ¼å¼ï¼š

```json
{
  "total": 45,
  "crawlTime": "2026-01-27 15:30:00",
  "downloadHeaders": {
    "User-Agent": "Mozilla/5.0 ...",
    "Referer": "https://ç›®æ ‡ç½‘ç«™çš„é¡µé¢URL/"
  },
  "reports": [
    {
      "id": "1",
      "name": "æŠ¥å‘Šæ ‡é¢˜",
      "date": "2026-01-15",
      "downloadUrl": "https://xxx.com/report.pdf",
      "fileType": "pdf"
    }
  ]
}
```

**é‡è¦ï¼š`downloadHeaders` å­—æ®µæ˜¯å¿…é¡»çš„**ï¼Œç”¨äºåç»­ä¸‹è½½ PDF/é™„ä»¶æ—¶ç»•è¿‡é˜²ç›—é“¾ï¼ˆ403 Forbiddenï¼‰ã€‚
- `Referer` åº”è®¾ä¸ºçˆ¬å–çš„ç›®æ ‡é¡µé¢ URLï¼ˆä¸æ˜¯ä¸‹è½½é“¾æ¥æœ¬èº«çš„åŸŸåï¼‰
- `User-Agent` åº”æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨

### ä»£ç ä¸­å¿…é¡»åŒ…å«çš„ä¿å­˜é€»è¾‘

```python
def save_results(reports: list, output_path: str, target_url: str = ""):
    # æ„å»ºä¸‹è½½å¤´ä¿¡æ¯ï¼ˆä¾›åç»­ä¸‹è½½ PDF/é™„ä»¶æ—¶ä½¿ç”¨ï¼Œç»•è¿‡é˜²ç›—é“¾ 403ï¼‰
    from urllib.parse import urlsplit
    _p = urlsplit(target_url)
    _origin = "{}://{}".format(_p.scheme or "https", _p.netloc)
    download_headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": target_url or _origin + "/",
    }
    result = {
        "total": len(reports),
        "crawlTime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "downloadHeaders": download_headers,
        "reports": reports
    }
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"å·²ä¿å­˜ {len(reports)} æ¡è®°å½•åˆ° {output_path}")
```

## è¾“å‡ºæ ¼å¼

ç›´æ¥è¾“å‡ºå®Œæ•´çš„Pythonä»£ç ï¼Œç”¨ ```python å’Œ ``` åŒ…è£¹ã€‚ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œåªè¾“å‡ºä»£ç ã€‚

## ä»£ç æ¨¡æ¿ç»“æ„

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
\"\"\"
çˆ¬è™«è„šæœ¬ - [ç½‘ç«™åç§°]
è‡ªåŠ¨ç”Ÿæˆäº PyGen

åŠŸèƒ½ï¼šçˆ¬å– [å…·ä½“åŠŸèƒ½æè¿°]
\"\"\"

import requests
import json
import os
import time
from datetime import datetime

# é…ç½®
BASE_API_URL = "..."
OUTPUT_DIR = r"..."  # ä½¿ç”¨æä¾›çš„è¾“å‡ºç›®å½•
HEADERS = {...}

# åˆ†ç±»é…ç½®ï¼ˆå¦‚æœæ˜¯SPAé¡µé¢éœ€è¦åˆ†ç±»å‚æ•°ï¼‰
CATEGORIES = {...}

def fetch_data(page_num: int = 1, category_params: dict = None) -> dict:
    \"\"\"è·å–æ•°æ®\"\"\"
    ...

def main():
    \"\"\"ä¸»å‡½æ•°\"\"\"
    ...

if __name__ == "__main__":
    main()
```
"""
        # æ‹¼æ¥çˆ¬å–æ¨¡å¼æŒ‡ä»¤å’Œé”™è¯¯æ¡ˆä¾‹ï¼ˆä½¿ç”¨è¯¦ç»†ç‰ˆï¼‰
        return base_prompt + detailed_crawl_mode_instruction + error_cases_section

    def _build_news_system_prompt(self) -> str:
        """æ„å»ºæ–°é—»èˆ†æƒ…åœºæ™¯çš„ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonçˆ¬è™«å·¥ç¨‹å¸ˆï¼Œä¸“æ³¨äºæ–°é—»å’Œèˆ†æƒ…ä¿¡æ¯é‡‡é›†ã€‚

## ä»»åŠ¡ç›®æ ‡

æ ¹æ®æä¾›çš„é¡µé¢ç»“æ„å’Œç”¨æˆ·éœ€æ±‚ï¼Œç”Ÿæˆä¸€ä¸ª**å®Œæ•´ã€ç‹¬ç«‹ã€å¯ç›´æ¥è¿è¡Œ**çš„Pythonæ–°é—»çˆ¬è™«è„šæœ¬ã€‚
çˆ¬å–çš„æ–°é—»å†…å®¹å°†ä¿å­˜ä¸º JSON æ–‡ä»¶æ ¼å¼ã€‚

## æ ¸å¿ƒè¦æ±‚

1. **ç‹¬ç«‹æ€§**ï¼šç”Ÿæˆçš„è„šæœ¬å¿…é¡»æ˜¯å®Œå…¨ç‹¬ç«‹çš„ï¼Œç”¨æˆ·åªéœ€è¦ `pip install` å¿…è¦çš„åº“å°±èƒ½ç›´æ¥è¿è¡Œ
2. **å®Œæ•´æ€§**ï¼šåŒ…å«æ‰€æœ‰å¿…è¦çš„å¯¼å…¥è¯­å¥ã€å‡½æ•°å®šä¹‰ã€ä¸»ç¨‹åºå…¥å£
3. **å¥å£®æ€§**ï¼šåŒ…å«é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶ã€è¯·æ±‚é—´éš”
4. **å¯è¯»æ€§**ï¼šä»£ç è¦æœ‰æ¸…æ™°çš„ä¸­æ–‡æ³¨é‡Š

## ã€é‡è¦ã€‘ç”¨æˆ·æˆªå›¾è¯†åˆ«

å¦‚æœç”¨æˆ·æä¾›äº†ç½‘é¡µæˆªå›¾ï¼š
1. ä»”ç»†åˆ†ææˆªå›¾ï¼Œè¯†åˆ«ç”¨æˆ·æ ‡æ³¨æˆ–å…³æ³¨çš„**ç›®æ ‡åŒºåŸŸ**ï¼ˆæ–°é—»åˆ—è¡¨ã€æ–‡ç« åŒºåŸŸç­‰ï¼‰
2. æ ¹æ®æˆªå›¾ä¸­çš„å¸ƒå±€å’Œå†…å®¹ï¼Œæ¨æ–­æ­£ç¡®çš„ CSS é€‰æ‹©å™¨
3. ç”Ÿæˆçš„çˆ¬è™«ä»£ç åº”**ç²¾ç¡®å®šä½åˆ°æˆªå›¾ä¸­å±•ç¤ºçš„åŒºåŸŸ**
4. å¦‚æœæˆªå›¾ä¸­æœ‰çº¢æ¡†ã€ç®­å¤´ç­‰æ ‡æ³¨ï¼Œé‚£æ˜¯ç”¨æˆ·å¸Œæœ›çˆ¬å–çš„å…·ä½“åŒºåŸŸ

## æŠ€æœ¯é€‰å‹ç­–ç•¥

### ã€åçˆ¬å…œåº•ã€‘
- å¦‚æœä½¿ç”¨ Playwrightï¼Œ**å¿…é¡»**å°è¯•å¯¼å…¥å¹¶åº”ç”¨ `playwright-stealth`ï¼š
```python
try:
    from playwright_stealth import stealth_sync
except ImportError:
    stealth_sync = None
# ...
if stealth_sync:
    try:
        stealth_sync(page)
    except: pass
```

### æ–°é—»é¡µé¢ä¸€èˆ¬ç‰¹ç‚¹
- æ–°é—»åˆ—è¡¨é¡µé€šå¸¸æ˜¯æœåŠ¡ç«¯æ¸²æŸ“æˆ–æœ‰ API æ¥å£
- ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ JSON APIï¼ˆå¦‚ /api/news, /api/articlesï¼‰
- å¦‚æœæ²¡æœ‰ APIï¼Œä½¿ç”¨ requests + BeautifulSoup è§£æé™æ€ HTML
- å¯¹äº SPA é¡µé¢ï¼Œä½¿ç”¨ Playwright

### éœ€è¦çˆ¬å–çš„æ–°é—»å­—æ®µ
1. **title**ï¼ˆå¿…é¡»ï¼‰ï¼šæ–°é—»æ ‡é¢˜
2. **date**ï¼ˆå¿…é¡»ï¼‰ï¼šå‘å¸ƒæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
3. **author**ï¼šä½œè€…/æ¥æº
4. **source**ï¼šåª’ä½“æ¥æº
5. **sourceUrl**ï¼šåŸæ–‡é“¾æ¥
6. **summary**ï¼šæ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
7. **content**ï¼šæ­£æ–‡å†…å®¹ï¼ˆå®Œæ•´ä¿ç•™ï¼ŒåŒ…å« HTML æ ‡ç­¾æˆ– Markdown æ ¼å¼çš„å›¾ç‰‡é“¾æ¥ï¼‰

## ã€å¼ºåˆ¶ã€‘å†…å®¹æ¸…æ´—è¦æ±‚ï¼ˆä¿®å¤å›¾ç‰‡åŠ è½½é—®é¢˜ï¼‰

åœ¨æå– `content` å­—æ®µåï¼Œ**å¿…é¡»**å¯¹ HTML å†…å®¹è¿›è¡Œæ¸…æ´—ï¼Œå°†æ‰€æœ‰ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼š

1. è§£æ HTML å­—ç¬¦ä¸²ï¼ˆä½¿ç”¨ BeautifulSoupï¼‰ã€‚
2. éå†æ‰€æœ‰ `<img>` æ ‡ç­¾çš„ `src` å±æ€§ã€‚
3. éå†æ‰€æœ‰ `<a>` æ ‡ç­¾çš„ `href` å±æ€§ã€‚
4. ä½¿ç”¨ `urllib.parse.urljoin(current_page_url, link)` å°†æ‰€æœ‰**ç›¸å¯¹è·¯å¾„**è½¬æ¢ä¸º**ç»å¯¹è·¯å¾„**ã€‚
5. è¿™ä¸€æ­¥æ˜¯å¿…é¡»çš„ï¼Œå¦åˆ™åœ¨æœ¬åœ°é¢„è§ˆæ—¶å›¾ç‰‡æ— æ³•åŠ è½½ã€‚

**ä»£ç å®ç°ç¤ºä¾‹**ï¼š

```python
from urllib.parse import urljoin
from bs4 import BeautifulSoup

def clean_html_content(html_content, base_url):
    \"\"\"å°† HTML ä¸­çš„ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„\"\"\"
    if not html_content:
        return ""
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ä¿®å¤å›¾ç‰‡é“¾æ¥
        for img in soup.find_all('img'):
            if img.get('src'):
                img['src'] = urljoin(base_url, img['src'])
                
        # ä¿®å¤è¶…é“¾æ¥
        for a in soup.find_all('a'):
            if a.get('href'):
                a['href'] = urljoin(base_url, a['href'])
                
        return str(soup)
    except Exception as e:
        print(f"å†…å®¹æ¸…æ´—å‡ºé”™: {e}")
        return html_content
```

## è¾“å‡ºæ•°æ®æ ¼å¼è¦æ±‚

çˆ¬å–ç»“æœå¿…é¡»ä¿å­˜ä¸º **JSON æ ¼å¼**ï¼š

```json
{
  "total": 25,
  "crawlTime": "2026-01-29 15:30:00",
  "articles": [
    {
      "id": "1",
      "title": "æ–°é—»æ ‡é¢˜ç¤ºä¾‹",
      "date": "2026-01-28",
      "source": "è´¢ç»ç½‘",
      "author": "å¼ ä¸‰",
      "sourceUrl": "https://xxx.com/news/1.html",
      "summary": "æ–°é—»æ‘˜è¦...",
      "content": "<p>æ–°é—»æ­£æ–‡å†…å®¹...</p><img src='...'>"
    }
  ]
}
```

### ä»£ç ä¸­å¿…é¡»åŒ…å«çš„ä¿å­˜é€»è¾‘

```python
def save_results(articles: list, output_path: str):
    # ä¿å­˜çˆ¬å–ç»“æœä¸ºJSON
    result = {
        "total": len(articles),
        "crawlTime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "articles": articles
    }
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"å·²ä¿å­˜ {len(articles)} æ¡æ–°é—»åˆ° {output_path}")
```
            lines.append(article['summary'])
            lines.append("")
        elif article.get('content'):
            # æˆªå–å‰ 500 å­—ä½œä¸ºæ‘˜è¦
            content = article['content'][:500]
            if len(article['content']) > 500:
                content += "..."
            lines.append(content)
            lines.append("")
        
        lines.append("---")
        lines.append("")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("\\n".join(lines))
    
    print(f"å·²ä¿å­˜ {len(articles)} æ¡æ–°é—»åˆ° {output_path}")
```

### åŒæ—¶ä¿å­˜ JSON æ ¼å¼ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰

```python
def save_results_json(articles: list, output_path: str):
    \"\"\"ä¿å­˜ä¸º JSON æ ¼å¼\"\"\"
    result = {
        "total": len(articles),
        "crawlTime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "articles": articles
    }
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
```

## ä¸»å‡½æ•°ç»“æ„

```python
def main():
    # é…ç½®
    START_DATE = "2026-01-01"
    END_DATE = "2026-12-31"
    OUTPUT_DIR = "./output"
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # çˆ¬å–æ–°é—»
    articles = crawl_news()
    
    # æ—¥æœŸè¿‡æ»¤
    filtered = [a for a in articles if START_DATE <= a.get('date', '') <= END_DATE]
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    md_path = os.path.join(OUTPUT_DIR, f"news_{timestamp}.md")
    json_path = os.path.join(OUTPUT_DIR, f"news_{timestamp}.json")
    
    save_to_markdown(filtered, md_path, "æ¥æºç½‘ç«™åç§°")
    save_results_json(filtered, json_path)

if __name__ == "__main__":
    main()
```

## ã€ç¡¬çº¦æŸã€‘ä¸è¦ç¡¬ç¼–ç é€‰æ‹©å™¨

1. æ ¹æ®ç”¨æˆ·æä¾›çš„é¡µé¢ç»“æ„å’Œæˆªå›¾åˆ†æï¼ŒåŠ¨æ€ç¡®å®šé€‰æ‹©å™¨
2. å¦‚æœç”¨æˆ·æˆªå›¾æ ‡æ³¨äº†ç‰¹å®šåŒºåŸŸï¼Œä¼˜å…ˆå®šä½è¯¥åŒºåŸŸ
3. ä½¿ç”¨é˜²å¾¡æ€§ç¼–ç¨‹ï¼Œå¤„ç†å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
"""

    def _compress_html(self, html_content: str) -> str:
        """
        ç»“æ„åŒ–å‹ç¼© HTML (Token ä¼˜åŒ–æ ¸å¿ƒç­–ç•¥):
        1. ç§»é™¤ script, style, svg, path, link, meta, noscript
        2. ä¿ç•™ DOM æ ‘ç»“æ„
        3. ä»…ä¿ç•™å…³é”®å±æ€§ (id, class, href, name, type...)
        4. æ–‡æœ¬å†…å®¹æˆªæ–­ (è¶…è¿‡ 80 å­—ç¬¦æˆªæ–­)
        """
        if not html_content:
            return ""
            
        # å¦‚æœå†…å®¹æœ¬èº«ä¸å¤§ï¼Œç›´æ¥è¿”å› (æ¯”å¦‚å°äº 15KB)
        if len(html_content) < 15000:
            return html_content

        try:
            from bs4 import BeautifulSoup, Comment
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # 1. ç§»é™¤æ— å…³æ ‡ç­¾ (å™ªéŸ³)
            for tag in soup(['script', 'style', 'svg', 'link', 'meta', 'noscript', 'iframe']):
                tag.decompose()
                
            # 2. ç§»é™¤æ³¨é‡Š
            for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
                comment.extract()
                
            # 3. éå†æ‰€æœ‰æ ‡ç­¾è¿›è¡Œå±æ€§æ¸…æ´—å’Œæ–‡æœ¬æˆªæ–­
            # å…³é”®å±æ€§ç™½åå•
            KEY_ATTRS = {
                'id', 'class', 'href', 'src', 'name', 'type', 
                'value', 'placeholder', 'action', 'method',
                'aria-label', 'role', 'title', 'alt'
            }
            
            for tag in soup.find_all(True):
                # 3.1 å±æ€§æ¸…æ´—
                current_attrs = list(tag.attrs.keys())
                for attr in current_attrs:
                    if attr not in KEY_ATTRS:
                        del tag.attrs[attr]
                
                # 3.2 æ–‡æœ¬æˆªæ–­ (ä»…é’ˆå¯¹å¶å­èŠ‚ç‚¹çš„æ–‡æœ¬)
                if tag.string and len(tag.string) > 80:
                    new_text = tag.string[:80] + "..."
                    tag.string.replace_with(new_text)
            
            cleaned_html = str(soup)
            
            # ç®€å•å‹ç¼©è¿ç»­ç©ºè¡Œ
            import re
            cleaned_html = re.sub(r'\n\s*\n', '\n', cleaned_html)
            
            return cleaned_html
            
        except ImportError:
            print("[WARN] bs4 not installed, falling back to raw truncation")
            return html_content[:30000] + "\n...(bs4 missing, truncated)..."
        except Exception as e:
            print(f"[WARN] HTML compression failed: {e}")
            return html_content[:30000] + "\n...(compression error, truncated)..."

    def _build_user_prompt(
        self,
        page_url: str,
        page_html: str,
        structure_summary: str,
        api_info: str,
        user_requirements: Optional[str] = None,
        start_date: str = "",
        end_date: str = "",
        enhanced_summary: str = ""
    ) -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯"""

        # ä½¿ç”¨ç»“æ„åŒ–å‹ç¼©å¤„ç† HTML
        compressed_html = self._compress_html(page_html)

        html_section = f"""
## é¡µé¢HTML (å·²ç»“æ„åŒ–å‹ç¼©)

```html
{compressed_html}
```
"""

        # æ—¶é—´èŒƒå›´è®¾ç½®
        date_section = ""
        if start_date and end_date:
            date_section = f"""
## ã€é‡è¦ã€‘çˆ¬å–æ—¶é—´èŒƒå›´

ç”¨æˆ·æŒ‡å®šçš„çˆ¬å–æ—¶é—´èŒƒå›´ï¼š
- å¼€å§‹æ—¶é—´ï¼š{start_date}
- ç»“æŸæ—¶é—´ï¼š{end_date}

è¯·åœ¨ç”Ÿæˆçš„ä»£ç ä¸­ä½¿ç”¨è¿™ä¸ªæ—¶é—´èŒƒå›´ï¼š
1. å¦‚æœAPIæ”¯æŒæ—¥æœŸè¿‡æ»¤ï¼Œåœ¨è¯·æ±‚å‚æ•°ä¸­åŠ å…¥æ—¥æœŸèŒƒå›´
2. å¦‚æœAPIä¸æ”¯æŒï¼Œåœ¨å®¢æˆ·ç«¯è¿‡æ»¤è·å–çš„æ•°æ®
3. ä¸è¦è‡ªåŠ¨ç”Ÿæˆæˆ–ç¡¬ç¼–ç å…¶ä»–æ—¶é—´èŒƒå›´
"""

        requirements_section = ""
        if user_requirements:
            requirements_section = f"""
## ç”¨æˆ·é¢å¤–éœ€æ±‚

{user_requirements}
"""

        # å¢å¼ºåˆ†æéƒ¨åˆ†
        enhanced_section = ""
        if enhanced_summary:
            enhanced_section = f"""
## ã€å…³é”®ã€‘å¢å¼ºé¡µé¢åˆ†æç»“æœ

{enhanced_summary}

**è¯·ç‰¹åˆ«æ³¨æ„ä¸Šè¿°åˆ†æç»“æœï¼Œå°¤å…¶æ˜¯ï¼š**
1. å¦‚æœ `hasData` ä¸º falseï¼Œé¡µé¢éœ€è¦é€‰æ‹©åˆ†ç±»æ‰èƒ½åŠ è½½æ•°æ®
2. å¦‚æœæ£€æµ‹åˆ° `category_params`ï¼Œå¿…é¡»åœ¨ä»£ç ä¸­å®šä¹‰åˆ†ç±»é…ç½®å¹¶éå†
3. å‚è€ƒ `menu_mapping` æ¥ç¡®å®šåˆ†ç±»åç§°å’Œå¯¹åº”çš„å‚æ•°å€¼
"""

        # è·å–è¾“å‡ºç›®å½•çš„ç»å¯¹è·¯å¾„
        output_dir = str(Path(__file__).parent / "output")

        # å°†â€œç”¨æˆ·é¢å¤–éœ€æ±‚/æ—¥æœŸèŒƒå›´â€ç­‰é«˜ä¼˜å…ˆçº§çº¦æŸæ”¾åœ¨ user_prompt æœ€å‰é¢
        prefix = ""
        if requirements_section.strip():
            prefix += requirements_section.strip() + "\n\n"
        if date_section.strip():
            prefix += date_section.strip() + "\n\n"

        return prefix + f"""è¯·ä¸ºä»¥ä¸‹é¡µé¢ç”Ÿæˆçˆ¬è™«è„šæœ¬ï¼š

## ç›®æ ‡URL

{page_url}

## é¡µé¢ç»“æ„åˆ†æ

{structure_summary}

## æ•è·çš„ç½‘ç»œè¯·æ±‚ï¼ˆé‡ç‚¹å…³æ³¨APIè¯·æ±‚ï¼‰

{api_info}

{enhanced_section}

{html_section}

## ä»»åŠ¡è¦æ±‚

1. åˆ†æé¡µé¢æ•°æ®æ¥æºï¼ˆAPIæ¥å£ or é™æ€HTMLï¼‰
2. ç”Ÿæˆèƒ½çˆ¬å–è¯¥é¡µé¢æ‰€æœ‰æ•°æ®çš„Pythonè„šæœ¬
3. å¦‚æœæœ‰åˆ†é¡µï¼Œå¿…é¡»å¤„ç†åˆ†é¡µé€»è¾‘
4. æå–æ¯æ¡è®°å½•çš„å…³é”®å­—æ®µï¼ˆæ ‡é¢˜ã€æ—¥æœŸã€é“¾æ¥ç­‰ï¼‰
5. å¦‚æœæœ‰ä¸‹è½½é“¾æ¥ï¼ˆPDFç­‰ï¼‰ï¼Œæå–ä¸‹è½½URL
6. ã€é‡è¦ã€‘å¦‚æœæ£€æµ‹åˆ°åˆ†ç±»å‚æ•°ï¼Œå¿…é¡»ï¼š
   - å®šä¹‰åˆ†ç±»é…ç½®å­—å…¸
   - éå†æ‰€æœ‰åˆ†ç±»è·å–å®Œæ•´æ•°æ®
   - åœ¨è¾“å‡ºä¸­æ ‡è®°æ¯æ¡æ•°æ®çš„åˆ†ç±»æ¥æº
7. å°†ç»“æœJSONæ–‡ä»¶ä¿å­˜åˆ°å›ºå®šç›®å½•ï¼š`{output_dir}`
   - ä½¿ç”¨ os.makedirs ç¡®ä¿ç›®å½•å­˜åœ¨
   - æ–‡ä»¶åä½¿ç”¨æœ‰æ„ä¹‰çš„åç§°ï¼ˆå¦‚ï¼šç½‘ç«™å_æ•°æ®ç±»å‹_æ—¶é—´.jsonï¼‰

è¯·ç›´æ¥è¾“å‡ºå®Œæ•´çš„Pythonä»£ç ï¼š
"""

    def _extract_api_info(
        self, 
        network_requests: Dict[str, List[Dict[str, Any]]],
        enhanced_analysis: Optional[Dict[str, Any]] = None
    ) -> str:
        """æå–APIè¯·æ±‚ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        lines = []
        
        # åŸºç¡€APIè¯·æ±‚
        api_requests = network_requests.get("api_requests", [])

        if not api_requests:
            lines.append("æœªæ•è·åˆ°æ˜æ˜¾çš„APIè¯·æ±‚ï¼Œé¡µé¢å¯èƒ½æ˜¯æœåŠ¡ç«¯æ¸²æŸ“çš„é™æ€HTMLã€‚\n")
        else:
            lines.append("### åˆå§‹é¡µé¢åŠ è½½æ—¶çš„APIè¯·æ±‚\n")
            for i, req in enumerate(api_requests[:10], 1):
                lines.append(f"#### è¯·æ±‚ {i}")
                lines.append(f"- URL: {req.get('url', '')}")
                lines.append(f"- Method: {req.get('method', 'GET')}")

                if req.get('post_data'):
                    lines.append(f"- POSTæ•°æ®: {req.get('post_data')[:500]}")

                if req.get('response_status'):
                    lines.append(f"- å“åº”çŠ¶æ€: {req.get('response_status')}")

                # ã€å…³é”®ã€‘æ˜¾ç¤ºå“åº”å­—æ®µç»“æ„ï¼Œå¸®åŠ© LLM è¯†åˆ«æ—¥æœŸå­—æ®µ
                if req.get('response_field_structure'):
                    field_structure = req.get('response_field_structure')
                    lines.append(f"\n- **ã€é‡è¦ã€‘APIå“åº”å­—æ®µç»“æ„**:")
                    lines.append("  ï¼ˆè¯·ä»”ç»†æ£€æŸ¥å“ªä¸ªå­—æ®µæ˜¯æ—¥æœŸå­—æ®µï¼Œç”¨äºæå–å‘å¸ƒæ—¥æœŸï¼‰")
                    
                    # æ ¼å¼åŒ–å­—æ®µç»“æ„ï¼Œç‰¹åˆ«æ ‡è®°å¯èƒ½çš„æ—¥æœŸå­—æ®µ
                    structure_str = self._format_field_structure(field_structure, indent=2)
                    lines.append(structure_str)
                    
                    # é¢å¤–æå–å¹¶é«˜äº®æ—¥æœŸå­—æ®µ
                    date_fields = self._find_date_fields(field_structure)
                    if date_fields:
                        lines.append(f"\n  **âš ï¸ æ£€æµ‹åˆ°çš„æ—¥æœŸç›¸å…³å­—æ®µ**: {', '.join(date_fields)}")
                        lines.append(f"  **è¯·ä½¿ç”¨è¿™äº›å­—æ®µæå–æŠ¥å‘Šçš„å‘å¸ƒæ—¥æœŸï¼Œè€Œä¸æ˜¯ä»æ ‡é¢˜ä¸­æå–å¹´ä»½ï¼**")

                if req.get('response_preview'):
                    preview = req.get('response_preview', '')[:800]
                    lines.append(f"- å“åº”é¢„è§ˆ: {preview}")

                lines.append("")
        
        # å¢å¼ºåˆ†æä¸­çš„äº¤äº’API
        if enhanced_analysis:
            interaction_apis = enhanced_analysis.get("interaction_apis", {})
            
            if interaction_apis.get("interaction_apis"):
                lines.append("\n### é€šè¿‡äº¤äº’æ•è·çš„APIè¯·æ±‚ï¼ˆç‚¹å‡»èœå•åï¼‰\n")
                
                for interaction in interaction_apis.get("interaction_apis", []):
                    menu_text = interaction.get("menu_text", "æœªçŸ¥èœå•")
                    lines.append(f"#### ç‚¹å‡»èœå• [{menu_text}] åçš„è¯·æ±‚ï¼š")
                    
                    for api in interaction.get("apis", [])[:3]:
                        lines.append(f"- URL: {api.get('url', '')}")
                        if api.get('response_preview'):
                            preview = api.get('response_preview', '')[:500]
                            lines.append(f"- å“åº”é¢„è§ˆ: {preview}")
                    lines.append("")
            
            # å‚æ•°åˆ†æ
            param_analysis = enhanced_analysis.get("param_analysis", {})
            
            if param_analysis.get("category_params"):
                lines.append("\n### ã€å…³é”®ã€‘è¯†åˆ«åˆ°çš„åˆ†ç±»å‚æ•°\n")
                lines.append("ä»¥ä¸‹å‚æ•°åœ¨ä¸åŒèœå•ç‚¹å‡»åå€¼ä¼šå˜åŒ–ï¼Œæ˜¯å¿…éœ€çš„åˆ†ç±»å‚æ•°ï¼š\n")
                
                for cat_param in param_analysis.get("category_params", []):
                    param_name = cat_param.get("param_name", "")
                    sample_values = cat_param.get("sample_values", [])
                    menu_mapping = cat_param.get("menu_mapping", {})
                    
                    lines.append(f"- **å‚æ•°å**: `{param_name}`")
                    lines.append(f"  - ç¤ºä¾‹å€¼: {sample_values}")
                    if menu_mapping:
                        lines.append(f"  - èœå•æ˜ å°„: {json.dumps(menu_mapping, ensure_ascii=False)}")
                    lines.append("")
            
            if param_analysis.get("common_params"):
                lines.append("\n### å›ºå®šå‚æ•°ï¼ˆæ‰€æœ‰è¯·æ±‚éƒ½ç›¸åŒï¼‰\n")
                for key, value in param_analysis.get("common_params", {}).items():
                    # æˆªæ–­è¿‡é•¿çš„å€¼
                    display_value = str(value)[:100] + "..." if len(str(value)) > 100 else value
                    lines.append(f"- `{key}`: {display_value}")

        return "\n".join(lines)

    def _format_field_structure(self, structure: Dict[str, Any], indent: int = 0) -> str:
        """æ ¼å¼åŒ–å­—æ®µç»“æ„ä¸ºå¯è¯»å­—ç¬¦ä¸²"""
        lines = []
        prefix = "  " * indent
        
        if not structure:
            return f"{prefix}(ç©º)"
        
        # å¤„ç†åˆ—è¡¨ç±»å‹çš„ç»“æ„
        if "_list_of" in structure:
            lines.append(f"{prefix}[åˆ—è¡¨] é•¿åº¦: {structure.get('_length', '?')}")
            if "_item_structure" in structure:
                lines.append(f"{prefix}åˆ—è¡¨å…ƒç´ ç»“æ„:")
                lines.append(self._format_field_structure(structure["_item_structure"], indent + 1))
            return "\n".join(lines)
        
        for key, info in structure.items():
            if key.startswith("_"):
                continue
                
            field_type = info.get("type", "unknown")
            example = info.get("example", "")
            likely_date = info.get("likely_date", False)
            
            # æ—¥æœŸå­—æ®µç‰¹æ®Šæ ‡è®°
            date_marker = " ğŸ“…ã€æ—¥æœŸå­—æ®µã€‘" if likely_date else ""
            
            if field_type in ("str", "int", "float", "bool", "NoneType"):
                example_str = f" = {repr(example)}" if example is not None else " = null"
                lines.append(f"{prefix}- `{key}` ({field_type}){example_str}{date_marker}")
            elif field_type == "list":
                length = info.get("length", "?")
                lines.append(f"{prefix}- `{key}` (list, é•¿åº¦: {length}){date_marker}")
                if "item_structure" in info:
                    lines.append(f"{prefix}  å…ƒç´ ç»“æ„:")
                    lines.append(self._format_field_structure(info["item_structure"], indent + 2))
            elif field_type == "object":
                lines.append(f"{prefix}- `{key}` (object):{date_marker}")
                if "fields" in info:
                    lines.append(self._format_field_structure(info["fields"], indent + 1))
            else:
                lines.append(f"{prefix}- `{key}` ({field_type}){date_marker}")
        
        return "\n".join(lines)

    def _find_date_fields(self, structure: Dict[str, Any], prefix: str = "") -> List[str]:
        """ä»å­—æ®µç»“æ„ä¸­æ‰¾å‡ºæ‰€æœ‰æ—¥æœŸç›¸å…³å­—æ®µ"""
        date_fields = []
        
        if not structure or not isinstance(structure, dict):
            return date_fields
        
        # å¤„ç†åˆ—è¡¨ç»“æ„
        if "_item_structure" in structure:
            return self._find_date_fields(structure["_item_structure"], prefix)
        
        for key, info in structure.items():
            if key.startswith("_"):
                continue
                
            full_key = f"{prefix}.{key}" if prefix else key
            
            if isinstance(info, dict):
                if info.get("likely_date"):
                    date_fields.append(full_key)
                
                # é€’å½’æ£€æŸ¥åµŒå¥—ç»“æ„
                if "item_structure" in info:
                    date_fields.extend(self._find_date_fields(info["item_structure"], full_key))
                if "fields" in info:
                    date_fields.extend(self._find_date_fields(info["fields"], full_key))
        
        return date_fields

    def _summarize_structure(self, structure: Dict[str, Any]) -> str:
        """ç”Ÿæˆé¡µé¢ç»“æ„æ‘˜è¦"""
        lines = []

        # è¡¨æ ¼ä¿¡æ¯
        tables = structure.get("tables", [])
        if tables:
            lines.append(f"### è¡¨æ ¼ ({len(tables)} ä¸ª)")
            for t in tables[:5]:
                lines.append(f"- é€‰æ‹©å™¨: `{t.get('selector')}`, è¡Œæ•°: {t.get('rows')}, åˆ—æ•°: {t.get('columnCount', '?')}")
                if t.get('headers'):
                    lines.append(f"  è¡¨å¤´: {', '.join(t.get('headers', [])[:8])}")
                
                # ã€æ–°å¢ã€‘æ˜¾ç¤ºæ—¥æœŸåˆ—ä½ç½®æç¤ºï¼ˆä»…ä¾›å‚è€ƒï¼Œå®é™…ä»£ç åº”ä½¿ç”¨æ™ºèƒ½æ‰«æï¼‰
                date_hints = t.get('dateColumnHints', [])
                if date_hints:
                    hint_texts = [f"åˆ—{h.get('columnIndex')}({h.get('headerText', '?')})" for h in date_hints[:3]]
                    lines.append(f"  âš ï¸ æ—¥æœŸå¯èƒ½åœ¨: {', '.join(hint_texts)} â€” **ä½†ä¸è¦ç¡¬ç¼–ç åˆ—ç´¢å¼•ï¼ä½¿ç”¨ `_pygen_smart_find_date_in_row_*` æ™ºèƒ½æ‰«æ**")
                
                # ä¸‹è½½åˆ—æç¤º
                download_cols = t.get('downloadColumnIndices', [])
                if download_cols:
                    lines.append(f"  ä¸‹è½½é“¾æ¥å¯èƒ½åœ¨: åˆ—{', åˆ—'.join(map(str, download_cols[:3]))}")

        # åˆ—è¡¨ä¿¡æ¯
        lists = structure.get("lists", [])
        if lists:
            lines.append(f"\n### åˆ—è¡¨ ({len(lists)} ä¸ª)")
            for l in lists[:5]:
                lines.append(f"- é€‰æ‹©å™¨: `{l.get('selector')}`, é¡¹æ•°: {l.get('itemCount')}")

        # é“¾æ¥ä¿¡æ¯
        links = structure.get("links", {})
        if links:
            pdf_links = links.get("pdfLinks", [])
            report_links = links.get("reportLinks", [])
            lines.append(f"\n### é“¾æ¥")
            lines.append(f"- æ€»é“¾æ¥æ•°: {links.get('totalLinks', 0)}")
            lines.append(f"- PDF/ä¸‹è½½é“¾æ¥: {len(pdf_links)} ä¸ª")
            lines.append(f"- æŠ¥å‘Šç›¸å…³é“¾æ¥: {len(report_links)} ä¸ª")

            if pdf_links[:3]:
                lines.append("- PDFé“¾æ¥ç¤ºä¾‹:")
                for pl in pdf_links[:3]:
                    lines.append(f"  - {pl.get('text', '')[:50]}: {pl.get('href', '')[:100]}")

        # åˆ†é¡µä¿¡æ¯
        pagination = structure.get("pagination", [])
        if pagination:
            lines.append(f"\n### åˆ†é¡µå…ƒç´  ({len(pagination)} ä¸ª)")
            for p in pagination[:5]:
                lines.append(f"- <{p.get('tag')}> {p.get('text')}")

        # è¡¨å•ä¿¡æ¯
        forms = structure.get("forms", [])
        if forms:
            lines.append(f"\n### è¡¨å• ({len(forms)} ä¸ª)")
            for f in forms[:3]:
                lines.append(f"- é€‰æ‹©å™¨: `{f.get('selector')}`, action: {f.get('action')}, method: {f.get('method')}")

        # ã€æ–°å¢ã€‘æ—¥æœŸå…ƒç´ ä¿¡æ¯
        # è¿™å¯¹äº API ä¸è¿”å›æ—¥æœŸä½† HTML ä¸­æ˜¾ç¤ºæ—¥æœŸçš„æƒ…å†µéå¸¸é‡è¦
        date_elements = structure.get("dateElements", [])
        if date_elements:
            lines.append(f"\n### ğŸ“… é¡µé¢ä¸­æ£€æµ‹åˆ°çš„æ—¥æœŸå…ƒç´  ({len(date_elements)} ä¸ª)")
            lines.append("**é‡è¦**ï¼šå¦‚æœ API å“åº”ä¸­æ²¡æœ‰æ—¥æœŸå­—æ®µï¼Œå¯ä»¥ä» HTML ä¸­æå–è¿™äº›æ—¥æœŸï¼")
            for de in date_elements[:5]:
                lines.append(f"- æ—¥æœŸå€¼: `{de.get('dateValue')}`, é€‰æ‹©å™¨: `{de.get('selector')}`, æ ‡ç­¾: {de.get('tag')}")
            if len(date_elements) > 5:
                lines.append(f"  ... è¿˜æœ‰ {len(date_elements) - 5} ä¸ªæ—¥æœŸå…ƒç´ ")

        # ã€æ–°å¢ã€‘â€œæ¡ç›®-æ—¥æœŸâ€å…³è”æ ·æœ¬ï¼šæ¯”å•ä¸ª dateElements æ›´å¯ç”¨ï¼ˆå¯åš joinï¼Œè€Œä¸æ˜¯é é¡ºåºçŒœï¼‰
        date_item_samples = structure.get("dateItemSamples", [])
        if date_item_samples:
            lines.append(f"\n### ğŸ“…ğŸ“„ æ—¥æœŸ-æ¡ç›®å…³è”æ ·æœ¬ ({len(date_item_samples)} ä¸ª)")
            lines.append("**å…³é”®**ï¼šè¿™äº›æ ·æœ¬å±•ç¤ºäº†â€œæ ‡é¢˜/æ¡ç›®å®¹å™¨â€ä¸â€œæ—¥æœŸâ€çš„å¯¹åº”å…³ç³»ã€‚è‹¥ API æ— æ—¥æœŸæˆ–æ—¥æœŸå­—æ®µä¸º nullï¼Œåº”ä¼˜å…ˆç”¨æµè§ˆå™¨æ¸²æŸ“åçš„ DOM æŒ‰æ­¤æ–¹å¼æå–å¹¶å…³è”ã€‚")
            for s in date_item_samples[:6]:
                title = (s.get("title") or "")[:60]
                lines.append(
                    f"- æ ‡é¢˜: `{title}` | æ—¥æœŸ: `{s.get('dateValue')}` | å®¹å™¨: `{s.get('containerSelector')}` | æ—¥æœŸèŠ‚ç‚¹: `{s.get('dateSelector')}`"
                )

        # SPA çº¿ç´¢ï¼ˆæé†’æ¨¡å‹ä¸è¦ç”¨ requests æŠ“â€œæ¸²æŸ“åHTMLâ€ï¼‰
        spa_hints = structure.get("spaHints", {})
        if isinstance(spa_hints, dict) and (spa_hints.get("hasHashRoute") or spa_hints.get("hasAppRoot")):
            lines.append("\n### ğŸ§© SPA çº¿ç´¢")
            lines.append(f"- hasHashRoute: {bool(spa_hints.get('hasHashRoute'))}")
            lines.append(f"- hasAppRoot: {bool(spa_hints.get('hasAppRoot'))}")

        return "\n".join(lines) if lines else "æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„ç»“æ„åŒ–å…ƒç´ "

    def _summarize_enhanced_analysis(self, enhanced_analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆå¢å¼ºåˆ†ææ‘˜è¦"""
        lines = []
        
        # 1. ã€æ ¸å¿ƒã€‘å·²éªŒè¯çš„åˆ†ç±»æ˜ å°„è¡¨
        verified_mapping = enhanced_analysis.get("verified_category_mapping", {})
        if verified_mapping and isinstance(verified_mapping, dict):
            menu_to_filters = verified_mapping.get("menu_to_filters", {})
            if menu_to_filters:
                lines.append("### ã€æ ¸å¿ƒæŒ‡ä»¤ã€‘å¿…é¡»éå†çš„åˆ†ç±»æ˜ å°„è¡¨")
                lines.append("è‡ªåŠ¨æ¢æµ‹å·²ç¡®è®¤ä»¥ä¸‹æ¿å—åŠå…¶å¯¹åº”çš„è¯·æ±‚å‚æ•°ï¼Œç”Ÿæˆçš„ä»£ç **å¿…é¡»éå†**è¿™äº›åˆ†ç±»ï¼š\n")
                lines.append("```json")
                lines.append(json.dumps(menu_to_filters, ensure_ascii=False, indent=2))
                lines.append("```")
                lines.append("\n**è¦æ±‚**ï¼šè¯·ç›´æ¥å°†ä¸Šè¿° JSON å¤åˆ¶åˆ°ä»£ç ä¸­çš„ `CATEGORIES` å­—å…¸ï¼Œå¹¶ä½¿ç”¨å¾ªç¯éå†æŠ“å–ã€‚")
        
        # æ•°æ®çŠ¶æ€
        data_status = enhanced_analysis.get("data_status", {})
        if data_status:
            lines.append("### æ•°æ®åŠ è½½çŠ¶æ€\n")
            lines.append(f"- **hasDataï¼ˆæ˜¯å¦æœ‰æ•°æ®ï¼‰**: {data_status.get('hasData', False)}")
            lines.append(f"- è¡¨æ ¼æ•°æ®è¡Œæ•°: {data_status.get('tableRowCount', 0)}")
            lines.append(f"- åˆ—è¡¨é¡¹æ•°é‡: {data_status.get('listItemCount', 0)}")
            
            empty_indicators = data_status.get('emptyIndicators', [])
            if empty_indicators:
                lines.append(f"- ç©ºæ•°æ®æŒ‡ç¤º: {empty_indicators}")
            
            menus = data_status.get('potentialMenus', [])
            if menus:
                lines.append(f"\n- **æ£€æµ‹åˆ°çš„èœå•é¡¹** ({len(menus)} ä¸ª):")
                for menu in menus[:15]:
                    lines.append(f"  - {menu.get('text', '')}")
        
        # å»ºè®®
        recommendations = enhanced_analysis.get("recommendations", [])
        if recommendations:
            lines.append("\n### ç³»ç»Ÿå»ºè®®\n")
            for rec in recommendations:
                lines.append(f"- âš ï¸ {rec}")
        
        return "\n".join(lines)

    def _extract_code_from_response(self, content: str) -> str:
        """ä»LLMå“åº”ä¸­æå–Pythonä»£ç """
        # å°è¯•æå– ```python ... ``` ä»£ç å—
        pattern = r'```python\s*(.*?)\s*```'
        matches = re.findall(pattern, content, re.DOTALL)

        if matches:
            # è¿”å›æœ€é•¿çš„ä»£ç å—ï¼ˆé€šå¸¸æ˜¯ä¸»ä»£ç ï¼‰
            return max(matches, key=len)

        # å¦‚æœæ²¡æœ‰ä»£ç å—æ ‡è®°ï¼Œå°è¯•æå–æ•´ä¸ªå†…å®¹
        lines = content.split('\n')
        code_lines = []
        in_code = False

        for line in lines:
            if line.strip().startswith('import ') or line.strip().startswith('from ') or line.strip().startswith('#'):
                in_code = True
            if in_code:
                code_lines.append(line)

        if code_lines:
            return '\n'.join(code_lines)

        return content

    def _generate_fallback_script(self, page_url: str, run_mode: str = "enterprise_report") -> str:
        """ç”Ÿæˆå¤‡ç”¨è„šæœ¬æ¨¡æ¿
        
        Args:
            page_url: ç›®æ ‡URL
            run_mode: è¿è¡Œæ¨¡å¼ ('enterprise_report' | 'news_sentiment')
        """
        if run_mode == "news_sentiment":
            return self._generate_news_fallback_script(page_url)
        
        return f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Fallback crawler script
Target URL: {page_url}

Note: This is a fallback template, generated when LLM fails.
Please modify the code according to actual page structure.
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
import os
from datetime import datetime

# Configuration
BASE_URL = "{page_url}"
HEADERS = {{
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}}

# Date extraction patterns
_DATE_PATTERNS = [
    re.compile(r'(\\d{{4}})[-/\\.](\\d{{1,2}})[-/\\.](\\d{{1,2}})'),
    re.compile(r'(\\d{{4}})å¹´(\\d{{1,2}})æœˆ(\\d{{1,2}})æ—¥'),
]

def _normalize_date(date_str: str) -> str:
    """Normalize date string to YYYY-MM-DD format"""
    if not date_str:
        return ""
    for pattern in _DATE_PATTERNS:
        match = pattern.search(date_str)
        if match:
            groups = match.groups()
            if len(groups[0]) == 4:
                year, month, day = groups[0], groups[1], groups[2]
                try:
                    if 1900 <= int(year) <= 2100 and 1 <= int(month) <= 12 and 1 <= int(day) <= 31:
                        return f"{{year}}-{{str(month).zfill(2)}}-{{str(day).zfill(2)}}"
                except:
                    pass
    return ""

def _smart_find_date_in_row(tds) -> str:
    """Smart date extraction: scan all columns in a table row"""
    for td in tds:
        try:
            # Try span, time elements first
            for tag in ['span', 'time']:
                elem = td.select_one(tag) if hasattr(td, 'select_one') else None
                if elem:
                    text = elem.get_text(strip=True)
                    date = _normalize_date(text)
                    if date:
                        return date
            # Try date/time class elements
            for sel in ['.date', '.time', '[class*="date"]', '[class*="time"]']:
                try:
                    elem = td.select_one(sel) if hasattr(td, 'select_one') else None
                    if elem:
                        text = elem.get_text(strip=True)
                        date = _normalize_date(text)
                        if date:
                            return date
                except:
                    pass
            # Try direct td text
            text = td.get_text(strip=True) if hasattr(td, 'get_text') else str(td)
            date = _normalize_date(text)
            if date:
                return date
        except:
            continue
    return ""

def fetch_page(url: str) -> str:
    """Fetch page content"""
    resp = requests.get(url, headers=HEADERS, timeout=30)
    resp.raise_for_status()
    return resp.text

def parse_list(html: str) -> list:
    """Parse list page"""
    soup = BeautifulSoup(html, "html.parser")
    results = []

    # Find table rows (with fallback for tables without tbody)
    rows = soup.select("table tbody tr") or soup.select("table tr")

    for row in rows[1:]:  # Skip header row
        cols = row.select("td")
        if cols and len(cols) >= 2:
            # Extract name from first column
            name_elem = cols[0].select_one("a") or cols[0]
            name = name_elem.get_text(strip=True) if name_elem else ""
            
            # Smart date extraction - scan all columns
            date = _smart_find_date_in_row(cols)
            
            # Extract download URL from last column or any column with PDF link
            download_url = ""
            for col in reversed(cols):
                link = col.select_one("a")
                if link and link.get("href"):
                    href = link.get("href", "")
                    if href and ('.pdf' in href.lower() or '/download' in href.lower() or 
                                href.startswith('http') or href.startswith('/')):
                        download_url = href
                        break
            
            if name and download_url:
                results.append({{
                    "name": name,
                    "date": date,
                    "downloadUrl": download_url,
                    "fileType": "pdf"
                }})

    return results

def save_results(data: list, output_dir: str = "."):
    """Save results to JSON file"""
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(output_dir, f"crawl_result_{{timestamp}}.json")
    
    # æ„å»ºä¸‹è½½å¤´ä¿¡æ¯ï¼ˆä¾›åç»­ä¸‹è½½ PDF/é™„ä»¶æ—¶ä½¿ç”¨ï¼Œç»•è¿‡é˜²ç›—é“¾ 403ï¼‰
    from urllib.parse import urlsplit
    _p = urlsplit(BASE_URL)
    _origin = "{{}}://{{}}".format(_p.scheme or "https", _p.netloc)
    download_headers = {{
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Referer": BASE_URL or _origin + "/",
    }}
    
    result = {{
        "total": len(data),
        "crawlTime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "downloadHeaders": download_headers,
        "reports": data
    }}
    
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"[OK] Saved {{len(data)}} records to {{filename}}")
    return filename

def main():
    """Main function"""
    print(f"Starting crawl: {{BASE_URL}}")

    try:
        html = fetch_page(BASE_URL)
        results = parse_list(html)

        if results:
            output_dir = os.path.dirname(os.path.abspath(__file__))
            output_dir = os.path.join(output_dir, "output")
            save_results(results, output_dir)
        else:
            print("[WARN] No data extracted, please check parsing logic")

    except Exception as e:
        print(f"[ERROR] Crawl failed: {{e}}")

if __name__ == "__main__":
    main()
'''

    def _generate_news_fallback_script(self, page_url: str) -> str:
        """ç”Ÿæˆæ–°é—»èˆ†æƒ…æ¨¡å¼çš„å¤‡ç”¨è„šæœ¬æ¨¡æ¿"""
        return f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
News Crawler Fallback Script
Target URL: {page_url}

Note: This is a fallback template for news crawling.
Please modify the code according to actual page structure.
"""

import json
import os
import re
from datetime import datetime

# ä½¿ç”¨ Playwright å¤„ç†åŠ¨æ€é¡µé¢
try:
    from playwright.sync_api import sync_playwright
    HAS_PLAYWRIGHT = True
    try:
        from playwright_stealth import stealth_sync
    except ImportError:
        stealth_sync = None
except ImportError:
    HAS_PLAYWRIGHT = False
    stealth_sync = None
    print("[WARN] Playwright not installed, trying requests...")
    import requests
    from bs4 import BeautifulSoup

# Configuration
BASE_URL = "{page_url}"

    def crawl_with_playwright():
    """ä½¿ç”¨ Playwright çˆ¬å–åŠ¨æ€é¡µé¢"""
    articles = []
    
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=["--no-sandbox", "--disable-blink-features=AutomationControlled"]
        )
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        
        # åº”ç”¨åçˆ¬å…œåº•
        if stealth_sync:
            try:
                stealth_sync(page)
            except: pass

        try:
            page.goto(BASE_URL, wait_until="domcontentloaded", timeout=60000)
            page.wait_for_timeout(3000)  # ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½
            
            # å°è¯•ç»•è¿‡ WAF (ç®€å•æ»šåŠ¨)
            page.evaluate("window.scrollBy(0, document.body.scrollHeight)")
            page.wait_for_timeout(2000)

            # å°è¯•å¤šç§å¸¸è§çš„æ–°é—»åˆ—è¡¨é€‰æ‹©å™¨
            selectors = [
                'ul li a', '.news-list a', '.article-list a',
                '[class*="news"] a', '[class*="article"] a',
                '.list a', 'a[href*="article"]', 'a[href*="news"]'
            ]
            
            for selector in selectors:
                links = page.query_selector_all(selector)
                if len(links) > 3:  # æ‰¾åˆ°è¶³å¤Ÿå¤šçš„é“¾æ¥
                    for link in links[:50]:  # æœ€å¤šå–50æ¡
                        try:
                            title = link.inner_text().strip()
                            href = link.get_attribute('href') or ''
                            
                            if title and len(title) > 5 and href:
                                # è¡¥å…¨ç›¸å¯¹é“¾æ¥
                                if href.startswith('/'):
                                    from urllib.parse import urljoin
                                    href = urljoin(BASE_URL, href)
                                
                                articles.append({{
                                    "title": title,
                                    "sourceUrl": href,
                                    "date": datetime.now().strftime("%Y-%m-%d"),
                                    "source": "",
                                    "author": "",
                                    "summary": ""
                                }})
                        except:
                            continue
                    
                    if articles:
                        break
        finally:
            browser.close()
    
    return articles

def crawl_with_requests():
    """ä½¿ç”¨ requests çˆ¬å–é™æ€é¡µé¢"""
    articles = []
    headers = {{
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }}
    
    try:
        resp = requests.get(BASE_URL, headers=headers, timeout=30)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨
        for link in soup.select('a')[:100]:
            title = link.get_text(strip=True)
            href = link.get('href', '')
            
            if title and len(title) > 10 and href:
                if href.startswith('/'):
                    from urllib.parse import urljoin
                    href = urljoin(BASE_URL, href)
                
                articles.append({{
                    "title": title,
                    "sourceUrl": href,
                    "date": "",
                    "source": "",
                    "author": "",
                    "summary": ""
                }})
    except Exception as e:
        print(f"[ERROR] {{e}}")
    
    return articles

def save_results(articles, output_dir):
    """ä¿å­˜ç»“æœä¸º JSON æ ¼å¼"""
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(output_dir, f"news_{{timestamp}}.json")
    
    result = {{
        "total": len(articles),
        "crawlTime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "articles": articles
    }}
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"[OK] Saved {{len(articles)}} news to {{filename}}")
    return filename

def main():
    print(f"[INFO] Starting news crawl: {{BASE_URL}}")
    
    if HAS_PLAYWRIGHT:
        articles = crawl_with_playwright()
    else:
        articles = crawl_with_requests()
    
    if articles:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output")
        save_results(articles, output_dir)
        print(f"[SUCCESS] Crawled {{len(articles)}} news articles")
    else:
        print("[WARN] No news extracted, please check page structure")

if __name__ == "__main__":
    main()
'''

    def analyze_menu_for_probing(
        self,
        menu_tree: Dict[str, Any],
        screenshot_base64: Optional[str] = None
    ) -> List[str]:
        """
        åˆ†æç›®å½•æ ‘å’Œæˆªå›¾ï¼Œå†³å®šéœ€è¦æ¢æµ‹å“ªäº›æ¿å—
        
        Args:
            menu_tree: é¡µé¢ç›®å½•æ ‘ç»“æ„
            screenshot_base64: é¡µé¢æˆªå›¾ï¼ˆBase64ï¼‰
            
        Returns:
            List[str]: éœ€è¦æ¢æµ‹çš„èœå•è·¯å¾„åˆ—è¡¨
        """
        import json
        
        # æå–å¶å­è·¯å¾„ä¾›é€‰æ‹©
        leaf_paths = menu_tree.get("leaf_paths", [])
        if not leaf_paths:
            return []
            
        # å¦‚æœå¶å­å¤ªå¤šï¼Œæˆªæ–­å±•ç¤ºä»¥é˜² Prompt è¿‡å¤§
        leaf_paths_display = leaf_paths[:200]
        truncated_msg = f"\n(è¿˜æœ‰ {len(leaf_paths) - 200} ä¸ªè·¯å¾„æœªæ˜¾ç¤º)" if len(leaf_paths) > 200 else ""
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çˆ¬è™«åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯è¾…åŠ©çˆ¬è™«ç¨‹åºå†³å®šâ€œéœ€è¦ç‚¹å‡»æ¢æµ‹å“ªäº›èœå•é¡¹â€ã€‚
ç¨‹åºä¼šè‡ªåŠ¨ç‚¹å‡»ä½ é€‰ä¸­çš„èœå•ï¼Œåˆ†æå…¶ç½‘ç»œè¯·æ±‚ï¼ˆæŠ“åŒ…ï¼‰ï¼Œä»è€Œé€†å‘å‡ºç½‘ç«™çš„ API å‚æ•°è§„å¾‹ã€‚

å†³ç­–åŸåˆ™ï¼š
1. **ã€æœ€é«˜ä¼˜å…ˆçº§ã€‘ä¸¥æ ¼éµå¾ªæˆªå›¾æŒ‡ç¤º**ï¼š
   - å¦‚æœç”¨æˆ·æä¾›äº†æˆªå›¾ï¼ˆé€šå¸¸åŒ…å«çº¢æ¡†ã€ç®­å¤´æˆ–é«˜äº®ï¼‰ï¼Œä½ **å¿…é¡»ä¸”åªèƒ½**é€‰æ‹©æˆªå›¾é‡Œæ˜ç¡®å±•ç¤ºå‡ºçš„æ¿å—ã€‚
   - **ä¸¥ç¦**é€‰æ‹©æˆªå›¾é‡Œä¸å­˜åœ¨ã€è¢«æŠ˜å æˆ–æœªå±•ç¤ºçš„èœå•é¡¹ã€‚
   - ä¾‹å¦‚ï¼šæˆªå›¾çº¢æ¡†åªæ¡†é€‰äº†â€œä¼ä¸šè¯„çº§â€ä¸‹çš„å­èœå•ï¼Œä½ å°±ç»ä¸èƒ½é€‰â€œé‡‘èæœºæ„è¯„çº§â€æˆ–â€œåœ°æ–¹æ”¿åºœå€ºâ€ã€‚

2. **ã€å¼ºåˆ¶ã€‘å­èœå•å¿…é¡»å…¨éƒ¨é€‰ä¸­ï¼ˆç¦æ­¢æŠ½æ ·/é‡‡æ ·ï¼‰**ï¼š
   - å¯¹äºæˆªå›¾é‡Œæ¡†é€‰çš„ä¸»èœå•ï¼ˆå¦‚"ä¼ä¸šè¯„çº§"ï¼‰ï¼Œ**å¿…é¡»**é€‰ä¸­å…¶ä¸‹çš„**æ‰€æœ‰**å¯è§å­èœå•ï¼Œ**ä¸å…è®¸**åªé€‰éƒ¨åˆ†ä½œä¸º"ä»£è¡¨æ€§æ ·æœ¬"ã€‚
   - åŸå› ï¼šæ¯ä¸ªå­èœå•çš„ API åˆ†ç±» ID æ˜¯**ä¸å¯é¢„æµ‹çš„**ï¼ˆå¦‚"ä¸»ä½“è¯„çº§"ID=121ï¼Œ"å…¬å¸å€ºåˆ¸"ID=200ï¼‰ï¼Œæ— æ³•é€šè¿‡æ•°å­—è§„å¾‹æ¨ç†ï¼Œå¿…é¡»é€ä¸€æ¢æµ‹ã€‚
   - ä¾‹å¦‚ï¼šå¦‚æœ"ä¼ä¸šè¯„çº§"ä¸‹æœ‰ 8 ä¸ªå­èœå•ï¼Œä½ **å¿…é¡»**å…¨éƒ¨é€‰ä¸­è¿™ 8 ä¸ªï¼Œä¸èƒ½åªé€‰ 4 ä¸ªã€‚

3. **æ•°æ®å¯†é›†å‹ä¼˜å…ˆ**ï¼šåœ¨æˆªå›¾èŒƒå›´å†…ï¼Œä¼˜å…ˆé€‰"å…¬å‘Š"ã€"ç ”æŠ¥"ã€"è¯„çº§ç»“æœ"ç­‰å«æ•°æ®çš„æ¿å—ã€‚

4. **å»é‡**ï¼šå¦‚æœå¤šä¸ªæ¿å—é«˜åº¦ç›¸ä¼¼ï¼ˆå¦‚æŒ‰å¹´ä»½åˆ†çš„ 2023/2024/2025ï¼‰ï¼Œå¯é€‰æœ€æ–°å¹´ä»½çš„æ¿å—ã€‚ä½†**ä¸åŒåç§°çš„åˆ†ç±»ç»å¯¹ä¸èƒ½å»é‡**ã€‚
"""

        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹ç½‘ç«™ç›®å½•æ ‘ï¼ˆåŠå‚è€ƒæˆªå›¾ï¼‰ï¼Œé€‰å‡ºæœ€å€¼å¾—æ¢æµ‹çš„â€œæ•°æ®åˆ—è¡¨/ä¸šåŠ¡æ¿å—â€è·¯å¾„ã€‚

## å¯é€‰è·¯å¾„åˆ—è¡¨ (JSON)
```json
{json.dumps(leaf_paths_display, ensure_ascii=False, indent=2)}
```
{truncated_msg}

## ä½ çš„ä»»åŠ¡
ä»ä¸Šè¿°åˆ—è¡¨ä¸­æŒ‘é€‰å‡ºéœ€è¦æ¢æµ‹çš„è·¯å¾„ã€‚
**é‡è¦**ï¼šå¦‚æœæä¾›äº†æˆªå›¾ï¼Œè¯·**ä¸¥æ ¼åªé€‰æ‹©æˆªå›¾é‡Œå±•ç¤ºå‡ºæ¥çš„æ¿å—**ï¼ˆåŒ…æ‹¬å…¶å­èœå•ï¼‰ã€‚ä¸è¦é€‰æ‹©æˆªå›¾é‡Œçœ‹ä¸è§æˆ–æœªå±•ç¤ºçš„æ¿å—ã€‚

## è¾“å‡ºè¦æ±‚
**åªè¾“å‡ºä¸€ä¸ªçº¯ JSON å­—ç¬¦ä¸²æ•°ç»„**ï¼Œä¸è¦åŒ…å« markdown æ ‡è®°æˆ–ä»»ä½•è§£é‡Šã€‚
æ ¼å¼ç¤ºä¾‹ï¼š
["ä¸€çº§å¸‚åœº/è¯„çº§ç»“æœ", "ä¸€çº§å¸‚åœº/è¯„çº§å…¬å‘Š"]
"""

        attachments = []
        if screenshot_base64:
            attachments.append(AttachmentData(
                filename="page_screenshot.jpg",
                base64_data=screenshot_base64,
                mime_type="image/jpeg"
            ))
            
        try:
            print(f"{self._dbg_prefix()} æ­£åœ¨è°ƒç”¨ LLM è¿›è¡Œèœå•æ¢æµ‹å†³ç­–...")
            response = self._call_llm(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                attachments=attachments,
                temperature=0.1
            )
            
            # æ¸…ç†å¯èƒ½çš„ markdown æ ‡è®°
            cleaned = response.strip()
            if cleaned.startswith("```json"):
                cleaned = cleaned[7:]
            if cleaned.startswith("```"):
                cleaned = cleaned[3:]
            if cleaned.endswith("```"):
                cleaned = cleaned[:-3]
            cleaned = cleaned.strip()
            
            selected_paths = json.loads(cleaned)
            if isinstance(selected_paths, list):
                # è¿‡æ»¤æ‰ä¸åœ¨åŸå§‹åˆ—è¡¨ä¸­çš„å¹»è§‰è·¯å¾„
                valid_paths = [p for p in selected_paths if p in leaf_paths]
                print(f"{self._dbg_prefix()} LLM é€‰ä¸­äº† {len(valid_paths)} ä¸ªæœ‰æ•ˆè·¯å¾„: {valid_paths}")
                return valid_paths
            return []
            
        except Exception as e:
            print(f"âŒ èœå•åˆ†æå¤±è´¥: {e}")
            # å…œåº•ï¼šå¦‚æœ LLM å¤±è´¥ï¼Œé»˜è®¤é€‰å‰ 5 ä¸ªéé¦–é¡µè·¯å¾„
            return [p for p in leaf_paths if "é¦–é¡µ" not in p and "å…³äº" not in p][:5]

    def get_token_usage(self) -> Dict[str, int]:
        """è·å–Tokenä½¿ç”¨ç»Ÿè®¡"""
        return {
            "prompt_tokens": self.total_prompt_tokens,
            "completion_tokens": self.total_completion_tokens,
            "total_tokens": self.total_prompt_tokens + self.total_completion_tokens,
        }
    
    def execute_and_diagnose(
        self,
        script_path: str,
        timeout: int = 120
    ) -> Tuple[bool, Optional[FailureReport]]:
        """
        æ‰§è¡Œè„šæœ¬å¹¶è¯Šæ–­æ•…éšœ
        
        Args:
            script_path: è„šæœ¬è·¯å¾„
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            (æ˜¯å¦æˆåŠŸ, æ•…éšœæŠ¥å‘Š)
        """
        from signals_collector import SignalsCollector, ExecutionStatus
        
        collector = SignalsCollector()
        signals = collector.execute_and_collect(script_path, timeout)
        
        # å¦‚æœæˆåŠŸï¼Œæ— éœ€è¯Šæ–­
        if signals.status == ExecutionStatus.SUCCESS:
            return True, None
        
        # è¯»å–è„šæœ¬å†…å®¹ç”¨äºåˆ†æ
        try:
            with open(script_path, 'r', encoding='utf-8') as f:
                code = f.read()
        except Exception:
            code = None
        
        # åˆ†ç±»æ•…éšœ
        report = self.failure_classifier.classify(signals, code)
        
        return False, report
    
    def generate_with_auto_repair(
        self,
        page_url: str,
        page_html: str,
        page_structure: Dict[str, Any],
        network_requests: Dict[str, List[Dict[str, Any]]],
        user_requirements: Optional[str] = None,
        start_date: str = "",
        end_date: str = "",
        enhanced_analysis: Optional[Dict[str, Any]] = None,
        test_after_generation: bool = False,
        script_save_path: Optional[str] = None
    ) -> Tuple[str, List[str]]:
        """
        ç”Ÿæˆçˆ¬è™«è„šæœ¬å¹¶å¯é€‰æ‰§è¡Œæµ‹è¯•+è‡ªåŠ¨ä¿®å¤
        
        Args:
            page_url: ç›®æ ‡é¡µé¢URL
            page_html: å®Œæ•´çš„é¡µé¢HTML
            page_structure: é¡µé¢ç»“æ„åˆ†æç»“æœ
            network_requests: æ•è·çš„ç½‘ç»œè¯·æ±‚
            user_requirements: ç”¨æˆ·é¢å¤–éœ€æ±‚
            start_date: çˆ¬å–å¼€å§‹æ—¶é—´
            end_date: çˆ¬å–ç»“æŸæ—¶é—´
            enhanced_analysis: å¢å¼ºåˆ†æç»“æœ
            test_after_generation: æ˜¯å¦åœ¨ç”Ÿæˆåæ‰§è¡Œæµ‹è¯•
            script_save_path: è„šæœ¬ä¿å­˜è·¯å¾„ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            
        Returns:
            (æœ€ç»ˆè„šæœ¬, ä¿®å¤æ—¥å¿—)
        """
        repair_history = []
        
        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆè„šæœ¬
        script = self.generate_crawler_script(
            page_url=page_url,
            page_html=page_html,
            page_structure=page_structure,
            network_requests=network_requests,
            user_requirements=user_requirements,
            start_date=start_date,
            end_date=end_date,
            enhanced_analysis=enhanced_analysis
        )
        
        if not test_after_generation or not script_save_path:
            return script, repair_history
        
        # ç¬¬äºŒæ­¥ï¼šä¿å­˜å¹¶æµ‹è¯•
        from signals_collector import SignalsCollector, ExecutionStatus
        
        for attempt in range(self.max_repair_attempts):
            # ä¿å­˜è„šæœ¬
            with open(script_save_path, 'w', encoding='utf-8') as f:
                f.write(script)
            
            # æ‰§è¡Œæµ‹è¯•
            collector = SignalsCollector()
            signals = collector.execute_and_collect(script_save_path, timeout=120)
            
            # æˆåŠŸåˆ™è¿”å›
            if signals.status == ExecutionStatus.SUCCESS:
                repair_history.append(f"âœ… ç¬¬{attempt+1}æ¬¡æ‰§è¡ŒæˆåŠŸ")
                return script, repair_history
            
            # å¤±è´¥åˆ™è¯Šæ–­å¹¶ä¿®å¤
            report = self.failure_classifier.classify(signals, script)
            repair_history.append(f"âŒ ç¬¬{attempt+1}æ¬¡æ‰§è¡Œå¤±è´¥: {report.summary}")
            
            # ç”Ÿæˆä¿®å¤æç¤º
            repair_prompt = report.to_repair_prompt()
            
            # è°ƒç”¨ LLM ä¿®å¤
            try:
                system_prompt = self._build_system_prompt()
                repair_response = self._call_llm(
                    system_prompt=system_prompt,
                    user_prompt=f"ä¿®å¤ä»¥ä¸‹çˆ¬è™«è„šæœ¬:\n\n```python\n{script}\n```\n\n{repair_prompt}",
                    attachments=None,
                    temperature=0.1
                )
                
                new_script = self._extract_code_from_response(repair_response)
                
                if new_script and new_script != script:
                    script = new_script
                    repair_history.append(f"ğŸ”§ ç¬¬{attempt+1}æ¬¡ä¿®å¤å®Œæˆ")
                else:
                    repair_history.append(f"âš ï¸ ç¬¬{attempt+1}æ¬¡ä¿®å¤æœªäº§ç”Ÿå˜åŒ–")
                    break
                    
            except Exception as e:
                repair_history.append(f"âŒ ä¿®å¤è°ƒç”¨å¤±è´¥: {str(e)}")
                break
        
        return script, repair_history
