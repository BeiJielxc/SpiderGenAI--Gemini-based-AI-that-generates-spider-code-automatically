#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PyGen API Server - FastAPI åç«¯

æä¾› REST API ä¾›å‰ç«¯è°ƒç”¨ï¼š
- POST /api/menu-tree: è·å–ç›®æ ‡é¡µé¢çš„ç›®å½•æ ‘
- POST /api/generate: å¯åŠ¨çˆ¬è™«è„šæœ¬ç”Ÿæˆä»»åŠ¡
- GET /api/status/{task_id}: è·å–ä»»åŠ¡çŠ¶æ€å’Œæ—¥å¿—
- GET /api/download/{filename}: ä¸‹è½½ç”Ÿæˆçš„è„šæœ¬æ–‡ä»¶
"""
import asyncio
import uuid
import time
import json
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, StreamingResponse
from pydantic import BaseModel

from config import Config
from chrome_launcher import ChromeLauncher
from browser_controller import BrowserController
from llm_agent import LLMAgent

# ============ Pydantic Models ============

class MenuTreeRequest(BaseModel):
    url: str
    
class MenuTreeResponse(BaseModel):
    url: str
    root: Optional[Dict[str, Any]]
    leaf_paths: List[str]
    
class AttachmentData(BaseModel):
    """é™„ä»¶æ•°æ®ï¼ˆå›¾ç‰‡/æ–‡ä»¶çš„ base64 ç¼–ç ï¼‰"""
    filename: str
    base64: str
    mimeType: str

class GenerateRequest(BaseModel):
    url: str
    startDate: str
    endDate: str
    outputScriptName: str
    extraRequirements: Optional[str] = ""
    siteName: Optional[str] = ""
    listPageName: Optional[str] = ""
    sourceCredibility: Optional[str] = ""  # ä¿¡æ¯æºå¯ä¿¡åº¦ï¼ˆT1/T2/T3ï¼‰
    runMode: str  # 'enterprise_report' | 'news_sentiment'
    crawlMode: str  # 'single_page' | 'multi_page' | 'auto_detect' | 'date_range_api'
    downloadReport: Optional[str] = "yes"  # 'yes' | 'no'
    selectedPaths: Optional[List[str]] = None  # ç”¨æˆ·é€‰ä¸­çš„ç›®å½•è·¯å¾„ï¼ˆä»… multi_page æ¨¡å¼ï¼‰
    attachments: Optional[List[AttachmentData]] = None  # å›¾ç‰‡/æ–‡ä»¶é™„ä»¶

class ReportFile(BaseModel):
    id: str
    name: str
    date: str
    downloadUrl: str
    fileType: str
    localPath: Optional[str] = None  # æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœå·²ä¸‹è½½ï¼‰
    isLocal: bool = False  # æ˜¯å¦æ˜¯æœ¬åœ°æ–‡ä»¶
    category: Optional[str] = None  # æ¥æºæ¿å—ï¼ˆå¤šé¡µçˆ¬å–æ—¶æ ‡è¯†ï¼‰

class NewsArticle(BaseModel):
    """æ–°é—»æ–‡ç« ï¼ˆæ–°é—»èˆ†æƒ…åœºæ™¯ï¼‰"""
    id: str
    title: str
    author: str = ""
    date: str
    source: str
    sourceUrl: str
    summary: Optional[str] = None
    content: Optional[str] = None
    category: Optional[str] = None  # æ¥æºæ¿å—ï¼ˆå¤šé¡µçˆ¬å–æ—¶æ ‡è¯†ï¼‰

class TaskStatusResponse(BaseModel):
    taskId: str
    status: str  # 'pending' | 'running' | 'completed' | 'failed'
    currentStep: int
    totalSteps: int
    stepLabel: str
    logs: List[str]
    resultFile: Optional[str] = None
    error: Optional[str] = None
    # ä¼ä¸šæŠ¥å‘Šåœºæ™¯
    reports: Optional[List[ReportFile]] = None
    downloadedCount: Optional[int] = None  # å·²ä¸‹è½½çš„æ–‡ä»¶æ•°é‡
    filesNotEnough: Optional[bool] = None  # æ—¥æœŸèŒƒå›´å†…æ–‡ä»¶æ˜¯å¦ä¸è¶³5ä»½
    pdfOutputDir: Optional[str] = None  # PDF ä¸‹è½½ç›®å½•
    # æ–°é—»èˆ†æƒ…åœºæ™¯
    newsArticles: Optional[List[NewsArticle]] = None
    markdownFile: Optional[str] = None
    totalCount: Optional[int] = None  # æ€»ç»“æœæ•°ï¼ˆå½“ç»“æœè¢«æˆªæ–­æ—¶ä½¿ç”¨ï¼‰

# ============ å…¨å±€çŠ¶æ€ ============

# ä»»åŠ¡å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redisï¼‰
tasks: Dict[str, Dict[str, Any]] = {}

# ä»»åŠ¡å¯¹åº”çš„å­è¿›ç¨‹ï¼ˆç”¨äºåœæ­¢ä»»åŠ¡ï¼‰
task_processes: Dict[str, Any] = {}

# ä»»åŠ¡å¯¹åº”çš„æµè§ˆå™¨èµ„æºï¼ˆlauncher, browserï¼‰
task_browsers: Dict[str, Dict[str, Any]] = {}

# ä»»åŠ¡å¯¹åº”çš„ asyncio Taskï¼ˆç”¨äºå–æ¶ˆï¼‰
task_asyncio_tasks: Dict[str, asyncio.Task] = {}

# ä»»åŠ¡å–æ¶ˆæ ‡å¿—
task_cancelled: Dict[str, bool] = {}

# é…ç½®
config: Optional[Config] = None


def _is_cancelled(task_id: str) -> bool:
    """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¢«å–æ¶ˆ"""
    return task_cancelled.get(task_id, False)

# ============ ç”Ÿå‘½å‘¨æœŸ ============

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global config
    try:
        config = Config()
        print("âœ“ é…ç½®åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âœ— é…ç½®åŠ è½½å¤±è´¥: {e}")
        config = None
    yield
    # æ¸…ç†
    print("åº”ç”¨å…³é—­")

# ============ FastAPI App ============

app = FastAPI(
    title="PyGen API",
    description="æ™ºèƒ½çˆ¬è™«è„šæœ¬ç”Ÿæˆå™¨ API",
    version="2.0",
    lifespan=lifespan
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173", "http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============ è¾…åŠ©å‡½æ•° ============

def _add_log(task_id: str, message: str):
    """æ·»åŠ æ—¥å¿—åˆ°ä»»åŠ¡"""
    if task_id in tasks:
        timestamp = datetime.now().strftime("%H:%M:%S")
        tasks[task_id]["logs"].append(f"[{timestamp}] {message}")

def _update_step(task_id: str, step: int, label: str):
    """æ›´æ–°ä»»åŠ¡æ­¥éª¤"""
    if task_id in tasks:
        tasks[task_id]["currentStep"] = step
        tasks[task_id]["stepLabel"] = label

# ============ æ ¸å¿ƒé€»è¾‘ï¼šè·å–ç›®å½•æ ‘ ============

async def _fetch_menu_tree(url: str) -> Dict[str, Any]:
    """å¯åŠ¨æµè§ˆå™¨å¹¶è·å–ç›®å½•æ ‘"""
    launcher = None
    browser = None
    
    try:
        # ä½¿ç”¨éšæœºç«¯å£ä»¥é¿å…å¤šè¯·æ±‚é—´çš„èµ„æºå†²çªï¼ˆé˜²æ­¢è¯·æ±‚Aå…³é—­äº†è¯·æ±‚Bæ­£åœ¨å¤ç”¨çš„æµè§ˆå™¨ï¼‰
        import random
        random_port = random.randint(10000, 20000)
        
        # å¯åŠ¨ Chromeï¼ˆheadless æ¨¡å¼ç”± config.yaml ç»Ÿä¸€æ§åˆ¶ï¼‰
        _headless = config.browser_headless if config else False
        launcher = ChromeLauncher(
            debug_port=random_port,
            user_data_dir=None,  # è®© Launcher è‡ªåŠ¨åˆ›å»ºä¸´æ—¶ç›®å½•ï¼Œé¿å… Profile é”å†²çª
            headless=_headless,
            auto_select_port=True
        )
        launcher.launch()
        
        # è¿æ¥æµè§ˆå™¨
        browser = BrowserController(
            cdp_url=launcher.get_ws_endpoint(),
            timeout=config.cdp_timeout if config else 60000
        )
        
        if not await browser.connect():
            # ç®€å•çš„é‡è¯•ä¸€æ¬¡
            await asyncio.sleep(1)
            if not await browser.connect():
                raise RuntimeError("æ— æ³•è¿æ¥åˆ° Chrome æµè§ˆå™¨")
        
        # æ‰“å¼€é¡µé¢
        success, error_msg = await browser.open(url)
        if not success:
            raise RuntimeError(f"æ— æ³•æ‰“å¼€ç›®æ ‡é¡µé¢: {error_msg}")
        
        # æ»šåŠ¨é¡µé¢
        await browser.scroll_page(times=2)
        
        # è·å–ç›®å½•æ ‘
        menu_tree = await browser.enumerate_menu_tree(max_depth=3)
        
        return {
            "url": url,
            "root": menu_tree.get("root"),
            "leaf_paths": menu_tree.get("leaf_paths", [])
        }
        
    finally:
        if browser:
            await browser.disconnect()
        if launcher:
            launcher.terminate()

# ============ æ ¸å¿ƒé€»è¾‘ï¼šç”Ÿæˆçˆ¬è™«è„šæœ¬ ============

async def _run_generation_task(task_id: str, request: GenerateRequest):
    """åå°æ‰§è¡Œçˆ¬è™«è„šæœ¬ç”Ÿæˆ"""
    launcher = None
    browser = None
    
    try:
        tasks[task_id]["status"] = "running"
        task_cancelled[task_id] = False  # åˆå§‹åŒ–å–æ¶ˆæ ‡å¿—

        # è¯»å–é…ç½®ï¼šæ˜¯å¦å¯ç”¨è‡ªåŠ¨ä¿®å¤/æ£€æŸ¥/åå¤„ç†
        auto_repair_enabled = config.llm_auto_repair if config else True
        
        # Step 1: å¯åŠ¨ Chrome
        if _is_cancelled(task_id):
            raise RuntimeError("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")
            
        _update_step(task_id, 0, "æ­£åœ¨å¯åŠ¨Chromeæµè§ˆå™¨")
        _add_log(task_id, "[INFO] æ­£åœ¨å¯åŠ¨ Chrome æµè§ˆå™¨...")
        
        # ä½¿ç”¨éšæœºç«¯å£ä»¥é¿å…å¤šä»»åŠ¡é—´çš„èµ„æºå†²çªï¼ˆé˜²æ­¢ä»»åŠ¡Aå…³é—­äº†ä»»åŠ¡Bæ­£åœ¨å¤ç”¨çš„æµè§ˆå™¨ï¼‰
        import random
        # éšæœºé€‰æ‹©ä¸€ä¸ªç«¯å£èŒƒå›´ï¼Œé¿å…ä¸å¸¸ç”¨ç«¯å£å†²çª
        random_port = random.randint(10000, 20000)
        
        # headless æ¨¡å¼ç”± config.yaml ç»Ÿä¸€æ§åˆ¶
        _headless = config.browser_headless if config else False
        launcher = ChromeLauncher(
            debug_port=random_port,
            user_data_dir=None, # è®© Launcher è‡ªåŠ¨åˆ›å»ºä¸´æ—¶ç›®å½•ï¼Œé¿å… Profile é”å†²çª
            headless=_headless,
            auto_select_port=True
        )
        launcher.launch()
        _add_log(task_id, f"[SUCCESS] Chrome å¯åŠ¨æˆåŠŸ (ç«¯å£ {launcher.actual_port}, headless={_headless})")
        
        # å­˜å‚¨æµè§ˆå™¨èµ„æºå¼•ç”¨ï¼Œä»¥ä¾¿åœæ­¢æ—¶å¯ä»¥å…³é—­
        task_browsers[task_id] = {"launcher": launcher, "browser": None}
        
        # Step 2: è¿æ¥æµè§ˆå™¨
        if _is_cancelled(task_id):
            raise RuntimeError("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")
            
        _update_step(task_id, 1, "æ­£åœ¨è¿æ¥æµè§ˆå™¨")
        ws_url = launcher.get_ws_endpoint()
        _add_log(task_id, f"[INFO] æ­£åœ¨è¿æ¥åˆ° CDP: {ws_url}")
        
        browser = BrowserController(
            cdp_url=ws_url,
            timeout=config.cdp_timeout if config else 60000
        )
        
        # å¢åŠ è¿æ¥é‡è¯•ä¸è‡ªåŠ¨æ¢å¤æœºåˆ¶
        connected = False
        for i in range(3):
            if await browser.connect():
                connected = True
                break
            
            _add_log(task_id, f"[WARNING] æµè§ˆå™¨è¿æ¥å°è¯• {i+1}/3 å¤±è´¥ï¼Œç­‰å¾…é‡è¯•...")
            await asyncio.sleep(2)
            
            # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•å‰ï¼Œä¸”çœ‹èµ·æ¥è¿æ¥å®Œå…¨ä¸é€šï¼Œå°è¯•é‡å¯ Chrome
            if i == 1:
                _add_log(task_id, "[WARNING] è¿æ¥æŒç»­å¤±è´¥ï¼Œå°è¯•é‡å¯ Chrome...")
                try:
                    launcher.terminate()
                    await asyncio.sleep(1)
                    launcher.launch()
                    # æ›´æ–° WS URL
                    browser.cdp_url = launcher.get_ws_endpoint()
                    _add_log(task_id, f"[INFO] Chrome å·²é‡å¯ï¼Œæ–°åœ°å€: {browser.cdp_url}")
                except Exception as e:
                    _add_log(task_id, f"[ERROR] é‡å¯ Chrome å¤±è´¥: {e}")

        if not connected:
            raise RuntimeError("æ— æ³•è¿æ¥åˆ° Chrome æµè§ˆå™¨ï¼Œè¯·æ£€æŸ¥ Chrome æ˜¯å¦æ­£ç¡®å®‰è£…æˆ–ç«¯å£æ˜¯å¦è¢«å ç”¨")

        _add_log(task_id, "[SUCCESS] æµè§ˆå™¨è¿æ¥æˆåŠŸ")
        
        # æ›´æ–°æµè§ˆå™¨å¼•ç”¨
        task_browsers[task_id]["browser"] = browser
        
        # Step 3: æ‰“å¼€ç›®æ ‡é¡µé¢
        if _is_cancelled(task_id):
            raise RuntimeError("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")
            
        _update_step(task_id, 2, "æ­£åœ¨æ‰“å¼€ç›®æ ‡é¡µé¢")
        _add_log(task_id, f"[INFO] æ­£åœ¨æ‰“å¼€: {request.url}")
        
        # ä½¿ç”¨ domcontentloaded ä½œä¸ºé»˜è®¤ç­‰å¾…ç­–ç•¥ï¼Œè€Œä¸æ˜¯ networkidle
        # networkidle å¯¹äºä¸æ–­åŠ è½½èµ„æºçš„é¡µé¢ï¼ˆå¦‚è½®æ’­å›¾ã€å³æ—¶é€šè®¯ï¼‰éå¸¸å®¹æ˜“è¶…æ—¶
        success, error_msg = await browser.open(request.url, wait_until="domcontentloaded")
        if not success:
            _add_log(task_id, f"[ERROR] é¡µé¢æ‰“å¼€å¤±è´¥: {error_msg}")
            raise RuntimeError(f"æ— æ³•æ‰“å¼€ç›®æ ‡é¡µé¢: {error_msg}")
        _add_log(task_id, "[SUCCESS] é¡µé¢åŠ è½½å®Œæˆ")
        
        # Step 4: æ»šåŠ¨é¡µé¢
        if _is_cancelled(task_id):
            raise RuntimeError("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")
            
        _update_step(task_id, 3, "æ­£åœ¨æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå†…å®¹")
        _add_log(task_id, "[INFO] æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½...")
        await browser.scroll_page(times=3)
        _add_log(task_id, "[SUCCESS] é¡µé¢æ»šåŠ¨å®Œæˆ")
        
        # Step 5: åˆ†æé¡µé¢ç»“æ„
        if _is_cancelled(task_id):
            raise RuntimeError("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")
            
        _update_step(task_id, 4, "æ­£åœ¨åˆ†æé¡µé¢ç»“æ„")
        _add_log(task_id, "[INFO] æ­£åœ¨åˆ†æé¡µé¢ç»“æ„...")
        
        # ç«‹å³æ›´æ–°æ­¥éª¤çŠ¶æ€ï¼Œç¡®ä¿å‰ç«¯èƒ½åŠæ—¶çœ‹åˆ°
        await asyncio.sleep(0.1)  # ç»™å‰ç«¯ä¸€ç‚¹æ—¶é—´æ¥æ”¶æ›´æ–°
        
        page_info = await browser.get_page_info()
        page_html = await browser.get_full_html()
        page_structure = await browser.analyze_page_structure()
        network_requests = browser.get_captured_requests()
        
        _add_log(task_id, f"[INFO] é¡µé¢æ ‡é¢˜: {page_info.get('title', 'æœªçŸ¥')}")
        _add_log(task_id, f"[INFO] HTML é•¿åº¦: {len(page_html):,} å­—ç¬¦")
        _add_log(task_id, f"[INFO] æ•è·è¯·æ±‚: {len(network_requests.get('all_requests', []))} ä¸ª")
        
        # Step 6: æ ¹æ®æ¨¡å¼å¤„ç†
        enhanced_analysis = None
        verified_mapping = None

        # è¯´æ˜ï¼š
        # - multi_pageï¼šç”¨æˆ·æ‰‹åŠ¨é€‰ç›®å½•æ ‘ï¼Œä»…å¯¹â€œé€‰ä¸­ç›®å½•â€æŠ“åŒ…è¿˜åŸå‚æ•°ï¼ˆå¯ä¿¡æ˜ å°„ï¼‰ï¼Œä¸åšé€šç”¨äº¤äº’æ¢æµ‹
        # - auto_detectï¼šè‡ªåŠ¨æ¢æµ‹æ¿å—å¹¶çˆ¬å–ï¼Œä¼šæ‰§è¡Œ enhanced_page_analysis() çš„é€šç”¨äº¤äº’æ¢æµ‹
        if request.crawlMode == "multi_page":
            _update_step(task_id, 5, "æ­£åœ¨æ„å»ºå¯ä¿¡åˆ†ç±»æ˜ å°„ï¼ˆé€‰ä¸­ç›®å½•ï¼‰")
            _add_log(task_id, "[INFO] å¤šé¡µæ¨¡å¼ï¼šä»…å¯¹é€‰ä¸­ç›®å½•æŠ“åŒ…è¿˜åŸå‚æ•°ï¼ˆä¸åšé€šç”¨äº¤äº’æ¢æµ‹ï¼‰")

            # æšä¸¾ç›®å½•æ ‘ï¼ˆä»…æšä¸¾ï¼Œä¸æŠ“åŒ…ï¼‰
            menu_tree = await browser.enumerate_menu_tree(max_depth=3)
            leaf_paths = menu_tree.get("leaf_paths", [])

            # æ ¹æ®ç”¨æˆ·é€‰æ‹©è¿‡æ»¤
            selected_paths = request.selectedPaths or leaf_paths
            _add_log(task_id, f"[INFO] ç›®å½•å¶å­æ€»æ•°: {len(leaf_paths)}ï¼Œç”¨æˆ·é€‰ä¸­: {len(selected_paths)}")

            # å¯¹é€‰ä¸­ç›®å½•æŠ“åŒ…è¿˜åŸå‚æ•°ï¼ˆå¯ä¿¡æ˜ å°„ï¼‰
            _add_log(task_id, "[INFO] æ­£åœ¨å¯¹é€‰ä¸­ç›®å½•æŠ“åŒ…è¿˜åŸå‚æ•°...")
            verified_mapping = await browser.capture_mapping_for_leaf_paths(selected_paths)
            menu_to_filters = (verified_mapping or {}).get("menu_to_filters", {}) if isinstance(verified_mapping, dict) else {}
            menu_to_urls = (verified_mapping or {}).get("menu_to_urls", {}) if isinstance(verified_mapping, dict) else {}
            mapping_count = len(menu_to_filters) + len(menu_to_urls)

            enhanced_analysis = {
                "menu_tree": menu_tree,
                "selected_leaf_paths": selected_paths,
                "verified_category_mapping": verified_mapping,
                # å…³é”®ï¼šæŠŠâ€œé€‰ä¸­ç›®å½•ç‚¹å‡»åâ€çš„çœŸå® API è¯·æ±‚æ ·æœ¬ä¹Ÿç»™åˆ° LLMï¼ˆé¿å…çŒœæµ‹æ¥å£/å‚æ•°ï¼‰
                "interaction_apis": (verified_mapping or {}).get("interaction_apis", {}),
                "recommendations": [
                    f"å·²å¯¹é€‰ä¸­ç›®å½•æ„å»ºå¯ä¿¡æ˜ å°„ï¼š{mapping_count} ä¸ªï¼ˆfilters={len(menu_to_filters)}ï¼Œurls={len(menu_to_urls)}ï¼‰"
                ],
            }

            _add_log(task_id, f"[SUCCESS] å·²æ„å»ºåˆ†ç±»æ˜ å°„: {mapping_count} ä¸ªï¼ˆfilters={len(menu_to_filters)}ï¼Œurls={len(menu_to_urls)}ï¼‰")

        elif request.crawlMode == "auto_detect":
            _update_step(task_id, 5, "æ­£åœ¨æ‰§è¡Œæ™ºèƒ½æ¿å—æ¢æµ‹ (Phase 1: å†³ç­–)")
            _add_log(task_id, "[INFO] è‡ªåŠ¨æ¢æµ‹æ¨¡å¼ï¼šå¯åŠ¨æ™ºèƒ½å†³ç­–æµç¨‹...")

            # 1. è·å–ç›®å½•æ ‘å’Œæˆªå›¾
            menu_tree = await browser.enumerate_menu_tree(max_depth=3)
            screenshot = await browser.take_screenshot_base64()
            _add_log(task_id, f"[INFO] å·²æå–ç›®å½•æ ‘ ({len(menu_tree.get('leaf_paths', []))} ä¸ªå¶å­) å’Œé¡µé¢æˆªå›¾")

            # 2. åˆå§‹åŒ– LLM (æå‰å®ä¾‹åŒ–ç”¨äºå†³ç­–)
            temp_llm = LLMAgent(
                api_key=config.qwen_api_key if config else "",
                model=config.qwen_model if config else "qwen-max",
                base_url=config.qwen_base_url if config else None,
                enable_auto_repair=False # å†³ç­–é˜¶æ®µä¸éœ€è¦ä¿®ä»£ç 
            )

            # 3. LLM å†³ç­–
            _add_log(task_id, "[INFO] æ­£åœ¨è¯·æ±‚ LLM å†³ç­–æ¢æµ‹ç›®æ ‡...")
            selected_paths = temp_llm.analyze_menu_for_probing(menu_tree, screenshot)
            
            if not selected_paths:
                _add_log(task_id, "[WARNING] LLM æœªé€‰ä¸­ä»»ä½•è·¯å¾„ï¼Œå°†å›é€€åˆ°é€šç”¨æ¢æµ‹")
                # å›é€€åˆ°æ—§çš„å¢å¼ºåˆ†æ
                enhanced_analysis = await browser.enhanced_page_analysis()
                verified_mapping = enhanced_analysis.get("verified_category_mapping")
            else:
                _add_log(task_id, f"[SUCCESS] LLM å†³ç­–é€‰ä¸­ {len(selected_paths)} ä¸ªæ¢æµ‹ç›®æ ‡: {selected_paths}")
                
                # 4. æ‰§è¡Œå®šå‘æŠ“åŒ… (Phase 2)
                _update_step(task_id, 5, f"æ­£åœ¨æ‰§è¡Œæ™ºèƒ½æ¿å—æ¢æµ‹ (Phase 2: å¯¹ {len(selected_paths)} ä¸ªç›®æ ‡å®šå‘æŠ“åŒ…)")
                _add_log(task_id, "[INFO] æ­£åœ¨æ‰§è¡Œå®šå‘ç‚¹å‡»ä¸æŠ“åŒ…...")
                
                verified_mapping = await browser.capture_mapping_for_leaf_paths(selected_paths)
                menu_to_filters = (verified_mapping or {}).get("menu_to_filters", {}) if isinstance(verified_mapping, dict) else {}
                menu_to_urls = (verified_mapping or {}).get("menu_to_urls", {}) if isinstance(verified_mapping, dict) else {}
                mapping_count = len(menu_to_filters) + len(menu_to_urls)
                
                # 5. æ„å»ºåˆ†æç»“æœ (Phase 3)
                enhanced_analysis = {
                    "menu_tree": menu_tree,
                    "selected_leaf_paths": selected_paths,
                    "verified_category_mapping": verified_mapping,
                    "interaction_apis": (verified_mapping or {}).get("interaction_apis", {}),
                    "recommendations": [
                        f"æ™ºèƒ½æ¢æµ‹å®Œæˆï¼Œå·²æ„å»ºå¯ä¿¡æ˜ å°„: {mapping_count} ä¸ªï¼ˆfilters={len(menu_to_filters)}ï¼Œurls={len(menu_to_urls)}ï¼‰"
                    ],
                    # è¡¥å……åŸºç¡€æ•°æ®çŠ¶æ€æ£€æµ‹ï¼ˆä¸ºäº†è·å– hasData ç­‰æ ‡è®°ï¼‰
                    "data_status": await browser.detect_data_status()
                }

            # å…œåº•ï¼šç¡®ä¿å­—æ®µå­˜åœ¨
            if not isinstance(verified_mapping, dict):
                verified_mapping = {}
                enhanced_analysis["verified_category_mapping"] = verified_mapping

            _add_log(
                task_id,
                f"[SUCCESS] æ™ºèƒ½æ¢æµ‹å®Œæˆï¼Œæ„å»ºåˆ†ç±»æ˜ å°„: "
                f"{len((verified_mapping or {}).get('menu_to_filters', {})) + len((verified_mapping or {}).get('menu_to_urls', {}))} ä¸ª"
            )
        
        elif request.crawlMode == "date_range_api":
            # æ—¥æœŸç­›é€‰ç±»ç½‘ç«™çˆ¬å–æ¨¡å¼ï¼šä¸‰å±‚ç­–ç•¥
            # ç¬¬ä¸€å±‚ï¼šçº¯ API ç›´è¿
            # ç¬¬äºŒå±‚ï¼šDOM ç‰¹å¾æ£€æµ‹ + è‡ªåŠ¨æ“ä½œæ—¥æœŸæ§ä»¶
            # ç¬¬ä¸‰å±‚ï¼šæˆªå›¾ + LLM è§†è§‰åˆ†æ
            _update_step(task_id, 5, "æ­£åœ¨åˆ†ææ—¥æœŸ APIï¼ˆä¸‰å±‚ç­–ç•¥ï¼‰")
            _add_log(task_id, "[INFO] æ—¥æœŸç­›é€‰æ¨¡å¼ï¼šå¯åŠ¨ä¸‰å±‚ç­–ç•¥...")
            _add_log(task_id, "[INFO]   ç¬¬ä¸€å±‚: çº¯ API ç›´è¿")
            _add_log(task_id, "[INFO]   ç¬¬äºŒå±‚: DOM ç‰¹å¾æ£€æµ‹ + è‡ªåŠ¨æ“ä½œ")
            _add_log(task_id, "[INFO]   ç¬¬ä¸‰å±‚: æˆªå›¾ + LLM è§†è§‰åˆ†æ")
            
            # å¯¼å…¥æ—¥æœŸ API æå–å™¨
            from date_api_extractor import DateAPIExtractor, extract_date_api_with_three_layers
            
            # å‡†å¤‡ LLM å®ä¾‹ï¼ˆç”¨äºç¬¬ä¸‰å±‚ï¼‰
            temp_llm = LLMAgent(
                api_key=config.qwen_api_key if config else "",
                model=config.qwen_model if config else "qwen-max",
                base_url=config.qwen_base_url if config else None,
                enable_auto_repair=False
            )
            
            # æ—¥å¿—å›è°ƒ
            def log_callback(msg: str):
                _add_log(task_id, f"[DATE-API] {msg}")
            
            # æ‰§è¡Œä¸‰å±‚ç­–ç•¥
            extractor = DateAPIExtractor(browser, temp_llm)
            date_api_result = await extractor.extract_with_three_layers(
                network_requests,
                request.startDate,
                request.endDate,
                log_callback=log_callback
            )
            
            if date_api_result.success and date_api_result.best_candidate:
                # æŸä¸€å±‚æˆåŠŸ
                candidate = date_api_result.best_candidate
                layer_name = {1: "çº¯ API ç›´è¿", 2: "DOM è‡ªåŠ¨æ“ä½œ", 3: "LLM è§†è§‰åˆ†æ"}.get(date_api_result.layer, "æœªçŸ¥")
                
                _add_log(task_id, f"[SUCCESS] ç¬¬ {date_api_result.layer} å±‚ï¼ˆ{layer_name}ï¼‰æˆåŠŸï¼")
                _add_log(task_id, f"[SUCCESS] è¯†åˆ«åˆ°æ—¥æœŸ API: {candidate.url[:80]}...")
                _add_log(task_id, f"[SUCCESS] æ—¥æœŸå‚æ•°: {list(candidate.date_params.keys())}")
                _add_log(task_id, f"[SUCCESS] å‚æ•°æ ¼å¼: {list(candidate.date_params.values())}")
                _add_log(task_id, f"[SUCCESS] éªŒè¯é€šè¿‡ï¼Œè·å–åˆ° {date_api_result.data_count} æ¡æ•°æ®")
                
                # ç”Ÿæˆä»£ç ç‰‡æ®µä¾› LLM å‚è€ƒ
                api_code_snippet = extractor.generate_api_code_snippet(
                    candidate, 
                    request.startDate, 
                    request.endDate
                )
                
                # æ„å»ºåˆ†æç»“æœ
                enhanced_analysis = {
                    "date_api_extraction": {
                        "success": True,
                        "layer": date_api_result.layer,
                        "layer_name": layer_name,
                        "api_url": candidate.url,
                        "method": candidate.method,
                        "base_params": candidate.params,
                        "date_params": candidate.date_params,
                        "data_count": date_api_result.data_count,
                        "code_snippet": api_code_snippet,
                        "replayed_url": extractor.build_replay_url(
                            candidate, request.startDate, request.endDate
                        )[0],
                        "replayed_data": date_api_result.replayed_data,
                    },
                    "recommendations": date_api_result.recommendations,
                    "mode": f"date_range_api_layer{date_api_result.layer}"
                }
                
                _add_log(task_id, f"[SUCCESS] å°†ç”Ÿæˆç›´æ¥è°ƒç”¨ API çš„è„šæœ¬ï¼ˆåŸºäºç¬¬ {date_api_result.layer} å±‚ç»“æœï¼‰")
                
            else:
                # ä¸‰å±‚å…¨éƒ¨å¤±è´¥
                _add_log(task_id, f"[ERROR] ä¸‰å±‚ç­–ç•¥å‡å¤±è´¥")
                
                # è¾“å‡ºå„å±‚å¤±è´¥åŸå› 
                if date_api_result.layer1_result:
                    _add_log(task_id, f"[INFO] ç¬¬ä¸€å±‚å¤±è´¥: {date_api_result.layer1_result.get('error', 'æœªçŸ¥')}")
                if date_api_result.layer2_result:
                    _add_log(task_id, f"[INFO] ç¬¬äºŒå±‚å¤±è´¥: {date_api_result.layer2_result.get('error', 'æœªçŸ¥')}")
                if date_api_result.layer3_result:
                    _add_log(task_id, f"[INFO] ç¬¬ä¸‰å±‚å¤±è´¥: {date_api_result.layer3_result.get('error', 'æœªçŸ¥')}")
                
                # åˆ—å‡ºå‘ç°çš„å€™é€‰ APIï¼ˆä¾›è°ƒè¯•ï¼‰
                if date_api_result.candidates:
                    _add_log(task_id, f"[INFO] å‘ç° {len(date_api_result.candidates)} ä¸ªå€™é€‰ API:")
                    for i, c in enumerate(date_api_result.candidates[:3]):
                        _add_log(task_id, f"[INFO]   {i+1}. {c.url[:60]}... (ç½®ä¿¡åº¦: {c.confidence:.2f})")
                
                # æ„å»ºåˆ†æç»“æœï¼ˆæ ‡è®°å¤±è´¥ï¼Œä½†ä»æä¾›å€™é€‰ä¿¡æ¯ï¼‰
                enhanced_analysis = {
                    "date_api_extraction": {
                        "success": False,
                        "layer": 0,
                        "error": date_api_result.error,
                        "layer1_error": (date_api_result.layer1_result or {}).get("error"),
                        "layer2_error": (date_api_result.layer2_result or {}).get("error"),
                        "layer3_error": (date_api_result.layer3_result or {}).get("error"),
                        "candidates_count": len(date_api_result.candidates),
                        "candidates": [
                            {
                                "url": c.url,
                                "method": c.method,
                                "date_params": c.date_params,
                                "confidence": c.confidence,
                                "verification_error": c.verification_result.get("error") if c.verification_result else None
                            }
                            for c in date_api_result.candidates[:5]
                        ]
                    },
                    "recommendations": date_api_result.recommendations,
                    "mode": "date_range_api_all_failed"
                }
                
                _add_log(task_id, "[WARNING] ä¸‰å±‚ç­–ç•¥å‡å¤±è´¥ï¼Œå°†å°è¯•ç”Ÿæˆé€šç”¨çˆ¬è™«è„šæœ¬")
        
        else:
            # å•é¡µæ¨¡å¼ï¼šåŸºç¡€åˆ†æï¼Œè·³è¿‡æ­¥éª¤ 5
            _add_log(task_id, "[INFO] å•é¡µæ¨¡å¼ï¼šä½¿ç”¨åŸºç¡€åˆ†æ")
            _update_step(task_id, 5, "å·²è·³è¿‡ï¼ˆå•é¡µæ¨¡å¼ï¼‰")
        
        # Step 7: è°ƒç”¨ LLM ç”Ÿæˆè„šæœ¬
        if _is_cancelled(task_id):
            raise RuntimeError("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")

        # ========== date_range_apiï¼šç¡®å®šæ€§æ¨¡æ¿ä¼˜å…ˆï¼ˆæˆåŠŸæ—¶è·³è¿‡ LLMï¼Œé¿å…è·‘åï¼‰ ==========
        script_code = None
        if request.crawlMode == "date_range_api":
            dae = (enhanced_analysis or {}).get("date_api_extraction", {}) if isinstance(enhanced_analysis, dict) else {}
            if isinstance(dae, dict) and dae.get("success") is True:
                _update_step(task_id, 6, "æ­£åœ¨ç”Ÿæˆç¡®å®šæ€§ API ç›´è¿è„šæœ¬ï¼ˆè·³è¿‡LLMï¼‰")
                _add_log(task_id, "[INFO] date_range_apiï¼šå·²éªŒè¯å¯ç”¨ APIï¼Œä½¿ç”¨ç¡®å®šæ€§æ¨¡æ¿ç”Ÿæˆè„šæœ¬ï¼ˆä¸è°ƒç”¨ LLMï¼‰")
                try:
                    from deterministic_templates import render_date_range_api_script, analyze_response_schema, build_llm_cloze_prompt, parse_llm_cloze_response
                    
                    # â”€â”€ è‡ªåŠ¨æ¨æ–­å­—æ®µæ˜ å°„ â”€â”€
                    field_mappings = None
                    sample_data = dae.get("replayed_data")
                    if sample_data and isinstance(sample_data, dict):
                        field_mappings = analyze_response_schema(sample_data, api_url=str(dae.get("api_url", "")))
                        _add_log(task_id, f"[INFO] å­—æ®µæ˜ å°„è‡ªåŠ¨æ¨æ–­: ç½®ä¿¡åº¦={field_mappings.get('confidence', 0):.0%}, "
                                          f"æ—¥æœŸ={field_mappings.get('date_fields', [])}, "
                                          f"æ ‡é¢˜={field_mappings.get('title_fields', [])}, "
                                          f"URL={field_mappings.get('url_fields', [])}")
                        
                        # å¦‚æœæœ‰æœªæ˜ å°„çš„å­—æ®µç±»åˆ« â†’ LLM å®Œå½¢å¡«ç©ºè¡¥å…¨
                        unmapped = field_mappings.get("unmapped", [])
                        if unmapped and sample_data:
                            _add_log(task_id, f"[INFO] å­—æ®µæ˜ å°„ä¸å®Œæ•´ ({unmapped})ï¼Œå°è¯• LLM å®Œå½¢å¡«ç©º...")
                            try:
                                items_for_llm = []
                                for k in ["data", "result", "list", "items", "rows"]:
                                    v = sample_data.get(k)
                                    if isinstance(v, list) and v:
                                        if isinstance(v[0], dict):
                                            items_for_llm = v[:1]
                                        elif isinstance(v[0], list) and v[0] and isinstance(v[0][0], dict):
                                            items_for_llm = v[0][:1]
                                        break
                                if isinstance(sample_data.get("pageHelp"), dict):
                                    ph_data = sample_data["pageHelp"].get("data", [])
                                    if ph_data:
                                        if isinstance(ph_data[0], dict):
                                            items_for_llm = ph_data[:1]
                                        elif isinstance(ph_data[0], list) and ph_data[0]:
                                            items_for_llm = [x for x in ph_data[0] if isinstance(x, dict)][:1]
                                
                                if items_for_llm:
                                    cloze_prompt = build_llm_cloze_prompt(items_for_llm[0], unmapped)
                                    llm_for_cloze = LLMAgent(
                                        api_key=config.qwen_api_key if config else "",
                                        model=config.qwen_model if config else "qwen-max",
                                        base_url=config.qwen_base_url if config else None,
                                        enable_auto_repair=False
                                    )
                                    cloze_resp = llm_for_cloze._call_llm(
                                        system_prompt="You are a JSON field mapping assistant. Only output valid JSON.",
                                        user_prompt=cloze_prompt,
                                        temperature=0.1
                                    )
                                    cloze_result = parse_llm_cloze_response(cloze_resp)
                                    if cloze_result:
                                        for field_type, field_names in cloze_result.items():
                                            if field_names and field_type in field_mappings:
                                                field_mappings[field_type] = field_names
                                        still_unmapped = [u for u in unmapped if not cloze_result.get(u)]
                                        field_mappings["unmapped"] = still_unmapped
                                        _add_log(task_id, f"[SUCCESS] LLM å®Œå½¢å¡«ç©ºè¡¥å…¨: {cloze_result}")
                            except Exception as cloze_err:
                                _add_log(task_id, f"[WARNING] LLM å®Œå½¢å¡«ç©ºå¤±è´¥ï¼ˆä¸å½±å“ç”Ÿæˆï¼‰: {cloze_err}")
                    
                    script_code = render_date_range_api_script(
                        target_url=request.url,
                        api_url=str(dae.get("api_url", "")),
                        method=str(dae.get("method", "GET")),
                        base_params=dae.get("base_params", {}) or {},
                        date_params=dae.get("date_params", {}) or {},
                        start_date=request.startDate,
                        end_date=request.endDate,
                        output_dir=str((Path(__file__).parent / "output")),
                        extra_headers={},
                        field_mappings=field_mappings,
                    )
                    _add_log(task_id, "[SUCCESS] å·²ç”Ÿæˆç¡®å®šæ€§ API è„šæœ¬ï¼ˆçº¯æ¥å£ç›´è¿ï¼‰")
                except Exception as tpl_err:
                    _add_log(task_id, f"[WARNING] ç¡®å®šæ€§æ¨¡æ¿ç”Ÿæˆå¤±è´¥ï¼Œå°†å›é€€åˆ° LLM: {tpl_err}")
                    script_code = None
            
        # ========== å…¶ä»–æ¨¡å¼ / æˆ–æ¨¡æ¿å¤±è´¥ï¼šèµ° LLM ç”Ÿæˆ ==========
        if script_code is None:
            _update_step(task_id, 6, "æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆçˆ¬è™«è„šæœ¬")
            _add_log(task_id, f"[INFO] æ­£åœ¨è°ƒç”¨ LLM: {config.qwen_model if config else 'unknown'}")
            _add_log(task_id, "[INFO] è¿™å¯èƒ½éœ€è¦ 20-60 ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            _add_log(task_id, f"[INFO] è‡ªåŠ¨ä¿®å¤/æ£€æŸ¥/åå¤„ç†: {'å¼€å¯' if auto_repair_enabled else 'å…³é—­'}")
            
            llm = LLMAgent(
                api_key=config.qwen_api_key if config else "",
                model=config.qwen_model if config else "qwen-max",
                base_url=config.qwen_base_url if config else None,
                enable_auto_repair=auto_repair_enabled
            )
            # æ„å»ºç”¨æˆ·éœ€æ±‚
            user_requirements = request.extraRequirements or ""
            if request.siteName:
                user_requirements += f"\nå®˜ç½‘åç§°: {request.siteName}"
            if request.listPageName:
                user_requirements += f"\nåˆ—è¡¨é¡µé¢åç§°: {request.listPageName}"
            if request.sourceCredibility:
                user_requirements += f"\nä¿¡æ¯æºå¯ä¿¡åº¦: {request.sourceCredibility}"
            
            # å‡†å¤‡é™„ä»¶ï¼ˆå›¾ç‰‡ï¼‰
            llm_attachments = None
            if request.attachments:
                from llm_agent import AttachmentData
                llm_attachments = [
                    AttachmentData(
                        filename=att.filename,
                        base64_data=att.base64,
                        mime_type=att.mimeType
                    )
                    for att in request.attachments
                ]
                _add_log(task_id, f"[INFO] å·²é™„åŠ  {len(llm_attachments)} ä¸ªå›¾ç‰‡/æ–‡ä»¶ç»™ LLM")
            
            try:
                script_code = llm.generate_crawler_script(
                    page_url=request.url,
                    page_html=page_html,
                    page_structure=page_structure,
                    network_requests=network_requests,
                    user_requirements=user_requirements if user_requirements.strip() else None,
                    start_date=request.startDate,
                    end_date=request.endDate,
                    enhanced_analysis=enhanced_analysis,
                    attachments=llm_attachments,
                    run_mode=request.runMode,
                    task_id=task_id
                )
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ fallback è„šæœ¬ï¼ˆLLM è°ƒç”¨å¤±è´¥ï¼‰
                if (
                    isinstance(script_code, str)
                    and ("fallback template" in script_code.lower() or "generated when llm fails" in script_code.lower())
                ):
                    _add_log(task_id, "[ERROR] LLM è°ƒç”¨å¤±è´¥ï¼Œå·²ä½¿ç”¨å¤‡ç”¨æ¨¡æ¿è„šæœ¬")
                    raise RuntimeError("LLM è°ƒç”¨å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆè„šæœ¬ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ– API é…ç½®ã€‚")
            except Exception as llm_err:
                error_msg = str(llm_err)
                _add_log(task_id, f"[ERROR] LLM ç”Ÿæˆè„šæœ¬å¤±è´¥: {error_msg}")
                # å¦‚æœé”™è¯¯ä¿¡æ¯ä¸­åŒ…å«ä»£ç†é”™è¯¯ï¼Œç»™å‡ºæ›´æ˜ç¡®çš„æç¤º
                if "ProxyError" in error_msg or "proxy" in error_msg.lower():
                    error_msg = "LLM API è¿æ¥å¤±è´¥ï¼ˆä»£ç†é”™è¯¯ï¼‰ã€‚è¯·æ£€æŸ¥ç½‘ç»œä»£ç†è®¾ç½®æˆ– VPN é…ç½®ã€‚"
                raise RuntimeError(error_msg)
            
            token_usage = llm.get_token_usage()
            _add_log(task_id, f"[SUCCESS] LLM ç”Ÿæˆå®Œæˆï¼ŒToken: {token_usage['total_tokens']:,}")
        else:
            # ç¡®å®šæ€§æ¨¡æ¿ï¼šä¸è°ƒç”¨ LLM
            _add_log(task_id, "[SUCCESS] å·²è·³è¿‡ LLMï¼šä½¿ç”¨ç¡®å®šæ€§æ¨¡æ¿ç”Ÿæˆè„šæœ¬")
        
        # å…œåº•ï¼šç¡®ä¿ script_code æ˜¯å­—ç¬¦ä¸²
        if not isinstance(script_code, str) or not script_code.strip():
            _add_log(task_id, "[ERROR] è„šæœ¬ç”Ÿæˆå¤±è´¥ï¼šscript_code ä¸ºç©ºæˆ–éå­—ç¬¦ä¸²")
            raise RuntimeError("è„šæœ¬ç”Ÿæˆå¤±è´¥ï¼šscript_code ä¸ºç©ºæˆ–éå­—ç¬¦ä¸²")
        
        # ã€å…³é”®ã€‘åå¤„ç†ï¼šæ³¨å…¥æ­£ç¡®çš„åˆ†ç±»æ˜ å°„ï¼ˆmulti_page / auto_detect ä¸”æœ‰å¯ä¿¡æ˜ å°„æ—¶ï¼‰
        # æ³¨æ„ï¼šè¿™æ˜¯æ•°æ®æ­£ç¡®æ€§çš„ä¿éšœï¼Œå¿…é¡»å§‹ç»ˆæ‰§è¡Œï¼Œä¸å— auto_repair å¼€å…³å½±å“
        # LLM å¯èƒ½å¿½ç•¥ verified_category_mapping è€Œè‡ªå·±çŒœæµ‹ IDï¼Œæ­¤å¤„å¼ºåˆ¶è¦†ç›–ä¸ºæ­£ç¡®å€¼
        if request.crawlMode in ("multi_page", "auto_detect") and verified_mapping:
            from main import _inject_selected_categories_and_fix_dates
            script_code = _inject_selected_categories_and_fix_dates(
                script_code=script_code,
                start_date=request.startDate,
                end_date=request.endDate,
                verified_mapping=verified_mapping
            )
            injected_filters = len((verified_mapping or {}).get("menu_to_filters", {})) if isinstance(verified_mapping, dict) else 0
            injected_urls = len((verified_mapping or {}).get("menu_to_urls", {})) if isinstance(verified_mapping, dict) else 0
            _add_log(task_id, f"[INFO] å·²æ³¨å…¥å¯ä¿¡æ˜ å°„ï¼ˆfilters={injected_filters}ï¼Œurls={injected_urls}ï¼‰")
        
        # æ³¨æ„ï¼šHTTP éŸ§æ€§å±‚ã€æ—¥æœŸæå–å·¥å…·ç­‰åå¤„ç†å·²ç§»è‡³ llm_agent.py ä¸­çš„æ¡ä»¶æ€§åå¤„ç†
        # è¿™é‡Œä¸å†éœ€è¦å•ç‹¬è°ƒç”¨
        
        # Step 8: éªŒè¯ç”Ÿæˆçš„ä»£ç 
        if _is_cancelled(task_id):
            raise RuntimeError("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")
            
        # Step 8: éªŒè¯ç”Ÿæˆçš„ä»£ç ï¼ˆå¯é€‰ï¼‰
        # å…³é—­ auto_repair æ—¶ï¼šä¸è¿›è¡Œä»»ä½•æ£€æŸ¥ï¼Œç›´æ¥ä¿å­˜å¹¶è¿è¡Œ
        if auto_repair_enabled:
            _update_step(task_id, 7, "ğŸ” æ­£åœ¨éªŒè¯ç”Ÿæˆçš„ä»£ç ")
            _add_log(task_id, "[INFO] æ­£åœ¨è¿›è¡Œé™æ€ä»£ç æ£€æŸ¥...")
            
            try:
                from validator import StaticCodeValidator
                validator = StaticCodeValidator()
                issues = validator.validate(script_code)
                
                if validator.has_errors():
                    error_issues = [i for i in issues if i.severity.value == "error"]
                    _add_log(task_id, f"[ERROR] ä»£ç éªŒè¯å¤±è´¥ï¼šå‘ç° {len(error_issues)} ä¸ªé”™è¯¯ï¼Œå·²åœæ­¢æ‰§è¡Œ")
                    # æŠŠæœ€å…³é”®çš„é”™è¯¯åŸå› å†™å‡ºæ¥ï¼ˆé¿å…åªçœ‹åˆ°â€œå¤±è´¥â€ä¸çŸ¥é“ä¸ºå•¥ï¼‰
                    for it in error_issues[:5]:
                        line_info = f"ï¼ˆè¡Œ {it.line_number}ï¼‰" if getattr(it, "line_number", None) else ""
                        _add_log(task_id, f"[ERROR] - [{it.code}]{line_info} {it.message}")
                        if getattr(it, "suggestion", ""):
                            _add_log(task_id, f"[ERROR]   å»ºè®®: {it.suggestion}")
                    raise RuntimeError("ç”Ÿæˆçš„è„šæœ¬æœªé€šè¿‡è¯­æ³•/è§„åˆ™éªŒè¯ï¼Œå·²ç»ˆæ­¢ä»»åŠ¡")
                else:
                    warn_issues = [i for i in issues if i.severity.value == "warning"]
                    if warn_issues:
                        _add_log(task_id, f"[WARNING] ä»£ç æ£€æŸ¥é€šè¿‡ï¼Œä½†æœ‰ {len(warn_issues)} ä¸ªè­¦å‘Š")
                        for it in warn_issues[:5]:
                            _add_log(task_id, f"[WARNING] - [{it.code}] {it.message}")
                    else:
                        _add_log(task_id, "[SUCCESS] ä»£ç éªŒè¯é€šè¿‡ âœ“")
            except ImportError:
                _add_log(task_id, "[INFO] éªŒè¯å™¨æ¨¡å—æœªå®‰è£…ï¼Œè·³è¿‡éªŒè¯")
            except Exception as val_err:
                _add_log(task_id, f"[WARNING] éªŒè¯æ—¶å‡ºç°é—®é¢˜: {val_err}")
        else:
            _update_step(task_id, 7, "å·²è·³è¿‡ï¼ˆauto_repair=falseï¼‰")
            _add_log(task_id, "[INFO] å·²å…³é—­è‡ªåŠ¨ä¿®å¤ï¼šè·³è¿‡é™æ€ä»£ç æ£€æŸ¥")
        
        # Step 9: ä¿å­˜è„šæœ¬
        _update_step(task_id, 8, "çˆ¬è™«è„šæœ¬å·²ç”Ÿæˆ")
        
        output_dir = config.output_dir if config else Path("./py")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_name = request.outputScriptName
        if not output_name.endswith(".py"):
            output_name += ".py"
        
        output_path = output_dir / output_name
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(script_code)
        
        _add_log(task_id, f"[SUCCESS] è„šæœ¬å·²ä¿å­˜: {output_path}")
        _add_log(task_id, f"[SUCCESS] æ–‡ä»¶å¤§å°: {len(script_code):,} å­—ç¬¦")
        
        # Step 10: è‡ªåŠ¨è¿è¡Œç”Ÿæˆçš„çˆ¬è™«è„šæœ¬
        if _is_cancelled(task_id):
            raise RuntimeError("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")
            
        _update_step(task_id, 9, "æ­£åœ¨è¿è¡Œçˆ¬è™«è„šæœ¬")
        _add_log(task_id, f"[INFO] æ­£åœ¨è¿è¡Œ: python {output_path}")
        
        import subprocess
        import sys
        import os as _os_env
        
        try:
            # è®¾ç½® UTF-8 ç¼–ç ç¯å¢ƒï¼Œé¿å… Windows GBK ç¼–ç é—®é¢˜ï¼ˆLLM ç”Ÿæˆçš„ä»£ç å¯èƒ½åŒ…å« emojiï¼‰
            script_env = _os_env.environ.copy()
            script_env["PYTHONIOENCODING"] = "utf-8"
            
            # è¿è¡Œç”Ÿæˆçš„è„šæœ¬ï¼ˆä½¿ç”¨ Popen ä»¥ä¾¿å¯ä»¥åœæ­¢ï¼‰
            process = subprocess.Popen(
                [sys.executable, str(output_path)],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                cwd=str(output_dir),
                env=script_env,
                encoding="utf-8",
                errors="replace"  # é‡åˆ°æ— æ³•è§£ç çš„å­—ç¬¦æ—¶æ›¿æ¢ï¼Œé¿å…å´©æºƒ
            )
            
            # å­˜å‚¨è¿›ç¨‹å¼•ç”¨ä»¥ä¾¿å¯ä»¥åœæ­¢
            task_processes[task_id] = process
            
            try:
                # ç­‰å¾…è¿›ç¨‹å®Œæˆï¼Œæœ€å¤š5åˆ†é’Ÿ
                stdout, stderr = process.communicate(timeout=300)
                
                if process.returncode == 0:
                    _add_log(task_id, "[SUCCESS] çˆ¬è™«è„šæœ¬è¿è¡ŒæˆåŠŸ")
                    # åªæ˜¾ç¤ºæœ€å1000å­—ç¬¦é¿å…æ—¥å¿—è¿‡é•¿
                    if stdout:
                        stdout_preview = stdout[-1000:] if len(stdout) > 1000 else stdout
                        _add_log(task_id, stdout_preview)
                elif process.returncode == -9 or process.returncode == -15:
                    _add_log(task_id, "[INFO] è„šæœ¬å·²è¢«ç”¨æˆ·åœæ­¢")
                else:
                    _add_log(task_id, f"[ERROR] è„šæœ¬è¿è¡Œå¤±è´¥ï¼Œè¿”å›ç : {process.returncode}")
                    if stderr:
                        _add_log(task_id, f"[STDERR] {stderr[-800:]}")
                    # ç›´æ¥ç»ˆæ­¢ä»»åŠ¡ï¼ˆå¦åˆ™ä¼šå‡ºç°â€œè„šæœ¬å¤±è´¥ä½†ä»»åŠ¡å®Œæˆâ€çš„è¯¯å¯¼ï¼‰
                    raise RuntimeError(f"çˆ¬è™«è„šæœ¬è¿è¡Œå¤±è´¥ï¼ˆè¿”å›ç  {process.returncode}ï¼‰")
                        
            except subprocess.TimeoutExpired:
                _add_log(task_id, "[WARNING] è„šæœ¬è¿è¡Œè¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰ï¼Œæ­£åœ¨ç»ˆæ­¢...")
                process.kill()
                process.wait()
            finally:
                # æ¸…ç†è¿›ç¨‹å¼•ç”¨
                if task_id in task_processes:
                    del task_processes[task_id]
                    
        except Exception as run_err:
            _add_log(task_id, f"[WARNING] è¿è¡Œè„šæœ¬æ—¶å‡ºé”™: {run_err}")
        
        # Step 11: éªŒè¯çˆ¬å–ç»“æœ
        _update_step(task_id, 10, "ğŸ“Š æ­£åœ¨éªŒè¯çˆ¬å–ç»“æœ")
        
        # æŸ¥æ‰¾è¾“å‡ºçš„æ–‡ä»¶ï¼ˆåœ¨ output ç›®å½•ï¼‰
        # å…¼å®¹ä¸¤ç§è·¯å¾„ï¼špygen/output å’Œ pygen/py/output
        possible_dirs = []
        if output_dir:
            possible_dirs.append(output_dir.parent / "output") # pygen/output
            possible_dirs.append(output_dir / "output")        # pygen/py/output
        else:
            possible_dirs.append(Path("./output"))
            possible_dirs.append(Path("pygen/output"))
            possible_dirs.append(Path("pygen/py/output"))
            
        # ç¡®å®šä¸»è¾“å‡ºç›®å½•ï¼ˆç”¨äºåç»­ä¸‹è½½æ–‡ä»¶å­˜æ”¾ç­‰ï¼‰ï¼Œä¼˜å…ˆé€‰æ‹©å­˜åœ¨çš„ç›®å½•
        output_json_dir = possible_dirs[0]
        for d in possible_dirs:
            if d.exists():
                output_json_dir = d
                break
            
        reports = []
        news_articles = []
        markdown_file = None
        
        all_json_files = []
        for d in possible_dirs:
            if d.exists():
                # æ³¨æ„ï¼šWindows ä¸‹å¯èƒ½å­˜åœ¨â€œä»¥ .json ç»“å°¾çš„ç›®å½•â€ï¼ˆå†å² bugï¼‰ï¼Œglob("*.json") ä¼šæŠŠç›®å½•ä¹Ÿè¿”å›
                # è¿™é‡Œæ˜¾å¼è¿‡æ»¤ï¼Œä»…ä¿ç•™çœŸæ­£çš„æ–‡ä»¶ï¼›è‹¥é‡åˆ° *.json ç›®å½•ï¼Œåˆ™å°è¯•è¯»å–å…¶å†…éƒ¨çš„ *.json æ–‡ä»¶ä½œä¸ºå…œåº•
                for p in d.glob("*.json"):
                    try:
                        if p.is_file():
                            all_json_files.append(p)
                        elif p.is_dir():
                            # å…œåº•ï¼šå°†ç›®å½•å†…çš„ json æ–‡ä»¶ä¹Ÿçº³å…¥å€™é€‰ï¼ˆä¿®å¤å†å²äº§ç‰©ï¼špygen_multi_*.json/xxx.jsonï¼‰
                            for q in p.glob("*.json"):
                                if q.is_file():
                                    all_json_files.append(q)
                    except Exception:
                        continue
        
        # å»é‡
        all_json_files = list(set(all_json_files))
        
        if all_json_files:
            # æ ¹æ®è¿è¡Œæ¨¡å¼å¤„ç†ä¸åŒçš„ç»“æœæ ¼å¼
            if request.runMode == "news_sentiment":
                # æ–°é—»èˆ†æƒ…æ¨¡å¼ï¼šæŸ¥æ‰¾ JSON æ–‡ä»¶ï¼ˆä¼˜å…ˆ news_ å¼€å¤´çš„æ–‡ä»¶ï¼‰
                all_json_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
                
                # åªè€ƒè™‘æœ€è¿‘ 5 åˆ†é’Ÿå†…åˆ›å»ºçš„æ–‡ä»¶
                import time as time_module
                recent_cutoff = time_module.time() - 300  # 5 åˆ†é’Ÿ
                recent_json_files = [f for f in all_json_files if f.stat().st_mtime > recent_cutoff]
                
                # ä¼˜å…ˆæŸ¥æ‰¾ news_ å¼€å¤´çš„æ–‡ä»¶
                news_json_files = [f for f in recent_json_files if f.name.startswith("news_")]
                json_files = news_json_files if news_json_files else recent_json_files
                
                _add_log(task_id, f"[DEBUG] æ‰¾åˆ° {len(recent_json_files)} ä¸ªæœ€è¿‘çš„ JSON æ–‡ä»¶ï¼Œå…¶ä¸­ {len(news_json_files)} ä¸ªæ˜¯ news_ å¼€å¤´çš„")
                
                latest_json = None
                for jf in json_files:
                    try:
                        with open(jf, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        # æ£€æŸ¥æ˜¯å¦åŒ…å« articles æˆ– news å­—æ®µ
                        if "articles" in data or "news" in data:
                            latest_json = jf
                            _add_log(task_id, f"[DEBUG] ä½¿ç”¨æ–‡ä»¶: {jf.name}ï¼ŒåŒ…å« {len(data.get('articles', data.get('news', [])))} æ¡æ•°æ®")
                            break
                    except Exception as e:
                        _add_log(task_id, f"[DEBUG] è·³è¿‡æ–‡ä»¶ {jf.name}: {e}")
                        continue
                
                if latest_json:
                    _add_log(task_id, f"[INFO] æ‰¾åˆ°æ–°é—» JSON æ–‡ä»¶: {latest_json}")
                    
                    try:
                        with open(latest_json, 'r', encoding='utf-8') as f:
                            data = json.load(f)

                        def _safe_str(v: Any, default: str = "") -> str:
                            """å°†ä»»æ„å€¼å®‰å…¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å… Pydantic å›  None/éå­—ç¬¦ä¸²ç±»å‹æŠ¥é”™"""
                            if v is None:
                                return default
                            if isinstance(v, str):
                                return v
                            try:
                                return str(v)
                            except Exception:
                                return default
                        
                        # è§£æ articles æ•°ç»„
                        articles_key = "articles" if "articles" in data else "news" if "news" in data else None
                        if articles_key and isinstance(data.get(articles_key), list):
                            for i, item in enumerate(data[articles_key]):
                                if not isinstance(item, dict):
                                    continue
                                
                                news_articles.append({
                                    "id": str(i + 1),
                                    "title": _safe_str(item.get("title"), "æœªçŸ¥") or "æœªçŸ¥",
                                    "author": _safe_str(item.get("author"), ""),
                                    "date": _safe_str(item.get("date"), ""),
                                    "source": _safe_str(item.get("source"), ""),
                                    "sourceUrl": _safe_str(item.get("sourceUrl") or item.get("url") or item.get("link"), ""),
                                    "summary": _safe_str(item.get("summary"), ""),
                                    "content": _safe_str(item.get("content"), ""),
                                    "category": _safe_str(item.get("category"), ""),  # æ¥æºæ¿å—
                                })
                            
                            _add_log(task_id, f"[SUCCESS] è§£æåˆ° {len(news_articles)} æ¡æ–°é—»")
                    except Exception as parse_err:
                        _add_log(task_id, f"[WARNING] è§£ææ–°é—» JSON å¤±è´¥: {parse_err}")
                else:
                    _add_log(task_id, f"[WARNING] æœªæ‰¾åˆ°åŒ…å«æ–°é—»æ•°æ®çš„ JSON æ–‡ä»¶")
            else:
                # ä¼ä¸šæŠ¥å‘Šæ¨¡å¼ï¼šåŸæœ‰é€»è¾‘
                # ä½¿ç”¨å·²ç»æ”¶é›†å¥½çš„ all_json_files
                json_files = all_json_files
                json_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
                
                if json_files:
                    latest_json = json_files[0]
                    _add_log(task_id, f"[INFO] æ‰¾åˆ°ç»“æœæ–‡ä»¶: {latest_json}")
                    
                    try:
                        with open(latest_json, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        # æå–ç”Ÿæˆè„šæœ¬è¾“å‡ºçš„ä¸‹è½½å¤´ï¼ˆå¦‚æœ‰ï¼‰ï¼Œä¾›åç»­ä¸‹è½½ PDF æ—¶ä½¿ç”¨
                        _script_download_headers = data.get("downloadHeaders") if isinstance(data, dict) else None
                        
                        # è§£æ reports æ•°ç»„ï¼ˆå…¼å®¹ä¸åŒè„šæœ¬å­—æ®µå‘½åï¼šname/title/reportName/textï¼‰
                        if isinstance(data, dict) and "reports" in data:
                            for i, item in enumerate(data["reports"]):
                                if not isinstance(item, dict):
                                    continue

                                name = (
                                    item.get("name")
                                    or item.get("title")
                                    or item.get("reportName")
                                    or item.get("report_title")
                                    or item.get("text")
                                    or "æœªçŸ¥"
                                )
                                download_url = (
                                    item.get("downloadUrl")
                                    or item.get("download_url")
                                    or item.get("url")
                                    or item.get("pdfUrl")
                                    or "#"
                                )
                                file_type = item.get("fileType") or item.get("file_type") or "pdf"

                                reports.append({
                                    "id": str(i + 1),
                                    "name": name,
                                    "date": item.get("date", ""),
                                    "downloadUrl": download_url,
                                    "fileType": file_type,
                                    "category": item.get("category", "")  # æ¥æºæ¿å—
                                })
                        # è‹¥ç”¨æˆ·æä¾›äº†æ—¥æœŸèŒƒå›´ï¼Œåˆ™åœ¨æœåŠ¡ç«¯åšä¸€æ¬¡"ç¡¬è¿‡æ»¤"ï¼Œé¿å…è„šæœ¬æŠŠæ— æ—¥æœŸ/è¶Šç•Œæ•°æ®ä¹Ÿå¡è¿›æ¥
                        # è§„åˆ™ï¼šåªä¿ç•™ date ä¸º YYYY-MM-DD ä¸”åœ¨ [startDate, endDate] èŒƒå›´å†…çš„æ•°æ®ï¼›æ— æ—¥æœŸç›´æ¥ä¸¢å¼ƒ
                        try:
                            start_date = (request.startDate or "").strip()
                            end_date = (request.endDate or "").strip()
                            if start_date and end_date:
                                before = len(reports)
                                filtered = []
                                for r in reports:
                                    d = (r.get("date") or "").strip()
                                    if len(d) == 10 and d[4] == "-" and d[7] == "-" and start_date <= d <= end_date:
                                        filtered.append(r)
                                reports = filtered
                                dropped = before - len(reports)
                                if dropped:
                                    _add_log(task_id, f"[INFO] å·²æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤ï¼šä¸¢å¼ƒ {dropped} æ¡ï¼ˆæ— æ—¥æœŸæˆ–ä¸åœ¨ {start_date}~{end_date}ï¼‰")
                        except Exception as _filter_err:
                            _add_log(task_id, f"[WARNING] æœåŠ¡ç«¯æ—¥æœŸè¿‡æ»¤å¤±è´¥ï¼ˆå·²å¿½ç•¥ï¼‰ï¼š{_filter_err}")

                        # ã€æ–°å¢ã€‘ç»“æœæˆªæ–­ï¼šå¦‚æœæŠ¥å‘Šæ•°é‡è¶…è¿‡ 100 ä¸”ç”¨æˆ·é€‰äº†â€œä¸ä¸‹è½½â€æˆ–â€œæ–°é—»æŠ¥å‘Šä¸‹è½½â€ç­‰åœºæ™¯ï¼Œä¸ºä¼˜åŒ–å‰ç«¯æ¸²æŸ“ï¼Œæˆªæ–­åˆ—è¡¨
                        # è®°å½•æ€»æ•°ä»¥ä¾¿å‰ç«¯å±•ç¤ºæç¤º
                        tasks[task_id]["totalCount"] = len(reports)
                        if len(reports) > 100:
                            _add_log(task_id, f"[INFO] ç»“æœè¿‡å¤šï¼ˆ{len(reports)} æ¡ï¼‰ï¼Œå·²æˆªæ–­ä¸ºå‰ 100 æ¡ä»¥ä¼˜åŒ–å±•ç¤º")
                            reports = reports[:100]

                        _add_log(task_id, f"[SUCCESS] è§£æåˆ° {len(reports)} æ¡æŠ¥å‘Šè®°å½•ï¼ˆæ€»åŒ¹é…: {tasks[task_id].get('totalCount', len(reports))}ï¼‰")
                        
                        # éªŒè¯è¾“å‡ºæ•°æ®è´¨é‡
                        try:
                            from validator import OutputValidator
                            output_validator = OutputValidator()
                            quality = output_validator.get_quality_report({"reports": reports})
                            
                            if quality["date_fill_rate"] < 0.3:
                                _add_log(task_id, f"[WARNING] æ—¥æœŸå¡«å……ç‡è¾ƒä½: {quality['date_fill_rate']:.1%}")
                            else:
                                _add_log(task_id, f"[INFO] æ•°æ®è´¨é‡: æ—¥æœŸå¡«å……ç‡ {quality['date_fill_rate']:.1%}")
                        except ImportError:
                            pass
                        except Exception as qual_err:
                            _add_log(task_id, f"[WARNING] è´¨é‡æ£€æŸ¥å¤±è´¥: {qual_err}")
                            
                    except Exception as parse_err:
                        _add_log(task_id, f"[WARNING] è§£æç»“æœæ–‡ä»¶å¤±è´¥: {parse_err}")
        else:
            _add_log(task_id, f"[INFO] æœªæ‰¾åˆ° output ç›®å½•: {output_json_dir}")
        
        # ============ PDF ä¸‹è½½é€»è¾‘ï¼ˆä¼ä¸šæŠ¥å‘Š/æ–°é—»æŠ¥å‘Šåœºæ™¯ + é€‰æ‹©ä¸‹è½½æ–‡ä»¶ï¼‰============
        downloaded_count = 0
        files_not_enough = False
        pdf_output_dir = None
        # ç¡®ä¿ _script_download_headers å·²å®šä¹‰ï¼ˆLLM ç”Ÿæˆçš„è„šæœ¬å¯èƒ½æ²¡æœ‰æ­¤å­—æ®µï¼‰
        if '_script_download_headers' not in dir():
            _script_download_headers = None
        
        if request.runMode in ["enterprise_report", "news_report_download"] and request.downloadReport == "yes" and len(reports) > 0:
            _add_log(task_id, "[INFO] æ­£åœ¨ä¸‹è½½å‰5ä¸ªPDFæ–‡ä»¶...")
            
            # åˆ›å»ºå”¯ä¸€çš„å­æ–‡ä»¶å¤¹ï¼štask_id + æ—¶é—´æˆ³
            import hashlib
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # ä½¿ç”¨ URL çš„å“ˆå¸Œå€¼ä½œä¸ºæ ‡è¯†ï¼ˆå–å‰8ä½ï¼‰
            url_hash = hashlib.md5(request.url.encode()).hexdigest()[:8]
            folder_name = f"{task_id}_{timestamp}_{url_hash}"
            
            pdf_output_dir = output_json_dir / "output_pdf" / folder_name
            pdf_output_dir.mkdir(parents=True, exist_ok=True)
            _add_log(task_id, f"[INFO] ä¸‹è½½ç›®å½•: output_pdf/{folder_name}")
            
            # æœ€å¤šä¸‹è½½5ä¸ª
            max_download = 5
            to_download = reports[:max_download]
            
            # æ£€æŸ¥æ˜¯å¦ä¸è¶³5ä¸ª
            if len(reports) < max_download:
                files_not_enough = True
                _add_log(task_id, f"[INFO] æ—¥æœŸèŒƒå›´å†…ä»…æœ‰ {len(reports)} ä¸ªæ–‡ä»¶ï¼Œä¸è¶³5ä»½")
            
            import httpx
            import urllib.parse
            from urllib.parse import urlparse as _dl_urlparse
            
            # â”€â”€ æ„å»ºé˜²ç›—é“¾è¯·æ±‚å¤´ï¼ˆæ³›åŒ–ï¼šä»ç›®æ ‡é¡µé¢ URL è‡ªåŠ¨æ´¾ç”Ÿ Refererï¼‰â”€â”€
            # ä¼˜å…ˆä½¿ç”¨ç”Ÿæˆè„šæœ¬è¾“å‡ºçš„ downloadHeadersï¼ˆæ›´ç²¾å‡†ï¼‰ï¼Œå¦åˆ™ä» request.url æ´¾ç”Ÿ
            _target_parsed = _dl_urlparse(request.url)
            _target_origin = f"{_target_parsed.scheme}://{_target_parsed.netloc}"
            _download_headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/pdf, application/octet-stream, */*",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Referer": request.url,  # ç”¨çˆ¬å–çš„ç›®æ ‡é¡µé¢ä½œä¸º Refererï¼ˆæœ€è‡ªç„¶ï¼‰
            }
            # åˆå¹¶è„šæœ¬è¾“å‡ºçš„ä¸‹è½½å¤´ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œè„šæœ¬å¤´ä¼˜å…ˆ
            if _script_download_headers and isinstance(_script_download_headers, dict):
                _download_headers.update(_script_download_headers)
            
            downloaded_reports = []
            
            for i, report in enumerate(to_download):
                download_url = report.get("downloadUrl", "")
                if not download_url or download_url == "#":
                    _add_log(task_id, f"[WARNING] æŠ¥å‘Š '{report.get('name', 'æœªçŸ¥')[:30]}...' æ— æœ‰æ•ˆä¸‹è½½é“¾æ¥ï¼Œè·³è¿‡")
                    downloaded_reports.append(report)  # ä¿ç•™ä½†ä¸æ ‡è®°ä¸ºæœ¬åœ°
                    continue
                
                try:
                    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
                    safe_name = report.get("name", f"report_{i+1}")[:50]
                    # ç§»é™¤æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
                    safe_name = "".join(c for c in safe_name if c.isalnum() or c in (' ', '-', '_', '.', 'ï¼ˆ', 'ï¼‰')).strip()
                    if not safe_name:
                        safe_name = f"report_{i+1}"
                    
                    file_ext = report.get("fileType", "pdf")
                    filename = f"{i+1}_{safe_name}.{file_ext}"
                    local_path = pdf_output_dir / filename
                    
                    # ä¸‹è½½æ–‡ä»¶ï¼ˆå¸¦é˜²ç›—é“¾å¤´ + 403 è‡ªåŠ¨é‡è¯•ï¼‰
                    dl_parsed = _dl_urlparse(download_url)
                    dl_headers = dict(_download_headers)
                    if dl_parsed.netloc and dl_parsed.netloc != _target_parsed.netloc:
                        dl_headers["Referer"] = f"{dl_parsed.scheme or 'https'}://{dl_parsed.netloc}/"
                    
                    # æ„å»ºå€™é€‰ URL åˆ—è¡¨ï¼ˆåŸå§‹ URL + å­åŸŸåå˜ä½“ï¼‰ï¼Œç”¨äº 403 é‡è¯•
                    candidate_urls = [download_url]
                    if dl_parsed.netloc:
                        # å¸¸è§çš„æ–‡ä»¶æ‰˜ç®¡å­åŸŸåå‰ç¼€
                        # å¦‚æ·±äº¤æ‰€: www.szse.cn/disc/... â†’ disc.szse.cn/disc/...
                        path = dl_parsed.path or ""
                        base_domain = dl_parsed.netloc.replace("www.", "")
                        file_subdomains = ["disc", "download", "file", "static", "cdn", "docs"]
                        for prefix in file_subdomains:
                            if path.startswith(f"/{prefix}/"):
                                alt_host = f"{prefix}.{base_domain}"
                                if alt_host != dl_parsed.netloc:
                                    alt_url = f"{dl_parsed.scheme or 'https'}://{alt_host}{path}"
                                    candidate_urls.append(alt_url)
                                break
                    
                    dl_success = False
                    async with httpx.AsyncClient(timeout=60.0, follow_redirects=True, verify=False) as client:
                        for try_url in candidate_urls:
                            response = await client.get(try_url, headers=dl_headers)
                            if response.status_code == 200:
                                with open(local_path, 'wb') as f:
                                    f.write(response.content)
                                _add_log(task_id, f"[SUCCESS] å·²ä¸‹è½½: {filename}")
                                downloaded_count += 1
                                relative_path = f"{folder_name}/{filename}"
                                report["localPath"] = relative_path
                                report["isLocal"] = True
                                dl_success = True
                                break
                            elif response.status_code == 403 and try_url != candidate_urls[-1]:
                                # 403 ä¸”è¿˜æœ‰å¤‡é€‰ URLï¼Œç»§ç»­å°è¯•
                                continue
                            else:
                                _add_log(task_id, f"[WARNING] ä¸‹è½½å¤±è´¥ ({response.status_code}): {report.get('name', 'æœªçŸ¥')[:30]}...")
                            
                except Exception as dl_err:
                    _add_log(task_id, f"[WARNING] ä¸‹è½½å‡ºé”™: {str(dl_err)[:50]}...")
                
                downloaded_reports.append(report)
            
            # åªä¿ç•™å·²ä¸‹è½½çš„æŠ¥å‘Šï¼ˆå‰5ä¸ªï¼‰
            reports = downloaded_reports
            _add_log(task_id, f"[SUCCESS] å…±ä¸‹è½½ {downloaded_count} ä¸ªæ–‡ä»¶åˆ° {pdf_output_dir}")
        
        # æ–°é—»èˆ†æƒ…æ¨¡å¼ï¼šå¦‚æœé€‰æ‹©ä¸‹è½½ï¼Œåˆ™ä¿å­˜ Markdown æ–‡ä»¶
        if request.runMode == "news_sentiment" and request.downloadReport == "yes" and len(news_articles) > 0:
            _add_log(task_id, "[INFO] æ­£åœ¨ç”Ÿæˆ Markdown æ–‡ä»¶...")
            
            # åˆ›å»º output_markdown ç›®å½•
            markdown_output_dir = config.output_dir.parent / "output" / "output_markdown"
            markdown_output_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆ Markdown å†…å®¹
            md_lines = [
                f"# æ–°é—»èˆ†æƒ…çˆ¬å–ç»“æœ",
                f"",
                f"- **çˆ¬å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                f"- **ç›®æ ‡ç½‘å€**: {request.url}",
                f"- **æ–°é—»æ•°é‡**: {len(news_articles)}",
                f"",
                f"---",
                f""
            ]
            
            for article in news_articles:
                md_lines.append(f"## {article.get('id', '')}. {article.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                md_lines.append(f"")
                if article.get('date'):
                    md_lines.append(f"- **æ—¥æœŸ**: {article['date']}")
                if article.get('source'):
                    md_lines.append(f"- **æ¥æº**: {article['source']}")
                if article.get('author'):
                    md_lines.append(f"- **ä½œè€…**: {article['author']}")
                if article.get('sourceUrl'):
                    md_lines.append(f"- **é“¾æ¥**: [{article['sourceUrl']}]({article['sourceUrl']})")
                if article.get('summary'):
                    md_lines.append(f"")
                    md_lines.append(f"> {article['summary']}")
                md_lines.append(f"")
                md_lines.append(f"---")
                md_lines.append(f"")
            
            # ä¿å­˜ Markdown æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            md_filename = f"news_{request.siteName or 'crawl'}_{timestamp}.md"
            md_filepath = markdown_output_dir / md_filename
            
            with open(md_filepath, 'w', encoding='utf-8') as f:
                f.write('\n'.join(md_lines))
            
            markdown_file = str(md_filepath)
            _add_log(task_id, f"[SUCCESS] Markdown å·²ä¿å­˜: {md_filepath}")
        
        # Step 12: ä»»åŠ¡å®Œæˆ
        _update_step(task_id, 11, "ğŸ‰ ä»»åŠ¡å®Œæˆ")
        
        # æ ¹æ®è¿è¡Œæ¨¡å¼ä¿å­˜ä¸åŒçš„ç»“æœ
        if request.runMode == "news_sentiment":
            tasks[task_id]["newsArticles"] = news_articles
            tasks[task_id]["markdownFile"] = markdown_file
        else:
            tasks[task_id]["reports"] = reports
            tasks[task_id]["downloadedCount"] = downloaded_count
            tasks[task_id]["filesNotEnough"] = files_not_enough
            tasks[task_id]["pdfOutputDir"] = str(pdf_output_dir) if pdf_output_dir else None
        _add_log(task_id, "[SUCCESS] âœ… ä»»åŠ¡å®Œæˆï¼")
        
        tasks[task_id]["status"] = "completed"
        tasks[task_id]["resultFile"] = str(output_path)
        
    except asyncio.CancelledError:
        # asyncio ä»»åŠ¡è¢«å–æ¶ˆï¼ˆç”¨æˆ·ç‚¹å‡»åœæ­¢ï¼‰
        _add_log(task_id, "[INFO] â›” ä»»åŠ¡å·²è¢«å¼ºåˆ¶åœæ­¢")
        tasks[task_id]["status"] = "failed"
        tasks[task_id]["error"] = "ä»»åŠ¡è¢«ç”¨æˆ·å¼ºåˆ¶åœæ­¢"
        # ä¸è¦é‡æ–°æŠ›å‡ºï¼Œè®© finally æ‰§è¡Œæ¸…ç†
        
    except Exception as e:
        error_str = str(e)
        # å¦‚æœæ˜¯ç”¨æˆ·å–æ¶ˆï¼Œä½¿ç”¨æ›´å‹å¥½çš„æ¶ˆæ¯
        if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in error_str:
            _add_log(task_id, "[INFO] ä»»åŠ¡å·²è¢«ç”¨æˆ·åœæ­¢")
            tasks[task_id]["error"] = "ä»»åŠ¡è¢«ç”¨æˆ·åœæ­¢"
        else:
            _add_log(task_id, f"[ERROR] ç”Ÿæˆå¤±è´¥: {error_str}")
            tasks[task_id]["error"] = error_str
            import traceback
            _add_log(task_id, f"[ERROR] {traceback.format_exc()}")
        
        tasks[task_id]["status"] = "failed"
        
    finally:
        # æ¸…ç†æµè§ˆå™¨èµ„æº
        try:
            if browser:
                await browser.disconnect()
        except:
            pass
        try:
            if launcher:
                launcher.terminate()
        except:
            pass
        
        # æ¸…ç†å…¨å±€å¼•ç”¨
        if task_id in task_browsers:
            del task_browsers[task_id]
        if task_id in task_cancelled:
            del task_cancelled[task_id]
        if task_id in task_asyncio_tasks:
            del task_asyncio_tasks[task_id]

# ============ API è·¯ç”± ============

@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "service": "PyGen API", "version": "2.0"}

@app.post("/api/menu-tree", response_model=MenuTreeResponse)
async def get_menu_tree(request: MenuTreeRequest):
    """
    è·å–ç›®æ ‡é¡µé¢çš„ç›®å½•æ ‘
    
    ç”¨äº"å¤šé¡µçˆ¬å–"æ¨¡å¼ï¼Œåœ¨ç›®å½•é€‰æ‹©é¡µå±•ç¤ºå¯é€‰ç›®å½•
    """
    if not request.url:
        raise HTTPException(status_code=400, detail="URL ä¸èƒ½ä¸ºç©º")
    
    url = request.url
    if not url.startswith("http"):
        url = "https://" + url
    
    try:
        result = await _fetch_menu_tree(url)
        return MenuTreeResponse(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç›®å½•æ ‘å¤±è´¥: {str(e)}")

@app.post("/api/generate")
async def start_generation(request: GenerateRequest):
    """
    å¯åŠ¨çˆ¬è™«è„šæœ¬ç”Ÿæˆä»»åŠ¡
    
    è¿”å›ä»»åŠ¡ IDï¼Œå‰ç«¯å¯é€šè¿‡ /api/status/{task_id} è½®è¯¢çŠ¶æ€
    """
    if not request.url:
        raise HTTPException(status_code=400, detail="URL ä¸èƒ½ä¸ºç©º")
    
    # åˆ›å»ºä»»åŠ¡
    task_id = str(uuid.uuid4())[:8]
    tasks[task_id] = {
        "taskId": task_id,
        "status": "pending",
        "currentStep": 0,
        "totalSteps": 12,  # å¢åŠ äº†ä»£ç éªŒè¯å’Œç»“æœéªŒè¯æ­¥éª¤
        "stepLabel": "å‡†å¤‡ä¸­...",
        "logs": [],
        "resultFile": None,
        "error": None,
        "reports": [],  # ä¼ä¸šæŠ¥å‘Šåœºæ™¯
        "downloadedCount": 0,  # å·²ä¸‹è½½æ–‡ä»¶æ•°é‡
        "filesNotEnough": False,  # æ–‡ä»¶æ˜¯å¦ä¸è¶³5ä»½
        "pdfOutputDir": None,  # PDF ä¸‹è½½ç›®å½•
        "newsArticles": [],  # æ–°é—»èˆ†æƒ…åœºæ™¯
        "markdownFile": None,  # æ–°é—»èˆ†æƒ… Markdown æ–‡ä»¶
        "createdAt": time.time()
    }
    
    # ä½¿ç”¨ asyncio.create_task åˆ›å»ºå¯å–æ¶ˆçš„ä»»åŠ¡
    asyncio_task = asyncio.create_task(_run_generation_task(task_id, request))
    task_asyncio_tasks[task_id] = asyncio_task
    
    return {"taskId": task_id, "message": "ä»»åŠ¡å·²åˆ›å»º"}

@app.get("/api/status/{task_id}", response_model=TaskStatusResponse)
async def get_task_status(task_id: str):
    """
    è·å–ä»»åŠ¡çŠ¶æ€å’Œæ—¥å¿—
    
    å‰ç«¯è½®è¯¢æ­¤æ¥å£ä»¥æ›´æ–°è¿›åº¦å’Œæ—¥å¿—
    """
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task = tasks[task_id]
    return TaskStatusResponse(
        taskId=task["taskId"],
        status=task["status"],
        currentStep=task["currentStep"],
        totalSteps=task["totalSteps"],
        stepLabel=task["stepLabel"],
        logs=task["logs"],
        resultFile=task.get("resultFile"),
        error=task.get("error"),
        reports=task.get("reports"),
        downloadedCount=task.get("downloadedCount"),
        filesNotEnough=task.get("filesNotEnough"),
        pdfOutputDir=task.get("pdfOutputDir"),
        newsArticles=task.get("newsArticles"),
        markdownFile=task.get("markdownFile"),
        totalCount=task.get("totalCount")
    )

@app.get("/api/download/{filename}")
async def download_file(filename: str):
    """
    ä¸‹è½½ç”Ÿæˆçš„è„šæœ¬æ–‡ä»¶
    """
    output_dir = config.output_dir if config else Path("./py")
    file_path = output_dir / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    return FileResponse(
        path=file_path,
        filename=filename,
        media_type="text/x-python"
    )


@app.get("/api/pdf/{filepath:path}")
async def view_pdf(filepath: str):
    """
    æŸ¥çœ‹æœ¬åœ°ä¸‹è½½çš„ PDF æ–‡ä»¶
    
    filepath æ ¼å¼: "å­æ–‡ä»¶å¤¹å/æ–‡ä»¶å.pdf" (ç›¸å¯¹äº output/output_pdf ç›®å½•)
    """
    import urllib.parse
    from starlette.responses import Response
    
    try:
        # è§£ç  URL ç¼–ç çš„è·¯å¾„
        decoded_path = urllib.parse.unquote(filepath)
        
        # ä¸ä¿å­˜æ–‡ä»¶æ—¶ä½¿ç”¨ç›¸åŒçš„è·¯å¾„é€»è¾‘
        # output_dir æ˜¯ py ç›®å½•ï¼Œoutput_dir.parent / "output" æ˜¯ output ç›®å½•
        output_dir = config.output_dir if config else Path("./py")
        pdf_base_dir = output_dir.parent / "output" / "output_pdf"
        
        # è°ƒè¯•æ—¥å¿—
        print(f"[DEBUG] è¯·æ±‚è·¯å¾„: {filepath}")
        print(f"[DEBUG] è§£ç è·¯å¾„: {decoded_path}")
        print(f"[DEBUG] output_dir: {output_dir}")
        print(f"[DEBUG] pdf_base_dir: {pdf_base_dir}")
        print(f"[DEBUG] pdf_base_dir ç»å¯¹è·¯å¾„: {pdf_base_dir.absolute()}")
        print(f"[DEBUG] pdf_base_dir å­˜åœ¨: {pdf_base_dir.exists()}")
        
        # é¦–å…ˆå°è¯•ç›´æ¥æ‹¼æ¥è·¯å¾„
        file_path = pdf_base_dir / decoded_path
        print(f"[DEBUG] å°è¯•è·¯å¾„: {file_path}")
        print(f"[DEBUG] æ–‡ä»¶å­˜åœ¨: {file_path.exists()}")
        
        if not file_path.exists() and pdf_base_dir.exists():
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•éå†å­æ–‡ä»¶å¤¹
            filename_only = Path(decoded_path).name
            for subdir in pdf_base_dir.iterdir():
                if subdir.is_dir():
                    potential_path = subdir / filename_only
                    if potential_path.exists():
                        file_path = potential_path
                        break
        
        if not file_path.exists():
            raise HTTPException(
                status_code=404, 
                detail=f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {filepath}, æŸ¥æ‰¾ç›®å½•: {pdf_base_dir}, å®Œæ•´è·¯å¾„: {file_path}"
            )
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®š MIME ç±»å‹
        ext = file_path.suffix.lower()
        mime_types = {
            ".pdf": "application/pdf",
            ".doc": "application/msword",
            ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            ".xls": "application/vnd.ms-excel",
            ".xlsx": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        }
        media_type = mime_types.get(ext, "application/octet-stream")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, 'rb') as f:
            content = f.read()
        
        # å¯¹æ–‡ä»¶åè¿›è¡Œ URL ç¼–ç ï¼Œå¤„ç†ä¸­æ–‡å­—ç¬¦
        # ä½¿ç”¨ RFC 5987 æ ¼å¼: filename*=UTF-8''encoded_filename
        encoded_filename = urllib.parse.quote(file_path.name)
        
        # ä½¿ç”¨ inline æ¨¡å¼è®©æµè§ˆå™¨ç›´æ¥æ˜¾ç¤ºè€Œä¸æ˜¯ä¸‹è½½
        return Response(
            content=content,
            media_type=media_type,
            headers={
                "Content-Disposition": f"inline; filename*=UTF-8''{encoded_filename}"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")


@app.post("/api/stop/{task_id}")
async def stop_task(task_id: str):
    """
    åœæ­¢æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
    
    ç»ˆæ­¢ä»»åŠ¡ç›¸å…³çš„æ‰€æœ‰è¿›ç¨‹ï¼ˆæµè§ˆå™¨ã€çˆ¬è™«è„šæœ¬ç­‰ï¼‰
    """
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task = tasks[task_id]
    
    # å¦‚æœä»»åŠ¡å·²ç»å®Œæˆæˆ–å¤±è´¥ï¼Œæ— éœ€åœæ­¢
    if task["status"] in ["completed", "failed"]:
        return {"message": "ä»»åŠ¡å·²ç»“æŸ", "status": task["status"]}
    
    _add_log(task_id, "[INFO] æ”¶åˆ°åœæ­¢è¯·æ±‚ï¼Œæ­£åœ¨å¼ºåˆ¶ç»ˆæ­¢ä»»åŠ¡...")
    
    # è®¾ç½®å–æ¶ˆæ ‡å¿—
    task_cancelled[task_id] = True
    
    # ã€å…³é”®ã€‘å–æ¶ˆ asyncio ä»»åŠ¡ï¼ˆç«‹å³ä¸­æ–­æ­£åœ¨ç­‰å¾…çš„å¼‚æ­¥æ“ä½œï¼‰
    if task_id in task_asyncio_tasks:
        asyncio_task = task_asyncio_tasks[task_id]
        if not asyncio_task.done():
            asyncio_task.cancel()
            _add_log(task_id, "[INFO] å¼‚æ­¥ä»»åŠ¡å·²å–æ¶ˆ")
        if task_id in task_asyncio_tasks:
            del task_asyncio_tasks[task_id]
    
    # å°è¯•ç»ˆæ­¢æ­£åœ¨è¿è¡Œçš„å­è¿›ç¨‹ï¼ˆçˆ¬è™«è„šæœ¬ï¼‰
    if task_id in task_processes:
        process = task_processes[task_id]
        try:
            import os
            
            # Windows ä¸Šä½¿ç”¨ terminate()ï¼ŒUnix ä¸Šå‘é€ SIGTERM
            if os.name == 'nt':
                process.terminate()
            else:
                import signal
                process.send_signal(signal.SIGTERM)
            
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´
            try:
                process.wait(timeout=3)
            except:
                # å¦‚æœè¿˜æ²¡ç»“æŸï¼Œå¼ºåˆ¶æ€æ­»
                process.kill()
                process.wait()
            
            _add_log(task_id, "[INFO] çˆ¬è™«è„šæœ¬å·²ç»ˆæ­¢")
        except Exception as e:
            _add_log(task_id, f"[WARNING] ç»ˆæ­¢è¿›ç¨‹æ—¶å‡ºé”™: {e}")
        finally:
            if task_id in task_processes:
                del task_processes[task_id]
    
    # å°è¯•å…³é—­æµè§ˆå™¨
    if task_id in task_browsers:
        browser_info = task_browsers[task_id]
        try:
            browser = browser_info.get("browser")
            launcher = browser_info.get("launcher")
            
            if browser:
                try:
                    await browser.disconnect()
                    _add_log(task_id, "[INFO] æµè§ˆå™¨è¿æ¥å·²æ–­å¼€")
                except Exception as e:
                    _add_log(task_id, f"[WARNING] æ–­å¼€æµè§ˆå™¨æ—¶å‡ºé”™: {e}")
            
            if launcher:
                try:
                    launcher.terminate()
                    _add_log(task_id, "[INFO] Chrome è¿›ç¨‹å·²ç»ˆæ­¢")
                except Exception as e:
                    _add_log(task_id, f"[WARNING] ç»ˆæ­¢ Chrome æ—¶å‡ºé”™: {e}")
                    
        except Exception as e:
            _add_log(task_id, f"[WARNING] æ¸…ç†æµè§ˆå™¨èµ„æºæ—¶å‡ºé”™: {e}")
        finally:
            if task_id in task_browsers:
                del task_browsers[task_id]
    
    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
    task["status"] = "failed"
    task["error"] = "ä»»åŠ¡è¢«ç”¨æˆ·åœæ­¢"
    _add_log(task_id, "[INFO] âœ… ä»»åŠ¡å·²åœæ­¢")
    
    return {"message": "ä»»åŠ¡å·²åœæ­¢", "taskId": task_id}

# ============ ä¸»å…¥å£ ============

if __name__ == "__main__":
    # Windows + Python 3.12+ é»˜è®¤ ProactorEventLoop åœ¨è¿›ç¨‹é€€å‡º/CTRL+C æ—¶
    # å¶å‘å‡ºç° "unclosed transport" / "I/O operation on closed pipe" çš„æ¸…ç†å™ªéŸ³ã€‚
    # åˆ‡æ¢ä¸º SelectorEventLoop å¯æ˜¾è‘—å‡å°‘è¯¥ç±»æŠ¥é”™ï¼ˆä¸å½±å“åŠŸèƒ½ï¼‰ã€‚
    import os as _os
    import asyncio as _asyncio
    if _os.name == "nt":
        try:
            _asyncio.set_event_loop_policy(_asyncio.WindowsSelectorEventLoopPolicy())  # type: ignore[attr-defined]
        except Exception:
            pass

    import uvicorn
    print("ğŸš€ å¯åŠ¨ PyGen API Server...")
    print("   è®¿é—® http://localhost:8000/docs æŸ¥çœ‹ API æ–‡æ¡£")
    uvicorn.run(app, host="0.0.0.0", port=8000)

